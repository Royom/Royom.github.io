

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" href="/img/favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Roy">
  <meta name="keywords" content="">
  
    <meta name="description" content="vllm框架入门">
<meta property="og:type" content="article">
<meta property="og:title" content="vllm入门记录">
<meta property="og:url" content="https://royom.github.io/2025/01/25/vllm%E5%85%A5%E9%97%A8/index.html">
<meta property="og:site_name" content="Roy&#39;s blog">
<meta property="og:description" content="vllm框架入门">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://royom.github.io/2025/01/25/vllm%E5%85%A5%E9%97%A8/image-20250207103304260.png">
<meta property="og:image" content="https://royom.github.io/2025/01/25/vllm%E5%85%A5%E9%97%A8/image-20250207105504507.png">
<meta property="og:image" content="https://royom.github.io/2025/01/25/vllm%E5%85%A5%E9%97%A8/image-20250207122328371.png">
<meta property="og:image" content="https://royom.github.io/2025/01/25/vllm%E5%85%A5%E9%97%A8/image-20250207135439795.png">
<meta property="og:image" content="https://royom.github.io/2025/01/25/vllm%E5%85%A5%E9%97%A8/image-20250210150655663.png">
<meta property="og:image" content="https://royom.github.io/2025/01/25/vllm%E5%85%A5%E9%97%A8/image-20250205094850863.png">
<meta property="og:image" content="https://royom.github.io/2025/01/25/vllm%E5%85%A5%E9%97%A8/image-20250226170707722.png">
<meta property="og:image" content="https://royom.github.io/2025/01/25/vllm%E5%85%A5%E9%97%A8/image-20250227141723859.png">
<meta property="article:published_time" content="2025-01-25T07:18:42.000Z">
<meta property="article:modified_time" content="2025-05-20T10:44:58.647Z">
<meta property="article:author" content="Roy">
<meta property="article:tag" content="科研, KV Cache">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://royom.github.io/2025/01/25/vllm%E5%85%A5%E9%97%A8/image-20250207103304260.png">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>vllm入门记录 - Roy&#39;s blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"royom.github.io","root":"/","version":"1.9.7","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Roy&#39;s blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="vllm入门记录"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-01-25 15:18" pubdate>
          2025年1月25日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          5.3k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          44 分钟
        
      </span>
    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> 次
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">vllm入门记录</h1>
            
            
              <div class="markdown-body">
                
                <h2 id="跑例程">1.跑例程</h2>
<h3 id="官方例程">1.1官方例程</h3>
<p>对vllm、SamplingParams这两个库有个初步印象</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> vllm <span class="hljs-keyword">import</span> LLM, SamplingParams<br><br><span class="hljs-comment"># Sample prompts.</span><br>prompts = [<br>    <span class="hljs-string">&quot;Hello, my name is&quot;</span>,<br>    <span class="hljs-string">&quot;The president of the United States is&quot;</span>,<br>    <span class="hljs-string">&quot;The capital of France is&quot;</span>,<br>    <span class="hljs-string">&quot;The future of AI is&quot;</span>,<br>]<br><span class="hljs-comment"># Create a sampling params object.</span><br>sampling_params = SamplingParams(temperature=<span class="hljs-number">0.8</span>, top_p=<span class="hljs-number">0.95</span>)<br><br><span class="hljs-comment"># Create an LLM.</span><br>llm = LLM(model=<span class="hljs-string">&quot;facebook/opt-125m&quot;</span>)<br><span class="hljs-comment"># Generate texts from the prompts. The output is a list of RequestOutput objects</span><br><span class="hljs-comment"># that contain the prompt, generated text, and other information.</span><br>outputs = llm.generate(prompts, sampling_params)<br><span class="hljs-comment"># Print the outputs.</span><br><span class="hljs-keyword">for</span> output <span class="hljs-keyword">in</span> outputs:<br>    prompt = output.prompt<br>    generated_text = output.outputs[<span class="hljs-number">0</span>].text<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Prompt: <span class="hljs-subst">&#123;prompt!r&#125;</span>, Generated text: <span class="hljs-subst">&#123;generated_text!r&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure>
<h3 id="例程2">1.2例程2</h3>
<p>网上的一个例程，在huggingface_hub调用的模型，参数、调用有一些区别，比如还指定了分词器用于然后规范化传入</p>
<p>不过都是model.generate</p>
<h3 id="vllm二次开发视频教程">1.3 vllm二次开发视频教程</h3>
<p>dtype精读设定可能影响你修改模型后的logits值</p>
<figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs clean">model_executor/models/__init__.py <br>这个文件，相较视频里面的又有更改，原来似乎是指示了对应架构模型的相关文件以及相关类<br>现在的这个文件的作用是将一些重要的类和函数从不同的子模块导入，并通过 __all__ 使它们可以通过 <span class="hljs-keyword">from</span> <span class="hljs-keyword">module</span> <span class="hljs-keyword">import</span> * 的方式直接使用。<br>这些导入的类和函数涉及到模型的特性（如LoRA、池化、多模态等），并且为模型的管理和检查提供了工具。<br></code></pre></td></tr></table></figure>
<h3 id="继续看官方例程和官方文档">1.4 继续看官方例程和官方文档</h3>
<p><a
target="_blank" rel="noopener" href="https://docs.vllm.ai/en/latest/design/v1/prefix_caching.html">官方文档</a></p>
<ol type="1">
<li>llm.generate生成的是一个RequestOutput类型的对象，这个类的定义在vllm/outputs.py中，这个对象的一个核心字段outputs是一个CompletionOutput类型的列表（也在outputs.py中定义），outputs[0].text是输出的文本（如果有多个输出结果可能用到outputs[1].text等）</li>
</ol>
<h4 id="chat.py">1.4.1 chat.py</h4>
<p>这里面用到了批量推理，用了llm.chat而非llm.generate，顺路了解了一下这两个LLM方法的区别</p>
<h4 id="官方文档-生成模型">1.4.2 官方文档-生成模型</h4>
<p>LLM.chat接受OpenAI的API格式，需要使用的时候在官方文档-Models-Generative
Models可以找到链接</p>
<h4 id="用vscode调试了一下">1.4.3 用VScode调试了一下</h4>
<p>看了一下output的结构</p>
<h4 id="官方文档-推理服务和表现">1.4.4 官方文档-推理服务和表现</h4>
<ol type="1">
<li>Inference and Serving
<ol type="1">
<li>Offline Inference
<ul>
<li>关于启动、参数配置可看此</li>
</ul></li>
<li>Distributed Inference and Serving
<ul>
<li>暂时用不上分布式，需要可看</li>
</ul></li>
</ol></li>
<li>Performance
<ol type="1">
<li>Optimization and Tuning
<ol type="1">
<li>如果打印抢占的警告表明KV 空间不太够用，要用一些方式提升</li>
<li>提到了分块预填充的方式（似乎是FastGen的方法）</li>
</ol></li>
</ol></li>
</ol>
<h4 id="官方文档-架构">1.4.5 官方文档-架构</h4>
<ol type="1">
<li><p>Design Documents</p>
<ol type="1">
<li><p><strong>Architecture Overview</strong></p>
<ol type="1">
<li>关系：<img src="/2025/01/25/vllm%E5%85%A5%E9%97%A8/image-20250207103304260.png" srcset="/img/loading.gif" lazyload class="" title="image-20250207103304260"> <img src="/2025/01/25/vllm%E5%85%A5%E9%97%A8/image-20250207105504507.png" srcset="/img/loading.gif" lazyload class="" title="image-20250207105504507"></li>
<li><figure>
<img
src="https://mmbiz.qpic.cn/mmbiz_png/VnDXQzNf28h86XrgCslRx0EBFHHbyt18F8ICtSl1yTmB6J72DcclI3TsIqtaw6NuZnyGGDAeVs9UhjHWyHOrqA/640?wx_fmt=png&amp;from=appmsg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" srcset="/img/loading.gif" lazyload
alt="图片" />
<figcaption aria-hidden="true">图片</figcaption>
</figure></li>
<li>展示了类之间的关系， 包括Worker、Model Runner、Model等</li>
</ol></li>
<li><p>Integration with Hugging face</p>
<ol type="1">
<li>阐释了vllm如何找模型以及在model_executor/models里面去找到对应的文件来对自己初始化（根据config里面的architectures字段确定初始类，通过映射找到其对应的模型类，然后去找到相应初始化文件）</li>
<li>此外，Tokenizer和 Model weight也是来自于Hugging face
<ol type="1">
<li>Tokenizer的相关参数要看Huggingface的文档</li>
</ol></li>
<li>config.json可以来自本地、本地缓存orHugging face仓库</li>
</ol></li>
<li><p><strong>vLLM Paged Attention</strong></p>
<ol type="1">
<li><blockquote>
<p>目前，vLLM 利用自己的多头查询注意内核实现（
<code>csrc/attention/attention_kernels.cu</code>
）。这个内核被设计为与vLLM的分页KV缓存兼容，其中键和值缓存存储在单独的块中（注意，这个块概念与GPU线程块不同。所以在后面的文档中，我将参考vLLM分页注意力block
称为“块”，而将 GPU 线程块称为“线程块”）。</p>
</blockquote></li>
<li><p><code>scalar_t</code> 表示查询、键和值数据元素的数据类型， 例如
FP16。 <code>HEAD_SIZE</code>表示每个头中的元素数量。
<code>BLOCK_SIZE</code>指的是每个区块中的代币数量。
<code>NUM_THREADS</code>表示每个线程块中的线程数。
<code>PARTITION_SIZE</code>表示张量并行 GPU
的数量（为简单起见，我们假设这是 0 并且张量并行被禁用）。</p></li>
<li><p><strong>解释了Sequence, context, vec, thread group, block , warp,
thread block, grid</strong></p></li>
<li><p><strong>详细解释QKV这些变量的计算过程在代码中的实现，但是看得非常困难，不是很懂。</strong></p></li>
</ol></li>
<li><p><strong>Automatic Prefix Caching</strong></p>
<ol type="1">
<li>这个之前看过，思路为主，现在是通过哈希映射来查找物理块，不用维护KV块之间的树结构。</li>
<li>谈了驱逐策略（LRU）</li>
</ol></li>
<li><p><a
target="_blank" rel="noopener" href="https://docs.vllm.ai/en/latest/design/v1/prefix_caching.html"><strong>Automatic
Prefix Caching(in V1 Design Documents)</strong></a></p>
<ol type="1">
<li><p>前缀缓存的实现是：<strong>KV cache manager</strong>.
<code>class KVCacheBlock</code></p></li>
<li><p>初始化KV Cache manager的时候就分配了所有KVCacheBlock</p></li>
<li><p>Free block queue是空闲队列 <img src="/2025/01/25/vllm%E5%85%A5%E9%97%A8/image-20250207122328371.png" srcset="/img/loading.gif" lazyload class="" title="image-20250207122328371"></p></li>
<li><p>请注意：只缓存完整的块</p></li>
<li><p>在<strong>vLLM
v0</strong>版本中，当检测到<code>块3</code>是重复的时，它会释放块3并让Request
2使用已经缓存的块1。因此，<code>块表</code>在Time
1时会变成<code>[0, 1]</code>。</p>
<p>然而，在<strong>vLLM
v1</strong>版本中，块表是追加式的（append-only），这意味着无法改变已经存在的块表。所以，<code>块表</code>会保持为<code>[0, 3]</code>，即使<code>块3</code>与<code>块1</code>重复。这个重复的块会在请求完成并释放时被清除。</p></li>
<li><p>要释放的块的顺序（比如一个请求对应块2348，添加到Free
queue的时候顺序是8432），因为更不容易被复用（可以理解为要完全匹配到的前缀更少？）</p></li>
<li><p><strong>这里有个非常好的示例来帮助理解 Request Blocks, Cache
Blocks, Block
Pool(list)与它的两个指针。</strong>有多张图片，选择其中一张作为范例
<img src="/2025/01/25/vllm%E5%85%A5%E9%97%A8/image-20250207135439795.png" srcset="/img/loading.gif" lazyload class="" title="image-20250207135439795"> 一个块用满的时候就会被缓存
一旦没有请求正在使用，就会从Cache Blocks踢出去到Block
Pool(list)，但是没有立即释放 请求申请块是从Block Pool(list)
申请的，如果同一时间有其他请求相同前缀的块会复用，不会额外申请。
请求申请块前如果Cache Blocks没有，那就在Block Pool(list)
中touch，找到相同块就会把它们拿走（比如这里的0,1,2)，然后再从Block
Pool(list)里申请块</p></li>
</ol></li>
</ol></li>
</ol>
<h4 id="其余博客">1.5 其余博客</h4>
<h5 id="vllm源码解析2-调度器策略">1.5.1<a
target="_blank" rel="noopener" href="https://agijuejin.feishu.cn/wiki/XEncwa9s0iF3fjkODlacbdLhnLq">vllm源码解析2-调度器策略</a></h5>
<img src="/2025/01/25/vllm%E5%85%A5%E9%97%A8/image-20250210150655663.png" srcset="/img/loading.gif" lazyload class="" title="image-20250210150655663">
<ul>
<li><p><a
target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/654910335">[猛猿：图解大模型计算加速系列：vLLM源码解析](https://zhuanlan.zhihu.com/p/691045737)</a>：有五篇，涵盖的内容非常广，对源码做了一个比较深入的解读，SequenceGroup,
Scheduler，Step...
而且有不少结构图，虽然更新后有一些改变了但还是很值得一读。</p>
<ul>
<li><p><strong>"1个prompt -&gt;
多个outputs"这样的结构组成一个<code>SequenceGroup</code>实例。在vLLM中有一个重要假设：一个seq_group中的所有seq共享1个prompt。</strong></p></li>
<li><p><strong>我们要记住vLLM调度中非常重要的一点：在1个推理阶段中，所有的seq_group要么全部处在prefill阶段。要么全部处在decode阶段。</strong></p></li>
<li><p>running队列中seq_group下的n个seqs在上1个推理阶段共生成了n个token。在本次调度中，我们要先为这n个token分配物理块空间，用于存放它们在本次调度中即将产生的KV值。<strong>对于1个seq来说，最坏的情况就是添加1个物理块；对于n个seqs来说，最坏的情况就是添加n个物理块（想想原理篇中讲过的copy-on-write机制）</strong></p></li>
<li><p><strong>waiting队列中所有的seq_group都没做过prefill，因此每个seq_group下面只有1条seq</strong>，这个seq即位prompt本身，所以我们取[0]即可拿出这个prompt</p></li>
<li><p>以下按prefix cached和prefix uncache的情况做区分：</p></li>
<li><p>uncached：分配物理块做<strong>PreFill</strong>的时候：由于seq_group下的所有seq共享一个prompt，
所以进一步令物理块的ref_count = num_seqs
（表示这些seqs的逻辑块都引用它了）。</p>
<ul>
<li>在使用<code>UncachedBlockAllocator</code>为wating队列中的某个seq_group分配物理块时，我们就是在对初始的这个prompt分配物理块。所以这个prompt有多少个逻辑块，我们就分配多少个可用的空闲物理块，同时注意更新物理块的ref_count。PS:当前版本移除了这个类，但可能实质还在</li>
<li>你一定发现了，这里我们做的只是<strong>给定一种“物理块的分配方案”</strong>，我们只是在制定这个seq_group可以使用哪些物理块，<strong>但并没有实际往物理块中添加数据！“添加数据”这一步留到这1步推理实际开始时，由CacheEngine按照这个方案，往物理块中实际添加KV
Cache。这个我们留在再后面的系列讲解。</strong></li>
</ul></li>
<li><p>uncached：分配物理块做<strong>decode</strong>的时候：为running/swapped队列中的seq_group分配decode需要的物理块</p>
<ul>
<li><strong>调用<code>self.block_manager.can_append_slot(seq_group)</code>方法</strong>，判断是否至少能为这个seq_group下的每个seq都分配1个空闲物理块。如果可以则认为能调度这个seq_group（原因和代码分析我们在源码解读2中细讲过，这里不赘述）。</li>
<li><strong>调用<code>self._append_slot(seq_group, blocks_to_copy)</code>方法</strong>，实际分配物理块。</li>
<li>同样，在这里我们依然要强调，调度器中只是给出了物理块的分配方案，并没有实际往物理块中添加数据，添加数据这一步是CacheEngine照着这个方案来实际操作的，这个我们放在后面的文章中讲解。</li>
</ul></li>
<li><p>cached:prefill阶段的分配：</p>
<ul>
<li>hash_of_block(logical_idx) &amp;
num.hashed_tokens_of_block(logical_idx)
，根据前面到本块的token来得到hash值。<strong>当两个等待做prefill的seq拥有同样的hash值时，说明它们共享一样的prompt，这时就可以重复利用已有的KV
cache</strong>（PS:不止涉及这两个方法）</li>
<li>代码中的相关prefix
cache情况从表面上理解就是和我上方从官网截的一样</li>
</ul></li>
<li><p>cached: decode阶段的分配：</p>
<ul>
<li>以n=2的并行解码为例，在启动copy-on-write机制的同时，我们也要重新计算物理块的hash值。和prefill阶段不同，在decode阶段，当这个物理块还没满的时候，我们会给它附一个相互不重复的默认hash值。<strong>我们会把附上hash值的物理块加入CachedBlockAllocator的<code>cached_blocks</code>属性中当一个子序列用完当前物理块的所有slots时（例如当子序列1生成J0后），我们再对这个物理块重新做hash计算，计算方式是hash(A~J0)。</strong></li>
<li>decode阶段的prefix
caching不是一种频繁地复用，而是一种累积到一定范围尽可能地长段复用，这也更加方便做KV
cache管理。</li>
</ul></li>
</ul></li>
<li><p><a
target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_42479327/article/details/141496484">vllm源码解析(一)：整体架构与推理代码</a>：有六篇
0.54版本，</p></li>
</ul>
<h5 id="vllm源码-llmengine">1.5.2 <a
target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/643336063">vllm源码
LLMEngine</a></h5>
<p>部分过时，思路大体还是一样</p>
<h5 id="一文详解vllm架构">1.5.3 <a
target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=Mzk0ODY4MjU3MQ==&amp;mid=2247493266&amp;idx=3&amp;sn=4e4e28f60d7b29486ff798e9a63baa94&amp;chksm=c20953e3a73adb133b3400b6b4a77c53cbbffe052587dfb3ebcb57fa5180b77fbb2fd7aa7826#rd">一文详解vLLM架构</a></h5>
<p>Scheduler结构</p>
<figure>
<img
src="https://mmbiz.qpic.cn/mmbiz_jpg/VnDXQzNf28h86XrgCslRx0EBFHHbyt18n2dft1duO7c50nt37F0icZ6YoPl78iciahVuiamOzDibG79jWeiaicVUpjrkw/640?wx_fmt=jpeg&amp;from=appmsg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" srcset="/img/loading.gif" lazyload
alt="图片" />
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<h3 id="自己看的一些源代码">1.6 自己看的一些源代码</h3>
<ul>
<li>core/block
<ul>
<li>prefix_caching_block.py
<ul>
<li><code>find_cached_blocks_prefix(self, block_hashes: List[int]) -&gt; List[int]</code>
这个方法采用的二分法，找出<code>block_hashes</code>列表中所有已缓存且已计算的块的前缀部分。</li>
</ul></li>
</ul></li>
<li>core
<ul>
<li>kv_cache_utils.py
<ul>
<li><code>FreeKVCacheBlockQueue</code> 这个应该就是设计文档中提到的free
block queue,
它处理的对象是<code>KVCacheBlock</code>详情参考源码注释</li>
</ul></li>
</ul></li>
</ul>
<h2 id="目录结构">2.目录结构</h2>
<p>一张解释初始化和推理的一个大致流程模块图</p>
<img src="/2025/01/25/vllm%E5%85%A5%E9%97%A8/image-20250205094850863.png" srcset="/img/loading.gif" lazyload class="" title="image-20250205094850863">
<h3 id="vllm">2.1vllm</h3>
<ul>
<li>model_executor
<ul>
<li>其中models是把每个模型都写了代码，如果要新的模型或许要再这里继续加代码</li>
<li>model_loader 把原来的pytorch(?)算子之类的转为vllm定义的 e.g.
线性层</li>
</ul></li>
<li>executor
<ul>
<li>都是基于executer_base.py这个基类进行的二次开发</li>
<li>有CPU、单GPU、分布式GPU、ray等方式</li>
</ul></li>
<li>主目录下的utils.py工具类包含内存管理、检测和测量，分布式计算中的通信以及其余四五个小型工具，以及比如随机初始化kvcache的调试工具。（应该是用于调试？）</li>
</ul>
<h2 id="动手实践">3.动手实践</h2>
<h3
id="离线推理调用本地模型修改最大token数">1.离线推理+调用本地模型+修改最大token数</h3>
<ul>
<li><p>基于调用Huggingface上的模型进行改造，原来的需要login（密钥）</p></li>
<li><p>去Huggingface上下的文件只用：<strong>config.json, tokenizer.json,
tokenizer_config.json, model.safetensors,
special_tokens_map.json</strong></p></li>
<li><p>不过改了之后因为config.json的原因？max_seq_len又超大了（131072），这个是在<strong>tokenizer.json</strong>里定义的</p></li>
<li><p>我在vllm serve的参数选择里看到说：</p>
<figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs gams">--<span class="hljs-built_in">max</span>-<span class="hljs-keyword">model</span>-len<br><span class="hljs-keyword">Model</span> context length. <span class="hljs-keyword">If</span> unspecified, will be automatically derived from the <span class="hljs-keyword">model</span> config.<br></code></pre></td></tr></table></figure>
<p><del>所以我推测应该只能在tokenizer.json中修改，vllm的LLM和Samplingparams都没有最大输入上下文这个参数</del></p></li>
<li><p>在LLM参数里设置这个max_model_len =
65536，虽然我在llm.py里面没有找到（其他的，但是它就是能运行），而且vllm
serve也是这个参数</p></li>
</ul>
<h3 id="多轮对话">2.多轮对话</h3>
<h4 id="offline">2.1 offline</h4>
<ul>
<li>Examples里面有个prefix_caching.py，指出LLM参数里要设置enable_prefix_caching=True</li>
<li>Benchmark里面有个benchmark_prefix_caching.py，是官方用语测试的脚本，准备在这个上面改，必须指定模型，最好指定数据集（否则随机生成）</li>
<li>谈一下对这个benchmark_prefix_caching.py的代码的一部分理解：它调用了FlexibleArgumentParser这个在utils.py中的工具，使之能传入多种参数控制，我用到的如下所示，</li>
</ul>
<figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs jboss-cli">    运行的使用命令<br><br>	修改参数做测试，output-len input-length-range <br>		python <span class="hljs-string">/home/roy/projects/test/new_benchmark_prefix_caching.py</span> \<br>        <span class="hljs-params">--model</span> <span class="hljs-string">/home/roy/models/meta-llamaLlama-3.2-1B-Instruct</span> \<br>        <span class="hljs-params">--dataset-path</span> <span class="hljs-string">/home/roy/models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json</span> \<br>        <span class="hljs-params">--max-model-len</span> 32768 \<br>        <span class="hljs-params">--num-prompts</span> 40 \<br>        <span class="hljs-params">--output-len</span> 3200 \<br>        <span class="hljs-params">--repeat-count</span> 4 \<br>        <span class="hljs-params">--input-length-range</span> 2048<span class="hljs-function">:3172</span> \<br>        <br>        <br>        <span class="hljs-params">--no-enable-prefix-caching</span><br>        <span class="hljs-params">--enable-prefix-caching</span><br><br>python benchmark_throughput.py \<br>        <span class="hljs-params">--model</span> <span class="hljs-string">/home/roy/models/meta-llamaLlama-3.2-1B-Instruct</span> \<br>        <span class="hljs-params">--dataset-path</span> <span class="hljs-string">/home/roy/models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json</span> \<br>        <span class="hljs-params">--max-model-len</span> 32768 \<br>        <span class="hljs-params">--num-prompts</span> 40 \<br>        <span class="hljs-params">--output-len</span> 3200 \<br>        <span class="hljs-params">--repeat-count</span> 4 \<br>        <span class="hljs-params">--input-length-range</span> 2048<span class="hljs-function">:3172</span> \<br>        <span class="hljs-params">---cpu-offload-gb</span> 16<br>        <span class="hljs-params">--no-enable-prefix-caching</span><br>        <span class="hljs-params">--enable-prefix-caching</span><br><br>python benchmarks/benchmark_throughput.py <span class="hljs-params">--output-len</span> 16 <span class="hljs-params">--input</span> 16 <span class="hljs-params">--model</span> <span class="hljs-string">/home/roy/models/meta-llamaLlama-3.2-1B-Instruct</span> <span class="hljs-params">--num-prompts</span> 256<br><br>-cpu-offload-gb 16<br><br>设置用不用V1：<br>~<span class="hljs-string">/.bashrc</span> <span class="hljs-comment">#永久设置文件在这里</span><br><span class="hljs-keyword">echo</span> $VLLM_USE_V1 <span class="hljs-comment">#检查</span><br>export VLLM_USE_V1=1 <span class="hljs-comment">#临时设置</span><br><span class="hljs-keyword">unset</span> VLLM_USE_V1 <span class="hljs-comment">#临时取消</span><br></code></pre></td></tr></table></figure>
<p><strong>结果</strong>：</p>
<p>增大--input-length-range，指定--output-len 100，</p>
<p>接下来的一些测试，没有指出则默认继承前面的测试数据（运行结果前为没开前缀缓存，后为开了前缀缓存）：</p>
<ol type="1">
<li><p>将output-len 指定为15000，平均prompt600
左右：运行结果648s/453s,</p></li>
<li><p>将input-length-range指定为 2048:4096，平均prompt
2700，运行结果1665s/1000s，</p></li>
<li><p>将repeat次数削减为1，prompt数量提升至40，我想知道上面明显的效果是不是因为repeat复用的多。运行结果896s/804s。</p></li>
<li><p>将prompt的数量进一步提升至80，运行结果1817s/1969s。</p></li>
<li><p>将prompt的数量降低至20，运行结果为400s/340s</p></li>
<li><p>把prompt的数量提升至120。prefix
cache的hit率在过程中从3.2上升到9.7%，运行结果为<strong>2636s/2737s</strong></p></li>
<li><p>将prompt的数量降低至10，repeat增加至2，运行结果为281s/280s。这个时候GPU
kv cache使用率只有50%左右，感觉不觉比很好的参考价值。</p></li>
<li><p>将prompt的数量提升至20，repeat保持为2，运行结果为717s/661s</p></li>
<li><p>将prompt的数量提升至120，repeat降低为1，输出token数降低为8192，看看相比之前保持长输入但是缩短输出的结果。运行结果为1006s/1031s</p></li>
<li><p>将输出token数升至20480，看看长输入和长输出会不会拉开差距。运行结果为4746s/5075s</p></li>
<li><p>升级至0.72+V1，采用9的“prompt的数量提升至120，repeat降低为1”。但是中间有不少抢占，而且运行结果为
/1164s</p></li>
<li><p>重启后又运行一次竟然因为显存不足（需要2G满足65536的最大token而只剩1.77G)。降低max-model-len为32768方运行。但是运行时间非常之久。1620s
/1339s。发现这个数据比之前的远多，又去测试一下0.72的v0版本的效果，为1606s/1368s（这次prefix
hit rate到了15%左右）</p></li>
<li><p>准备对比V0和V1在高命中率的情况。2048:3172 output-len 4096
--repeat-count 4 V0: 769s/625s 命中率从75降低到64
v1：627s/531s。再测试了一下把输出继续改小至3200tokens。
455s/416s</p></li>
</ol>
<p>观察：</p>
<ul>
<li>当token数较大的时候，最开始迅速处理prompt，处理好后开始生成，KV
cache迅速增长，占用大量内存，然后本来running
reqs很多，逐渐下降，因为空间不够，就变成了Pending
reqs，接下来算是一个循环吧，当结束一些请求的时候，如果腾出了不少空间，则既处理一部分prompt（我猜测是调度策略当觉得KV空间足够的时候就会处理新的prompt），然后把一部分pending
reqs转移到running reqs，随着KV占用继续增加又会转移至一部分至pending
reqs(这里的转移策略是什么？)直到有一些running reqs运行结束。</li>
<li>当启用前缀缓存且input-length-range较大的时候，或许是因为能共用较多的KV
cache，可以看到最开始并行处理的Reqs是比较多的</li>
</ul>
<h3 id="powerinfer与vllm的融合">3.PowerInfer与vllm的融合</h3>
<h4 id="powerinfer的运行">0.PowerInfer的运行</h4>
<p>rm -rf build 移除Cmake的build缓存后终于找到CUDA了</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs stylus">运行命令<br>./build/bin/<span class="hljs-selector-tag">main</span> -m ./models/ReluLLaMA-<span class="hljs-number">7</span>B/llama-<span class="hljs-number">7</span>b-relu<span class="hljs-selector-class">.powerinfer</span><span class="hljs-selector-class">.gguf</span> -n <span class="hljs-number">256</span> -t <span class="hljs-number">8</span> -<span class="hljs-selector-tag">p</span> <span class="hljs-string">&quot;Introduce yourself&quot;</span><br>./build/bin/<span class="hljs-selector-tag">main</span> -m ./models/ReluLLaMA-<span class="hljs-number">70</span>B/llama-<span class="hljs-number">70</span>b-relu<span class="hljs-selector-class">.q4</span><span class="hljs-selector-class">.powerinfer</span><span class="hljs-selector-class">.gguf</span> -n <span class="hljs-number">256</span> -t <span class="hljs-number">8</span> -<span class="hljs-selector-tag">p</span> <span class="hljs-string">&quot;Introduce yourself&quot;</span><br></code></pre></td></tr></table></figure>
<p>论文阅读简单记录</p>
<ul>
<li>简单说：幂律分布的神经元被类似的预测器分为 hot/cold 等类型，hot
神经元在 GPU 上算，cold 神经元在 CPU 上算。</li>
<li>问题：
<ul>
<li>稀疏模型在batch size &gt; 1时会有问题？</li>
<li>只有稀疏模型（乃至Relu）能用？</li>
</ul></li>
<li>之后可能再利用到的知识：
<ul>
<li>在32批次以内，相比分配给CPU需要计算再挪给GPU计算，直接在CPU计算然后返回结果更快</li>
<li>不同层的热神经元不一样，不同模型差的也会比较多，不同任务也不一样。</li>
<li>似乎前面的层热神经元会少一些。</li>
<li>作者指出：各种领域中，前20％最常激活的神经元中有超过90％的重叠</li>
<li>论文做了一个可以单独计算行列的算子，而不需要把稀疏矩阵先转为密集矩阵</li>
<li>MLP块(PowerInfer主要是这个)、自注意力块都存在稀疏激活特性</li>
</ul></li>
</ul>
<h4 id="参考的资料"><strong>参考的资料：</strong></h4>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/671761052">刀刀宁
llama.cpp源码分析</a>：了解llama.cpp架构必看，关注核心函数。</li>
<li><a
target="_blank" rel="noopener" href="https://alleny.xyz/post/powerinfer-source-analysis-1/">PoweInfer源码解析</a>：一个人写的博客，可以看PowInfer做了什么更改。</li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/665027154">Codelearner
llama.cpp源码解析</a>
2023-11的源码解析，包含代码结构、调用流程、算子解析</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/ggml-org/ggml/blob/master/docs/gguf.md">github
上的 gguf.md</a> 描述了<strong>模型文件的格式</strong></li>
<li><a
target="_blank" rel="noopener" href="https://blog.csdn.net/daihaoguang/article/details/141998001">vLLM
(4) - LLMEngine上篇</a> 这个系列偏新
24.9，<strong>类图</strong>很适合理解初始化和推理过程</li>
</ul>
<p><strong>可能存在的问题：</strong></p>
<ul>
<li>源代码是基于llama.cpp的，llama.cpp又要基于GGML，处理的模型文件是特别的gguf类型的模型文件，移植过去应该也只能处理这一类，而且尚且不知道这一类模型权重、激活文件处理起来会有什么不一样的地方。</li>
<li>需要一个额外训练的预测器，但预测器的代码似乎没有开源</li>
<li>机器学习库上，一个底层用的ggml，一个底层用的torch</li>
</ul>
<p><strong>大体思路：</strong></p>
<ul>
<li>看懂框架内部实现之间的组件关系，如果功能能像车轮一样简单解耦移植是最好的，否则就还要看看涉及的其他关联情况。</li>
<li>看看vllm框架怎么扩展其他功能，是要从什么级别的底层开始写功能。</li>
<li>先试试移植冷热神经元的判断入手</li>
</ul>
<h4 id="实践">实践</h4>
<p>基础知识内容：</p>
<ul>
<li><img src="/2025/01/25/vllm%E5%85%A5%E9%97%A8/image-20250226170707722.png" srcset="/img/loading.gif" lazyload class="" title="image-20250226170707722"></li>
<li><img src="/2025/01/25/vllm%E5%85%A5%E9%97%A8/image-20250227141723859.png" srcset="/img/loading.gif" lazyload class="" title="image-20250227141723859"></li>
<li><p><strong>input_tensor</strong> 的维度为[batch_size, seq_len,
hidden_dim]，<strong>线性层 W_q, W_k, W_v</strong>的维度为 [hidden_dim,
num_heads * head_dim]，这三个是正方形。矩阵乘法后得到的QKV矩阵再做
<strong>切分+维度交换</strong>，最后得到 [batch_size, num_heads,
seq_len, head_dim]。只从维度的角度上看其实就像把hidden_dim拆成了head_dim
* num_heads。</p></li>
<li><blockquote>
<p>为了实现神经元粒度的划分，我们对模型加载方式进行了改进，并提供了相关的稀疏算子。此外，我们还增强了对CPU和GPU算子的并行处理能力。</p>
</blockquote></li>
<li><p>根据DejaVu：注意力头中超过80%的部分会在实验中变得不活跃，而 MLP
层上的平均稀疏性则可以发现，超过95%的MLP参数都可以不参与推理</p></li>
<li><p>Powerinfer 的 online predictor 好像是直接复用的 DejaVu 开源的
predictor</p></li>
</ul>
<p>正在看/examples/main/main.cpp</p>
<p>先关注llama.cpp中的一些核心代码</p>
<ul>
<li>ggml-cuda-cuda.cu
中涉及核心矩阵运算，包含：ggml_cuda_op_mul_mat核心的矩阵运算</li>
<li>examples.cpp 逻辑流程部分是在 main.cpp（llama2 推理流程）中维护了
kv_cache 的 struct ，以及相关的支持操作函数（如
llama_kv_cache_seq_shift、llama_kv_cache_seq_rm 等）。</li>
<li>转换器 convert.py 脚本中
预测器的作用是什么？<strong>去看看PowerInfer源码(与llama.cpp不同）</strong></li>
<li>rms_norm_f32(ggml-cuda.cu中) 是处理的模型架构中的 RMS Norm</li>
</ul>
<h4 id="具体实践"><strong>具体实践：</strong></h4>
<ul>
<li>限定条件：单卡，模型是llama2的ggml版，预测器是已经训练好的。暂时不考虑量化、并行计算</li>
<li>根据vllm来看，或许要自己写一个对应架构的运行程序？</li>
<li>PowerInfer中较重要的：
<ul>
<li>main.cpp 是程序运行入口，有参数解析、初始化模型和上下文，180行
llama_init_from_gpt_params(实现 common.cpp中llama_init_from_gpt_params )
这部分</li>
<li>重头戏是 llama.cpp 中 llama_model_load 函数的
<strong>llm_laod_sparse_mdoel_tensors</strong>
做了稀疏推理下的模型加载（如果不是稀疏模型应该就是用原来的
llama.cpp）</li>
<li>把 <strong>llm_load_sparse_model_tensors</strong>
移植，尤其关注里面的 <strong>llm_load_gpu_split</strong>
，这里面又调用了两个函数，<code>llm_load_gpu_split_with_budget</code>和<code>llama_model_offload_ffn_split</code>。其中<code>llm_load_gpu_split_with_budget</code>主要负责加载GPU
Index（即哪些计算被卸载到GPU上），<code>llama_model_offload_ffn_split</code>则负责根据GPU
Index把每层的Feed Forward层按需加载到GPU上。</li>
</ul></li>
<li>vllm架构：
<ul>
<li><strong>加载过程：</strong><a
target="_blank" rel="noopener" href="https://blog.csdn.net/daihaoguang/article/details/141998001">vLLM
(4) - LLMEngine上篇</a> LLMEngine 会去产生
model_executor(ExecutorBase类) 。从 UniProcExecutor（继承自
ExecutorBase类），可以看到它使用了一个worker包装类并最后初始化了worker。
Worker 调用 model_runner 去加载模型，model_runner 里面的 load_model
是负责模型加载的，这个函数里面调用了 get_model 它会去启动
loader，先是去找模型文件格式所对应的加载类(比如
GGUFModelLoader，然后使用这个类下面实际使用的
load_model），在这个实际运行的子类里面它最后一步会进行真正的权重加载，也就是
**_get_weights_iterator** 函数(在 loader.py 里面)，往里走来到
weight_utils.py 文件查看这个函数，可以看到它的处理。</li>
<li>model_executor/model_loader/loader.py 里 可以看到官方做了 GGUF
模型文件的加载</li>
</ul></li>
<li>vllm里修改的地方：
<ul>
<li>参数加上一个参数用来启动一些内容？</li>
<li></li>
</ul></li>
</ul>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E7%A7%91%E7%A0%94-KV-Cache/" class="print-no-link">#科研, KV Cache</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>vllm入门记录</div>
      <div>https://royom.github.io/2025/01/25/vllm入门/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Roy</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年1月25日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/01/09/%E8%93%9D%E6%A1%A5%E6%9D%AF/python-notebook-main/README/" title="">
                        <span class="hidden-mobile"></span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>

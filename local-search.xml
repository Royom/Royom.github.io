<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>vllm入门记录</title>
    <link href="/2025/01/25/vllm%E5%85%A5%E9%97%A8/"/>
    <url>/2025/01/25/vllm%E5%85%A5%E9%97%A8/</url>
    
    <content type="html"><![CDATA[<h2 id="跑例程">1.跑例程</h2><h3 id="官方例程">1.1官方例程</h3><p>对vllm、SamplingParams这两个库有个初步印象</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> vllm <span class="hljs-keyword">import</span> LLM, SamplingParams<br><br><span class="hljs-comment"># Sample prompts.</span><br>prompts = [<br>    <span class="hljs-string">&quot;Hello, my name is&quot;</span>,<br>    <span class="hljs-string">&quot;The president of the United States is&quot;</span>,<br>    <span class="hljs-string">&quot;The capital of France is&quot;</span>,<br>    <span class="hljs-string">&quot;The future of AI is&quot;</span>,<br>]<br><span class="hljs-comment"># Create a sampling params object.</span><br>sampling_params = SamplingParams(temperature=<span class="hljs-number">0.8</span>, top_p=<span class="hljs-number">0.95</span>)<br><br><span class="hljs-comment"># Create an LLM.</span><br>llm = LLM(model=<span class="hljs-string">&quot;facebook/opt-125m&quot;</span>)<br><span class="hljs-comment"># Generate texts from the prompts. The output is a list of RequestOutput objects</span><br><span class="hljs-comment"># that contain the prompt, generated text, and other information.</span><br>outputs = llm.generate(prompts, sampling_params)<br><span class="hljs-comment"># Print the outputs.</span><br><span class="hljs-keyword">for</span> output <span class="hljs-keyword">in</span> outputs:<br>    prompt = output.prompt<br>    generated_text = output.outputs[<span class="hljs-number">0</span>].text<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Prompt: <span class="hljs-subst">&#123;prompt!r&#125;</span>, Generated text: <span class="hljs-subst">&#123;generated_text!r&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure><h3 id="例程2">1.2例程2</h3><p>网上的一个例程，在huggingface_hub调用的模型，参数、调用有一些区别，比如还指定了分词器用于然后规范化传入</p><p>不过都是model.generate</p><h3 id="vllm二次开发视频教程">1.3 vllm二次开发视频教程</h3><p>dtype精读设定可能影响你修改模型后的logits值</p><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs clean">model_executor/models/__init__.py <br>这个文件，相较视频里面的又有更改，原来似乎是指示了对应架构模型的相关文件以及相关类<br>现在的这个文件的作用是将一些重要的类和函数从不同的子模块导入，并通过 __all__ 使它们可以通过 <span class="hljs-keyword">from</span> <span class="hljs-keyword">module</span> <span class="hljs-keyword">import</span> * 的方式直接使用。<br>这些导入的类和函数涉及到模型的特性（如LoRA、池化、多模态等），并且为模型的管理和检查提供了工具。<br></code></pre></td></tr></table></figure><h3 id="继续看官方例程和官方文档">1.4 继续看官方例程和官方文档</h3><p><ahref="https://docs.vllm.ai/en/latest/design/v1/prefix_caching.html">官方文档</a></p><ol type="1"><li>llm.generate生成的是一个RequestOutput类型的对象，这个类的定义在vllm/outputs.py中，这个对象的一个核心字段outputs是一个CompletionOutput类型的列表（也在outputs.py中定义），outputs[0].text是输出的文本（如果有多个输出结果可能用到outputs[1].text等）</li></ol><h4 id="chat.py">1.4.1 chat.py</h4><p>这里面用到了批量推理，用了llm.chat而非llm.generate，顺路了解了一下这两个LLM方法的区别</p><h4 id="官方文档-生成模型">1.4.2 官方文档-生成模型</h4><p>LLM.chat接受OpenAI的API格式，需要使用的时候在官方文档-Models-GenerativeModels可以找到链接</p><h4 id="用vscode调试了一下">1.4.3 用VScode调试了一下</h4><p>看了一下output的结构</p><h4 id="官方文档-推理服务和表现">1.4.4 官方文档-推理服务和表现</h4><ol type="1"><li>Inference and Serving<ol type="1"><li>Offline Inference<ul><li>关于启动、参数配置可看此</li></ul></li><li>Distributed Inference and Serving<ul><li>暂时用不上分布式，需要可看</li></ul></li></ol></li><li>Performance<ol type="1"><li>Optimization and Tuning<ol type="1"><li>如果打印抢占的警告表明KV 空间不太够用，要用一些方式提升</li><li>提到了分块预填充的方式（似乎是FastGen的方法）</li></ol></li></ol></li></ol><h4 id="官方文档-架构">1.4.5 官方文档-架构</h4><ol type="1"><li><p>Design Documents</p><ol type="1"><li><p><strong>Architecture Overview</strong></p><ol type="1"><li>关系：<img src="/2025/01/25/vllm%E5%85%A5%E9%97%A8/image-20250207103304260.png" class="" title="image-20250207103304260"> <img src="/2025/01/25/vllm%E5%85%A5%E9%97%A8/image-20250207105504507.png" class="" title="image-20250207105504507"></li><li><figure><imgsrc="https://mmbiz.qpic.cn/mmbiz_png/VnDXQzNf28h86XrgCslRx0EBFHHbyt18F8ICtSl1yTmB6J72DcclI3TsIqtaw6NuZnyGGDAeVs9UhjHWyHOrqA/640?wx_fmt=png&amp;from=appmsg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1"alt="图片" /><figcaption aria-hidden="true">图片</figcaption></figure></li><li>展示了类之间的关系， 包括Worker、Model Runner、Model等</li></ol></li><li><p>Integration with Hugging face</p><ol type="1"><li>阐释了vllm如何找模型以及在model_executor/models里面去找到对应的文件来对自己初始化（根据config里面的architectures字段确定初始类，通过映射找到其对应的模型类，然后去找到相应初始化文件）</li><li>此外，Tokenizer和 Model weight也是来自于Hugging face<ol type="1"><li>Tokenizer的相关参数要看Huggingface的文档</li></ol></li><li>config.json可以来自本地、本地缓存orHugging face仓库</li></ol></li><li><p><strong>vLLM Paged Attention</strong></p><ol type="1"><li><blockquote><p>目前，vLLM 利用自己的多头查询注意内核实现（<code>csrc/attention/attention_kernels.cu</code>）。这个内核被设计为与vLLM的分页KV缓存兼容，其中键和值缓存存储在单独的块中（注意，这个块概念与GPU线程块不同。所以在后面的文档中，我将参考vLLM分页注意力block称为“块”，而将 GPU 线程块称为“线程块”）。</p></blockquote></li><li><p><code>scalar_t</code> 表示查询、键和值数据元素的数据类型， 例如FP16。 <code>HEAD_SIZE</code>表示每个头中的元素数量。<code>BLOCK_SIZE</code>指的是每个区块中的代币数量。<code>NUM_THREADS</code>表示每个线程块中的线程数。<code>PARTITION_SIZE</code>表示张量并行 GPU的数量（为简单起见，我们假设这是 0 并且张量并行被禁用）。</p></li><li><p><strong>解释了Sequence, context, vec, thread group, block , warp,thread block, grid</strong></p></li><li><p><strong>详细解释QKV这些变量的计算过程在代码中的实现，但是看得非常困难，不是很懂。</strong></p></li></ol></li><li><p><strong>Automatic Prefix Caching</strong></p><ol type="1"><li>这个之前看过，思路为主，现在是通过哈希映射来查找物理块，不用维护KV块之间的树结构。</li><li>谈了驱逐策略（LRU）</li></ol></li><li><p><ahref="https://docs.vllm.ai/en/latest/design/v1/prefix_caching.html"><strong>AutomaticPrefix Caching(in V1 Design Documents)</strong></a></p><ol type="1"><li><p>前缀缓存的实现是：<strong>KV cache manager</strong>.<code>class KVCacheBlock</code></p></li><li><p>初始化KV Cache manager的时候就分配了所有KVCacheBlock</p></li><li><p>Free block queue是空闲队列 <img src="/2025/01/25/vllm%E5%85%A5%E9%97%A8/image-20250207122328371.png" class="" title="image-20250207122328371"></p></li><li><p>请注意：只缓存完整的块</p></li><li><p>在<strong>vLLMv0</strong>版本中，当检测到<code>块3</code>是重复的时，它会释放块3并让Request2使用已经缓存的块1。因此，<code>块表</code>在Time1时会变成<code>[0, 1]</code>。</p><p>然而，在<strong>vLLMv1</strong>版本中，块表是追加式的（append-only），这意味着无法改变已经存在的块表。所以，<code>块表</code>会保持为<code>[0, 3]</code>，即使<code>块3</code>与<code>块1</code>重复。这个重复的块会在请求完成并释放时被清除。</p></li><li><p>要释放的块的顺序（比如一个请求对应块2348，添加到Freequeue的时候顺序是8432），因为更不容易被复用（可以理解为要完全匹配到的前缀更少？）</p></li><li><p><strong>这里有个非常好的示例来帮助理解 Request Blocks, CacheBlocks, BlockPool(list)与它的两个指针。</strong>有多张图片，选择其中一张作为范例<img src="/2025/01/25/vllm%E5%85%A5%E9%97%A8/image-20250207135439795.png" class="" title="image-20250207135439795"> 一个块用满的时候就会被缓存一旦没有请求正在使用，就会从Cache Blocks踢出去到BlockPool(list)，但是没有立即释放 请求申请块是从Block Pool(list)申请的，如果同一时间有其他请求相同前缀的块会复用，不会额外申请。请求申请块前如果Cache Blocks没有，那就在Block Pool(list)中touch，找到相同块就会把它们拿走（比如这里的0,1,2)，然后再从BlockPool(list)里申请块</p></li></ol></li></ol></li></ol><h4 id="其余博客">1.5 其余博客</h4><h5 id="vllm源码解析2-调度器策略">1.5.1<ahref="https://agijuejin.feishu.cn/wiki/XEncwa9s0iF3fjkODlacbdLhnLq">vllm源码解析2-调度器策略</a></h5><img src="/2025/01/25/vllm%E5%85%A5%E9%97%A8/image-20250210150655663.png" class="" title="image-20250210150655663"><ul><li><p><ahref="https://zhuanlan.zhihu.com/p/654910335">[猛猿：图解大模型计算加速系列：vLLM源码解析](https://zhuanlan.zhihu.com/p/691045737)</a>：有五篇，涵盖的内容非常广，对源码做了一个比较深入的解读，SequenceGroup,Scheduler，Step...而且有不少结构图，虽然更新后有一些改变了但还是很值得一读。</p><ul><li><p><strong>"1个prompt -&gt;多个outputs"这样的结构组成一个<code>SequenceGroup</code>实例。在vLLM中有一个重要假设：一个seq_group中的所有seq共享1个prompt。</strong></p></li><li><p><strong>我们要记住vLLM调度中非常重要的一点：在1个推理阶段中，所有的seq_group要么全部处在prefill阶段。要么全部处在decode阶段。</strong></p></li><li><p>running队列中seq_group下的n个seqs在上1个推理阶段共生成了n个token。在本次调度中，我们要先为这n个token分配物理块空间，用于存放它们在本次调度中即将产生的KV值。<strong>对于1个seq来说，最坏的情况就是添加1个物理块；对于n个seqs来说，最坏的情况就是添加n个物理块（想想原理篇中讲过的copy-on-write机制）</strong></p></li><li><p><strong>waiting队列中所有的seq_group都没做过prefill，因此每个seq_group下面只有1条seq</strong>，这个seq即位prompt本身，所以我们取[0]即可拿出这个prompt</p></li><li><p>以下按prefix cached和prefix uncache的情况做区分：</p></li><li><p>uncached：分配物理块做<strong>PreFill</strong>的时候：由于seq_group下的所有seq共享一个prompt，所以进一步令物理块的ref_count = num_seqs（表示这些seqs的逻辑块都引用它了）。</p><ul><li>在使用<code>UncachedBlockAllocator</code>为wating队列中的某个seq_group分配物理块时，我们就是在对初始的这个prompt分配物理块。所以这个prompt有多少个逻辑块，我们就分配多少个可用的空闲物理块，同时注意更新物理块的ref_count。PS:当前版本移除了这个类，但可能实质还在</li><li>你一定发现了，这里我们做的只是<strong>给定一种“物理块的分配方案”</strong>，我们只是在制定这个seq_group可以使用哪些物理块，<strong>但并没有实际往物理块中添加数据！“添加数据”这一步留到这1步推理实际开始时，由CacheEngine按照这个方案，往物理块中实际添加KVCache。这个我们留在再后面的系列讲解。</strong></li></ul></li><li><p>uncached：分配物理块做<strong>decode</strong>的时候：为running/swapped队列中的seq_group分配decode需要的物理块</p><ul><li><strong>调用<code>self.block_manager.can_append_slot(seq_group)</code>方法</strong>，判断是否至少能为这个seq_group下的每个seq都分配1个空闲物理块。如果可以则认为能调度这个seq_group（原因和代码分析我们在源码解读2中细讲过，这里不赘述）。</li><li><strong>调用<code>self._append_slot(seq_group, blocks_to_copy)</code>方法</strong>，实际分配物理块。</li><li>同样，在这里我们依然要强调，调度器中只是给出了物理块的分配方案，并没有实际往物理块中添加数据，添加数据这一步是CacheEngine照着这个方案来实际操作的，这个我们放在后面的文章中讲解。</li></ul></li><li><p>cached:prefill阶段的分配：</p><ul><li>hash_of_block(logical_idx) &amp;num.hashed_tokens_of_block(logical_idx)，根据前面到本块的token来得到hash值。<strong>当两个等待做prefill的seq拥有同样的hash值时，说明它们共享一样的prompt，这时就可以重复利用已有的KVcache</strong>（PS:不止涉及这两个方法）</li><li>代码中的相关prefixcache情况从表面上理解就是和我上方从官网截的一样</li></ul></li><li><p>cached: decode阶段的分配：</p><ul><li>以n=2的并行解码为例，在启动copy-on-write机制的同时，我们也要重新计算物理块的hash值。和prefill阶段不同，在decode阶段，当这个物理块还没满的时候，我们会给它附一个相互不重复的默认hash值。<strong>我们会把附上hash值的物理块加入CachedBlockAllocator的<code>cached_blocks</code>属性中当一个子序列用完当前物理块的所有slots时（例如当子序列1生成J0后），我们再对这个物理块重新做hash计算，计算方式是hash(A~J0)。</strong></li><li>decode阶段的prefixcaching不是一种频繁地复用，而是一种累积到一定范围尽可能地长段复用，这也更加方便做KVcache管理。</li></ul></li></ul></li><li><p><ahref="https://blog.csdn.net/weixin_42479327/article/details/141496484">vllm源码解析(一)：整体架构与推理代码</a>：有六篇0.54版本，</p></li></ul><h5 id="vllm源码-llmengine">1.5.2 <ahref="https://zhuanlan.zhihu.com/p/643336063">vllm源码LLMEngine</a></h5><p>部分过时，思路大体还是一样</p><h5 id="一文详解vllm架构">1.5.3 <ahref="https://mp.weixin.qq.com/s?__biz=Mzk0ODY4MjU3MQ==&amp;mid=2247493266&amp;idx=3&amp;sn=4e4e28f60d7b29486ff798e9a63baa94&amp;chksm=c20953e3a73adb133b3400b6b4a77c53cbbffe052587dfb3ebcb57fa5180b77fbb2fd7aa7826#rd">一文详解vLLM架构</a></h5><p>Scheduler结构</p><figure><imgsrc="https://mmbiz.qpic.cn/mmbiz_jpg/VnDXQzNf28h86XrgCslRx0EBFHHbyt18n2dft1duO7c50nt37F0icZ6YoPl78iciahVuiamOzDibG79jWeiaicVUpjrkw/640?wx_fmt=jpeg&amp;from=appmsg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1"alt="图片" /><figcaption aria-hidden="true">图片</figcaption></figure><h3 id="自己看的一些源代码">1.6 自己看的一些源代码</h3><ul><li>core/block<ul><li>prefix_caching_block.py<ul><li><code>find_cached_blocks_prefix(self, block_hashes: List[int]) -&gt; List[int]</code>这个方法采用的二分法，找出<code>block_hashes</code>列表中所有已缓存且已计算的块的前缀部分。</li></ul></li></ul></li><li>core<ul><li>kv_cache_utils.py<ul><li><code>FreeKVCacheBlockQueue</code> 这个应该就是设计文档中提到的freeblock queue,它处理的对象是<code>KVCacheBlock</code>详情参考源码注释</li></ul></li></ul></li></ul><h2 id="目录结构">2.目录结构</h2><p>一张解释初始化和推理的一个大致流程模块图</p><img src="/2025/01/25/vllm%E5%85%A5%E9%97%A8/image-20250205094850863.png" class="" title="image-20250205094850863"><h3 id="vllm">2.1vllm</h3><ul><li>model_executor<ul><li>其中models是把每个模型都写了代码，如果要新的模型或许要再这里继续加代码</li><li>model_loader 把原来的pytorch(?)算子之类的转为vllm定义的 e.g.线性层</li></ul></li><li>executor<ul><li>都是基于executer_base.py这个基类进行的二次开发</li><li>有CPU、单GPU、分布式GPU、ray等方式</li></ul></li><li>主目录下的utils.py工具类包含内存管理、检测和测量，分布式计算中的通信以及其余四五个小型工具，以及比如随机初始化kvcache的调试工具。（应该是用于调试？）</li></ul><h2 id="动手实践">3.动手实践</h2><h3id="离线推理调用本地模型修改最大token数">1.离线推理+调用本地模型+修改最大token数</h3><ul><li><p>基于调用Huggingface上的模型进行改造，原来的需要login（密钥）</p></li><li><p>去Huggingface上下的文件只用：<strong>config.json, tokenizer.json,tokenizer_config.json, model.safetensors,special_tokens_map.json</strong></p></li><li><p>不过改了之后因为config.json的原因？max_seq_len又超大了（131072），这个是在<strong>tokenizer.json</strong>里定义的</p></li><li><p>我在vllm serve的参数选择里看到说：</p><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs gams">--<span class="hljs-built_in">max</span>-<span class="hljs-keyword">model</span>-len<br><span class="hljs-keyword">Model</span> context length. <span class="hljs-keyword">If</span> unspecified, will be automatically derived from the <span class="hljs-keyword">model</span> config.<br></code></pre></td></tr></table></figure><p><del>所以我推测应该只能在tokenizer.json中修改，vllm的LLM和Samplingparams都没有最大输入上下文这个参数</del></p></li><li><p>在LLM参数里设置这个max_model_len =65536，虽然我在llm.py里面没有找到（其他的，但是它就是能运行），而且vllmserve也是这个参数</p></li></ul><h3 id="多轮对话">2.多轮对话</h3><h4 id="offline">2.1 offline</h4><ul><li>Examples里面有个prefix_caching.py，指出LLM参数里要设置enable_prefix_caching=True</li><li>Benchmark里面有个benchmark_prefix_caching.py，是官方用语测试的脚本，准备在这个上面改，必须指定模型，最好指定数据集（否则随机生成）</li><li>谈一下对这个benchmark_prefix_caching.py的代码的一部分理解：它调用了FlexibleArgumentParser这个在utils.py中的工具，使之能传入多种参数控制，我用到的如下所示，</li></ul><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs jboss-cli">    运行的使用命令<br><br>修改参数做测试，output-len input-length-range <br>python <span class="hljs-string">/home/roy/projects/test/new_benchmark_prefix_caching.py</span> \<br>        <span class="hljs-params">--model</span> <span class="hljs-string">/home/roy/models/meta-llamaLlama-3.2-1B-Instruct</span> \<br>        <span class="hljs-params">--dataset-path</span> <span class="hljs-string">/home/roy/models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json</span> \<br>        <span class="hljs-params">--max-model-len</span> 32768 \<br>        <span class="hljs-params">--num-prompts</span> 40 \<br>        <span class="hljs-params">--output-len</span> 3200 \<br>        <span class="hljs-params">--repeat-count</span> 4 \<br>        <span class="hljs-params">--input-length-range</span> 2048<span class="hljs-function">:3172</span> \<br>        <br>        <br>        <span class="hljs-params">--no-enable-prefix-caching</span><br>        <span class="hljs-params">--enable-prefix-caching</span><br><br>python benchmark_throughput.py \<br>        <span class="hljs-params">--model</span> <span class="hljs-string">/home/roy/models/meta-llamaLlama-3.2-1B-Instruct</span> \<br>        <span class="hljs-params">--dataset-path</span> <span class="hljs-string">/home/roy/models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json</span> \<br>        <span class="hljs-params">--max-model-len</span> 32768 \<br>        <span class="hljs-params">--num-prompts</span> 40 \<br>        <span class="hljs-params">--output-len</span> 3200 \<br>        <span class="hljs-params">--repeat-count</span> 4 \<br>        <span class="hljs-params">--input-length-range</span> 2048<span class="hljs-function">:3172</span> \<br>        <span class="hljs-params">---cpu-offload-gb</span> 16<br>        <span class="hljs-params">--no-enable-prefix-caching</span><br>        <span class="hljs-params">--enable-prefix-caching</span><br><br>python benchmarks/benchmark_throughput.py <span class="hljs-params">--output-len</span> 16 <span class="hljs-params">--input</span> 16 <span class="hljs-params">--model</span> <span class="hljs-string">/home/roy/models/meta-llamaLlama-3.2-1B-Instruct</span> <span class="hljs-params">--num-prompts</span> 256<br><br>-cpu-offload-gb 16<br><br>设置用不用V1：<br>~<span class="hljs-string">/.bashrc</span> <span class="hljs-comment">#永久设置文件在这里</span><br><span class="hljs-keyword">echo</span> $VLLM_USE_V1 <span class="hljs-comment">#检查</span><br>export VLLM_USE_V1=1 <span class="hljs-comment">#临时设置</span><br><span class="hljs-keyword">unset</span> VLLM_USE_V1 <span class="hljs-comment">#临时取消</span><br></code></pre></td></tr></table></figure><p><strong>结果</strong>：</p><p>增大--input-length-range，指定--output-len 100，</p><p>接下来的一些测试，没有指出则默认继承前面的测试数据（运行结果前为没开前缀缓存，后为开了前缀缓存）：</p><ol type="1"><li><p>将output-len 指定为15000，平均prompt600左右：运行结果648s/453s,</p></li><li><p>将input-length-range指定为 2048:4096，平均prompt2700，运行结果1665s/1000s，</p></li><li><p>将repeat次数削减为1，prompt数量提升至40，我想知道上面明显的效果是不是因为repeat复用的多。运行结果896s/804s。</p></li><li><p>将prompt的数量进一步提升至80，运行结果1817s/1969s。</p></li><li><p>将prompt的数量降低至20，运行结果为400s/340s</p></li><li><p>把prompt的数量提升至120。prefixcache的hit率在过程中从3.2上升到9.7%，运行结果为<strong>2636s/2737s</strong></p></li><li><p>将prompt的数量降低至10，repeat增加至2，运行结果为281s/280s。这个时候GPUkv cache使用率只有50%左右，感觉不觉比很好的参考价值。</p></li><li><p>将prompt的数量提升至20，repeat保持为2，运行结果为717s/661s</p></li><li><p>将prompt的数量提升至120，repeat降低为1，输出token数降低为8192，看看相比之前保持长输入但是缩短输出的结果。运行结果为1006s/1031s</p></li><li><p>将输出token数升至20480，看看长输入和长输出会不会拉开差距。运行结果为4746s/5075s</p></li><li><p>升级至0.72+V1，采用9的“prompt的数量提升至120，repeat降低为1”。但是中间有不少抢占，而且运行结果为/1164s</p></li><li><p>重启后又运行一次竟然因为显存不足（需要2G满足65536的最大token而只剩1.77G)。降低max-model-len为32768方运行。但是运行时间非常之久。1620s/1339s。发现这个数据比之前的远多，又去测试一下0.72的v0版本的效果，为1606s/1368s（这次prefixhit rate到了15%左右）</p></li><li><p>准备对比V0和V1在高命中率的情况。2048:3172 output-len 4096--repeat-count 4 V0: 769s/625s 命中率从75降低到64v1：627s/531s。再测试了一下把输出继续改小至3200tokens。455s/416s</p></li></ol><p>观察：</p><ul><li>当token数较大的时候，最开始迅速处理prompt，处理好后开始生成，KVcache迅速增长，占用大量内存，然后本来runningreqs很多，逐渐下降，因为空间不够，就变成了Pendingreqs，接下来算是一个循环吧，当结束一些请求的时候，如果腾出了不少空间，则既处理一部分prompt（我猜测是调度策略当觉得KV空间足够的时候就会处理新的prompt），然后把一部分pendingreqs转移到running reqs，随着KV占用继续增加又会转移至一部分至pendingreqs(这里的转移策略是什么？)直到有一些running reqs运行结束。</li><li>当启用前缀缓存且input-length-range较大的时候，或许是因为能共用较多的KVcache，可以看到最开始并行处理的Reqs是比较多的</li></ul><h3 id="powerinfer与vllm的融合">3.PowerInfer与vllm的融合</h3><h4 id="powerinfer的运行">0.PowerInfer的运行</h4><p>rm -rf build 移除Cmake的build缓存后终于找到CUDA了</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs stylus">运行命令<br>./build/bin/<span class="hljs-selector-tag">main</span> -m ./models/ReluLLaMA-<span class="hljs-number">7</span>B/llama-<span class="hljs-number">7</span>b-relu<span class="hljs-selector-class">.powerinfer</span><span class="hljs-selector-class">.gguf</span> -n <span class="hljs-number">256</span> -t <span class="hljs-number">8</span> -<span class="hljs-selector-tag">p</span> <span class="hljs-string">&quot;Introduce yourself&quot;</span><br>./build/bin/<span class="hljs-selector-tag">main</span> -m ./models/ReluLLaMA-<span class="hljs-number">70</span>B/llama-<span class="hljs-number">70</span>b-relu<span class="hljs-selector-class">.q4</span><span class="hljs-selector-class">.powerinfer</span><span class="hljs-selector-class">.gguf</span> -n <span class="hljs-number">256</span> -t <span class="hljs-number">8</span> -<span class="hljs-selector-tag">p</span> <span class="hljs-string">&quot;Introduce yourself&quot;</span><br></code></pre></td></tr></table></figure><p>论文阅读简单记录</p><ul><li>简单说：幂律分布的神经元被类似的预测器分为 hot/cold 等类型，hot神经元在 GPU 上算，cold 神经元在 CPU 上算。</li><li>问题：<ul><li>稀疏模型在batch size &gt; 1时会有问题？</li><li>只有稀疏模型（乃至Relu）能用？</li></ul></li><li>之后可能再利用到的知识：<ul><li>在32批次以内，相比分配给CPU需要计算再挪给GPU计算，直接在CPU计算然后返回结果更快</li><li>不同层的热神经元不一样，不同模型差的也会比较多，不同任务也不一样。</li><li>似乎前面的层热神经元会少一些。</li><li>作者指出：各种领域中，前20％最常激活的神经元中有超过90％的重叠</li><li>论文做了一个可以单独计算行列的算子，而不需要把稀疏矩阵先转为密集矩阵</li><li>MLP块(PowerInfer主要是这个)、自注意力块都存在稀疏激活特性</li></ul></li></ul><h4 id="参考的资料"><strong>参考的资料：</strong></h4><ul><li><a href="https://zhuanlan.zhihu.com/p/671761052">刀刀宁llama.cpp源码分析</a>：了解llama.cpp架构必看，关注核心函数。</li><li><ahref="https://alleny.xyz/post/powerinfer-source-analysis-1/">PoweInfer源码解析</a>：一个人写的博客，可以看PowInfer做了什么更改。</li><li><a href="https://zhuanlan.zhihu.com/p/665027154">Codelearnerllama.cpp源码解析</a>2023-11的源码解析，包含代码结构、调用流程、算子解析</li><li><ahref="https://github.com/ggml-org/ggml/blob/master/docs/gguf.md">github上的 gguf.md</a> 描述了<strong>模型文件的格式</strong></li><li><ahref="https://blog.csdn.net/daihaoguang/article/details/141998001">vLLM(4) - LLMEngine上篇</a> 这个系列偏新24.9，<strong>类图</strong>很适合理解初始化和推理过程</li></ul><p><strong>可能存在的问题：</strong></p><ul><li>源代码是基于llama.cpp的，llama.cpp又要基于GGML，处理的模型文件是特别的gguf类型的模型文件，移植过去应该也只能处理这一类，而且尚且不知道这一类模型权重、激活文件处理起来会有什么不一样的地方。</li><li>需要一个额外训练的预测器，但预测器的代码似乎没有开源</li><li>机器学习库上，一个底层用的ggml，一个底层用的torch</li></ul><p><strong>大体思路：</strong></p><ul><li>看懂框架内部实现之间的组件关系，如果功能能像车轮一样简单解耦移植是最好的，否则就还要看看涉及的其他关联情况。</li><li>看看vllm框架怎么扩展其他功能，是要从什么级别的底层开始写功能。</li><li>先试试移植冷热神经元的判断入手</li></ul><h4 id="实践">实践</h4><p>基础知识内容：</p><ul><li><img src="/2025/01/25/vllm%E5%85%A5%E9%97%A8/image-20250226170707722.png" class="" title="image-20250226170707722"></li><li><img src="/2025/01/25/vllm%E5%85%A5%E9%97%A8/image-20250227141723859.png" class="" title="image-20250227141723859"></li><li><p><strong>input_tensor</strong> 的维度为[batch_size, seq_len,hidden_dim]，<strong>线性层 W_q, W_k, W_v</strong>的维度为 [hidden_dim,num_heads * head_dim]，这三个是正方形。矩阵乘法后得到的QKV矩阵再做<strong>切分+维度交换</strong>，最后得到 [batch_size, num_heads,seq_len, head_dim]。只从维度的角度上看其实就像把hidden_dim拆成了head_dim* num_heads。</p></li><li><blockquote><p>为了实现神经元粒度的划分，我们对模型加载方式进行了改进，并提供了相关的稀疏算子。此外，我们还增强了对CPU和GPU算子的并行处理能力。</p></blockquote></li><li><p>根据DejaVu：注意力头中超过80%的部分会在实验中变得不活跃，而 MLP层上的平均稀疏性则可以发现，超过95%的MLP参数都可以不参与推理</p></li><li><p>Powerinfer 的 online predictor 好像是直接复用的 DejaVu 开源的predictor</p></li></ul><p>正在看/examples/main/main.cpp</p><p>先关注llama.cpp中的一些核心代码</p><ul><li>ggml-cuda-cuda.cu中涉及核心矩阵运算，包含：ggml_cuda_op_mul_mat核心的矩阵运算</li><li>examples.cpp 逻辑流程部分是在 main.cpp（llama2 推理流程）中维护了kv_cache 的 struct ，以及相关的支持操作函数（如llama_kv_cache_seq_shift、llama_kv_cache_seq_rm 等）。</li><li>转换器 convert.py 脚本中预测器的作用是什么？<strong>去看看PowerInfer源码(与llama.cpp不同）</strong></li><li>rms_norm_f32(ggml-cuda.cu中) 是处理的模型架构中的 RMS Norm</li></ul><h4 id="具体实践"><strong>具体实践：</strong></h4><ul><li>限定条件：单卡，模型是llama2的ggml版，预测器是已经训练好的。暂时不考虑量化、并行计算</li><li>根据vllm来看，或许要自己写一个对应架构的运行程序？</li><li>PowerInfer中较重要的：<ul><li>main.cpp 是程序运行入口，有参数解析、初始化模型和上下文，180行llama_init_from_gpt_params(实现 common.cpp中llama_init_from_gpt_params )这部分</li><li>重头戏是 llama.cpp 中 llama_model_load 函数的<strong>llm_laod_sparse_mdoel_tensors</strong>做了稀疏推理下的模型加载（如果不是稀疏模型应该就是用原来的llama.cpp）</li><li>把 <strong>llm_load_sparse_model_tensors</strong>移植，尤其关注里面的 <strong>llm_load_gpu_split</strong>，这里面又调用了两个函数，<code>llm_load_gpu_split_with_budget</code>和<code>llama_model_offload_ffn_split</code>。其中<code>llm_load_gpu_split_with_budget</code>主要负责加载GPUIndex（即哪些计算被卸载到GPU上），<code>llama_model_offload_ffn_split</code>则负责根据GPUIndex把每层的Feed Forward层按需加载到GPU上。</li></ul></li><li>vllm架构：<ul><li><strong>加载过程：</strong><ahref="https://blog.csdn.net/daihaoguang/article/details/141998001">vLLM(4) - LLMEngine上篇</a> LLMEngine 会去产生model_executor(ExecutorBase类) 。从 UniProcExecutor（继承自ExecutorBase类），可以看到它使用了一个worker包装类并最后初始化了worker。Worker 调用 model_runner 去加载模型，model_runner 里面的 load_model是负责模型加载的，这个函数里面调用了 get_model 它会去启动loader，先是去找模型文件格式所对应的加载类(比如GGUFModelLoader，然后使用这个类下面实际使用的load_model），在这个实际运行的子类里面它最后一步会进行真正的权重加载，也就是**_get_weights_iterator** 函数(在 loader.py 里面)，往里走来到weight_utils.py 文件查看这个函数，可以看到它的处理。</li><li>model_executor/model_loader/loader.py 里 可以看到官方做了 GGUF模型文件的加载</li></ul></li><li>vllm里修改的地方：<ul><li>参数加上一个参数用来启动一些内容？</li><li></li></ul></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>科研, KV Cache</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2025/01/09/%E8%93%9D%E6%A1%A5%E6%9D%AF/python-notebook-main/README/"/>
    <url>/2025/01/09/%E8%93%9D%E6%A1%A5%E6%9D%AF/python-notebook-main/README/</url>
    
    <content type="html"><![CDATA[<h1 id="蓝桥杯笔记">蓝桥杯笔记</h1><p>准备蓝桥杯python组时的代码及结果，供时间不太够的同学冲击省一。博客内容的.ipynb文件版本，可以直接跑，更方便做笔记。</p><p>博客指路：https://blog.csdn.net/wtyuong/article/details/124540468</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>蓝桥杯备赛</title>
    <link href="/2025/01/09/%E8%93%9D%E6%A1%A5%E6%9D%AF%E5%A4%87%E8%B5%9B/"/>
    <url>/2025/01/09/%E8%93%9D%E6%A1%A5%E6%9D%AF%E5%A4%87%E8%B5%9B/</url>
    
    <content type="html"><![CDATA[<h1 id="参考资料">参考资料</h1><ol type="1"><li><ahref="https://www.bilibili.com/video/BV12rCVYeErw/?spm_id_from=333.1007.top_right_bar_window_default_collection.content.click&amp;vd_source=8ee65aba2fbc74db17c49168d65cc4be">30分钟过完蓝桥杯常见知识点</a>罗列了知识框架</li><li><ahref="https://blog.csdn.net/m0_53192838/article/details/129372894">Acwing算法基础课涉及的题目</a></li></ol><h1 id="知识框架省赛">知识框架(省赛)：</h1><ol type="1"><li>基础算法（循环、递归、前缀和、差分、双指针+、 STL用法）</li><li>排序（桶排序+、快速排序）</li><li>搜索 (DFS, BFS, 剪枝，记忆化）</li><li>贪心（常见贪心模型）</li><li>二分（二分数组、二分答案）</li><li>动态规划+（背包DP，状压DP, 树形 DP, DP 简单优化）</li><li>数据结构（树状数组+，ST表，手写栈、队列、链表，单调栈+、单调队列+）</li><li>图论（并查集， LCA ，Dijkstra, FIoyd)</li><li>数学（唯一分解定理+， GCD 和 LCM，快速幂，乘法逆元组合数，欧拉筛法+）</li><li>字符串（ kmp 会背就行， Hash, 马拉车）</li></ol><h1 id="学习内容">学习内容：</h1><h2 id="第一节-递归">第一节 递归</h2><p>2025.1.10,做了一道作业一道练习题，重新回顾了一下python，第二道超时了</p><p>2025.2.19 又开始学习，当前进度1.2看到01:19:32</p><h3 id="第二节-二分与前缀和">第二节 二分与前缀和</h3><p>在<strong>整数二分</strong>中，如果ans在红半段的终点(L=M)，就要M =(L+R+1)；如果是绿半段的起点(R=M)就不用</p><p><strong>实数二分</strong>中间的位置一定有数，不用担心</p><p><strong>模板最好背下来。</strong></p><p><img src="/2025/01/09/蓝桥杯备赛/image-20250224161022282.png"  alt="image-20250224161022282" style="zoom:50%;" /></p><img src="/2025/01/09/%E8%93%9D%E6%A1%A5%E6%9D%AF%E5%A4%87%E8%B5%9B/image-20250224172129139.png" class="" title="image-20250224172129139"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">算法模板：<br>l,r = -<span class="hljs-number">1</span>, n<br><span class="hljs-keyword">while</span> l + <span class="hljs-number">1</span>  != r:<br>    m = (l + r) // <span class="hljs-number">2</span><br>    <span class="hljs-keyword">if</span> IsBlue(m):<br>        l = m<br>    <span class="hljs-keyword">else</span>:<br>        r = m<br><span class="hljs-keyword">return</span> l <span class="hljs-keyword">or</span> r<br></code></pre></td></tr></table></figure><p>-1 3 4 4 6</p><h2 id="python基础相关">Python基础相关</h2><ul><li><p>def函数内的函数可以访问外面一层函数的变量，但需要修改就需要用<code>nonlocal</code>来声明，而且必须是先声明再修改，不能声明的同时修改；全局变量应该也是一样只是用<code>global</code></p></li><li><p><strong>开关问题</strong>可以用二进制来表示<code>常用 &lt;&lt; 以及 ^= &amp; |=</code></p></li><li><p>为了前缀和表示方便可以手动加 [0]</p></li><li><p>假如要求的整数A是小于浮点数B，可以让他等效于小于等于 B 向上取整再-1 如果是大于浮点数B，可以让他等效为 大于等于 B 向下取整再 +1</p></li></ul><h2 id="经典模型">经典模型</h2><p><strong>货仓选址/老马喝水</strong>：找一个点使得数轴上各个点到该点的距离之和最小。结论：奇数情况是中位数，偶数是中间两个数取平均值</p><h2 id="每日一题">每日一题</h2><p>蛋糕游戏：两个各自都要理论上要最优策略最后就让情况只剩下一种了，这种题不能假设两边的行动逻辑来做。不过理解完他们的行动策略之后有一个疑问，这个行动策略确实能保证吃的蛋糕数量上贝茜是最少的，但是为什么这就是最多的呢？</p><p>奶牛体检：<strong>遍历每个点作为中心点</strong>，然后<strong>同时考虑奇数区间和偶数区间</strong>。和暴力枚举在第一步都是O(N^2)，但是在枚举后的处理从O(N)降低到了O(1)</p><p>农夫约翰最喜欢的操作：新知识：<strong>破环成链优化(空间换时间，额外扩展数组并计算前缀和)，货仓选址模型</strong>，这里还有数学推导，便于计算。想到破环成链后还要反向用枚举来找到最优的x。</p><p>平衡细菌：做一次差分(前一个数减后一个数)，近似于等于求导一次</p><p>牛奶交换：还是破环成链</p><p>最大限度提高生产力：从数学表达式来说，挪动一个值或许有不一样的思路。二分模板是必背啊。</p><p>哞语言逻辑：有段的时候考虑一下前缀和</p><p>沿栅栏散步：同意是有段，因为xy的取值是≤10^3，所以可以用前缀和来便捷的找长度。</p><p>空调：二进制枚举，</p><h3 id="工具库">工具库</h3><ul><li>itertools.accumulate(列表)：可以自动遍历生成前缀和列表，不过返回的是迭代器（懒加载），所以一般来说要list(itertools.accumulate(data))</li><li>from collections import defaultdict 初始化字典 举例：bag =defaultdict(int)</li><li>import sys 标准化输入输出（一次性批量）</li><li>采取输入和打印的交互：<ul><li>把输入转为列表的形式(stdin.read读取所有 input读取一行）：</li><li>data = sys.stdin.read().split / data = input().split()不过read会读取到换行符 ""，所以用.split()可以消去<code>input()</code>看情况使用<code>input().split</code> 或者是<code>list(input())</code> 因为后者会把比如说 "1 2 3" 读取为 ['1', ' ','2', ' ', '3']<ul><li>平常一般用的： list(map(int, input())) 输入1 2 3 可以得到 [1, 2,3]</li></ul></li><li>输出（平常一般用的）<ul><li>1.输出列表中的结果并且用换行符分开：sys.stdout.write("".join(results)+'"")</li><li>先把输出的结果添加到一个列表里面，并且是用f按变量添加的用results.append(f"{min_bessie} {min_elisie})：</li><li></li></ul></li></ul></li><li>abs() 取绝对值</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2024/12/23/DeFT%EF%BC%9A%E4%BD%BF%E7%94%A8Flash%E6%A0%91%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E8%A7%A3%E7%A0%81%EF%BC%8C%E4%BB%A5%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%95%88%E7%9A%84%E6%A0%91%E7%BB%93%E6%9E%84LLM%E6%8E%A8%E7%90%86/"/>
    <url>/2024/12/23/DeFT%EF%BC%9A%E4%BD%BF%E7%94%A8Flash%E6%A0%91%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E8%A7%A3%E7%A0%81%EF%BC%8C%E4%BB%A5%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%95%88%E7%9A%84%E6%A0%91%E7%BB%93%E6%9E%84LLM%E6%8E%A8%E7%90%86/</url>
    
    <content type="html"><![CDATA[<p>论文原址：<a href="https://arxiv.org/abs/2404.00242">DeFT: Decodingwith Flash Tree-attention for Efficient Tree-structured LLMInference</a></p><h1 id="前置知识">前置知识：</h1><ol type="1"><li><p><ahref="https://zhuanlan.zhihu.com/p/31558973">深度学习中GPU和下线存的关系</a>粗看了一下，之后看优化可以再看</p></li><li><p>QKV一般情况：<strong>Q（3,emb_size) K<sup>T</sup>(emb_size,3)</strong>相乘得到<strong>表示输入序列中一个元素对领一个元素的注意力程度的注意力得分矩阵QK<sup>T</sup>(3,3)</strong>再与VALUE(3, emb_size)点乘得到Attention(3, emb_size)</p></li><li><p><ahref="https://juejin.cn/post/7340825900421333004#heading-15">推理优化</a>对全过程</p></li><li><p><ahref="https://blog.csdn.net/shui123546yi/article/details/134823641">LLMbatch</a>：不止于batch，介绍了一些llm inference的基础知识</p><blockquote><p>LLM服务的性能受到内存的限制，为内存和IO受限型memory-IObound，计算资源不是瓶颈。就是说，当前将1MB的数据加载到GPU的计算核心所花费的时间比这些计算core对1MB数据执行LLM计算所花费的更多。这意味着LLM推理吞吐量在很大程度上取决于您可以将多大的batch放入高带宽GPU内存。参见(understand-perf），以了解更多详细信息。</p></blockquote></li><li><p><ahref="https://blog.csdn.net/jinzhuojun/article/details/139693922">AI推理优化</a>，涵盖flash-decode,2024-11-30，时效性强，内容扎实，涵盖面广<strong>值得再看</strong></p><p>里面提到的一些KVCache相关优化论文：</p><ul><li><strong>对于request间有相同的prefix的情况，将KVCache缓存下来可以避免重复计算。</strong>这就是Prefixcaching的基本思想。比如论文《Improving Large Language Model Throughputwith Efficient Long-Term Memory Management》基于PagedAttention提出prefixcache，该prefix cache可被swap到CPU与disk。<ahref="https://zhuanlan.zhihu.com/p/660192497">别人的解读和总结</a>。进一步的vattention再优化了vllm</li></ul></li><li><p><ahref="https://blog.csdn.net/javastart/article/details/137948164">KVCache加速策略分类</a>，KVcache主要分成5个方向的优化，即<strong>Sparse、Quantization、Allocator、Window、share</strong>用于构建整体框架</p></li><li><p><ahref="https://fancyerii.github.io/2023/10/23/flashattention/">详解Flash-attention，它翻译过来的原链接也非常值得一看</a></p><ul><li>理解了"kernel fusion":其实就是把本来在SRAM和HBM中往返的操作写在一起了</li><li>一般的Pytorch里面，masking, softmax,dropout占用了大部分时间，尽管矩阵计算涉及大量flops，但是花的时间却很少</li><li>FlashAttention的重要思想：Tilling(前向和后向传递中使用；简单来讲就是将N*N的Softmax/分数矩阵划分成块)+重新计算(仅在后向传递中使用)</li></ul></li><li><p><ahref="https://blog.csdn.net/v_JULY_v/article/details/89894058">RNN，LSTM详解</a>：非常顶级易懂的解释，如果有相关问题可以再看这个博客</p></li><li><p><ahref="https://blog.csdn.net/v_JULY_v/article/details/133619540">非常详细的Flash-Attention讲解</a>：详细到看不懂，脱离我现在的需求了，但这个博主的文章都比较深入</p></li><li><p><ahref="https://blog.csdn.net/qq_43814415/article/details/140019196">大模型推理知识总结</a>2024-7-15总结的，还比较全面</p></li><li><p>对我来说的新词汇：</p><ul><li>树状注意力</li><li></li></ul></li><li></li></ol><h1 id="一.概要">一.概要</h1><ul><li><strong>背景：</strong>大型语言模型（LLMs）被越来越多地用于处理<strong>具有共享前缀token的树结构</strong>中的多生成调用任务，例如少样本提示、多步推理、推测性解码等。</li><li><strong>问题</strong>：现有的<strong>树基应用推理系统</strong>效率不高，因为<strong>在注意力计算期间对查询和KV缓存的分区不当</strong>，导致两个主要问题：<strong>(1)KV缓存的内存访问（IO）重用不足；(2) 负载均衡差。</strong></li><li><strong>解决方案</strong>：提出了DEFT，这是一种<strong>硬件高效</strong>的注意力算法，具有<strong>前缀感知和负载平衡的KV缓存分区</strong>。通过<strong>KV引导分组和扁平树KV分割机制</strong>，减少了注意力计算期间KV缓存的读写操作，并提高了GPU利用率。</li><li><strong>结果</strong>：DEFT在三个实际的树基工作负载中，相比于最先进的注意力算法，实现了高达2.52/3.82倍的端到端/注意力延迟加速。</li></ul><blockquote><p>"具有共享前缀token的树结构"这个是什么？</p><p>树基应用推理系统 这个是什么？</p><p>硬件高效？</p></blockquote><h3 id="更具体的描述">更具体的描述：</h3><h4 id="动机">1.动机</h4><p>现在有许多应用被设计为<strong>处理具有内部树结构的序列</strong>，比如：includingself-consistency(<strong>自洽</strong>) (Wang et al., <ahref="https://arxiv.org/html/2404.00242v3#bib.bib39">2022</a>), few-shotprompting(<strong>少样本提示</strong>) (Mann et al., <ahref="https://arxiv.org/html/2404.00242v3#bib.bib25">2020</a>),multi-step reasoning (<strong>多步推理</strong>)(Yao et al., <ahref="https://arxiv.org/html/2404.00242v3#bib.bib44">2023</a>; Hao etal., <a href="https://arxiv.org/html/2404.00242v3#bib.bib11">2023</a>;Xie et al., <ahref="https://arxiv.org/html/2404.00242v3#bib.bib43">2024</a>), andspeculative decoding(<strong>推测解码</strong>) (Miao et al., <ahref="https://arxiv.org/html/2404.00242v3#bib.bib27">2023</a>; Cai etal., <a href="https://arxiv.org/html/2404.00242v3#bib.bib5">2024</a>),etc, as shown in Figure <ahref="https://arxiv.org/html/2404.00242v3#S1.F1">1</a>.<strong>通常，这些应用程序会比传统应用程序产生更多的token。</strong>我们需要一种更高效的解码算法来应对这种<strong>交互范式从基于序列的解码到基于树的解码</strong>的转变。</p><blockquote><p>有时间看看这些内部树结构的序列是怎么样的</p></blockquote><p>当请求在树结构中具有共享前缀时，为基于序列的解码而设计的现有推理系统，会因无法prefix-aware<code>下一个或多个前缀而引入冗余三个层次</code>：</p><ul><li>(1)计算——例如，一个批次里请求之间共享提示的KV缓存的冗余重新计算（ <ahref="https://arxiv.org/html/2404.00242v3#bib.bib16">Hugging Face，</a>） ；</li><li>(2)内存存储——例如，共享前缀的KV缓存的冗余存储( <ahref="https://arxiv.org/html/2404.00242v3#bib.bib16">Hugging Face,</a> ;Kwon et al., <ahref="https://arxiv.org/html/2404.00242v3#bib.bib20">2023</a> ; <ahref="https://arxiv.org/html/2404.00242v3#bib.bib29">NVIDIA,</a> );</li><li>(3)内存访问(IO) ——例如在注意力计算期间重复加载共享系统提示的KV缓存(<a href="https://arxiv.org/html/2404.00242v3#bib.bib16">HuggingFace,</a> ; Kwon et al., <ahref="https://arxiv.org/html/2404.00242v3#bib.bib20">2023</a> ; <ahref="https://arxiv.org/html/2404.00242v3#bib.bib29">NVIDIA,</a> )</li></ul><p>尽管一些基于树的推理系统（Zheng et al., <ahref="https://arxiv.org/html/2404.00242v3#bib.bib47">2023</a> ；Gim etal., <a href="https://arxiv.org/html/2404.00242v3#bib.bib9">2023</a>；Cai et al., <ahref="https://arxiv.org/html/2404.00242v3#bib.bib5">2024</a> ；Miao etal., <a href="https://arxiv.org/html/2404.00242v3#bib.bib27">2023</a>）解决了前两个问题，<strong>但它们在很大程度上忽略了第三个问题，可以说是最关键的方面：<em>内存访问</em></strong>，这在内存绑定的LLM推理中至关重要（Shazeer，<a href="https://arxiv.org/html/2404.00242v3#bib.bib33">2019</a> ；Cai等）等人， <ahref="https://arxiv.org/html/2404.00242v3#bib.bib5">2024</a> ；金等人，<a href="https://arxiv.org/html/2404.00242v3#bib.bib19">2023</a> ）。</p><blockquote><p>这些都是可以去看的相关文章</p><p>之前也有看到说现在速度不行的问题就是GPU&gt;IO吞吐，彼此佐证了。“LLM推理是受内存限制的(Shazeer, <ahref="https://arxiv.org/html/2404.00242v3#bib.bib33">2019</a> ; Kim etal., <a href="https://arxiv.org/html/2404.00242v3#bib.bib19">2023</a> ;Cai et al., <ahref="https://arxiv.org/html/2404.00242v3#bib.bib5">2024</a> )，其中每次前向传递都需要从内存传输所有模型参数和KV缓存。较慢但较大的高带宽内存(HBM) 到速度较快但较小的 GPU 共享内存”</p></blockquote><img src="/2024/12/23/DeFT%EF%BC%9A%E4%BD%BF%E7%94%A8Flash%E6%A0%91%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E8%A7%A3%E7%A0%81%EF%BC%8C%E4%BB%A5%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%95%88%E7%9A%84%E6%A0%91%E7%BB%93%E6%9E%84LLM%E6%8E%A8%E7%90%86/image-20241224141402803.png" class="" title="image-20241224141402803"><blockquote><p>这张图里提到了树型解码方式：自洽、少样本提示、多步推理（比如思维树）</p></blockquote><h4 id="问题">2.问题</h4><p>为了加速树结构LLM推理，一个<strong>重要的问题是我们是否可以利用多级联前缀中的共享模式来设计更快、更节省内存的注意力算法</strong>。由于以下<strong>两个关键问题</strong>，这项任务具有挑战性。</p><ul><li><strong>C1：如何保证KV缓存内存访问的前缀感知？</strong>当前的内存高效注意力算法（Daoet al., <a href="https://arxiv.org/html/2404.00242v3#bib.bib6">2022</a>; <a href="https://arxiv.org/html/2404.00242v3#bib.bib7">2023</a> ; Honget al., <a href="https://arxiv.org/html/2404.00242v3#bib.bib15">2023</a>）针对基于序列的解码进行了优化，这导致内存访问期间缺乏前缀意识。结果，KV缓存中的共享前缀被重复加载。</li><li><strong>C2：如何拆分树形结构的KV缓存以实现负载平衡和高GPU利用率？</strong>为了获得最佳的 GPU 利用率，当前基于序列的解码的 KV分割策略——Flash-Decoding (Dao et al., <ahref="https://arxiv.org/html/2404.00242v3#bib.bib7">2023</a> ) ，将序列KV 分割成块——不能直接应用于树结构的 KV。<strong>树结构的 KV缓存也需要有效分区</strong>：然而，如果我们天真地按节点分割它们，<strong>不同节点之间的令牌长度可能会有很大差异</strong>（例如，在推测性解码中（Caiet al., <a href="https://arxiv.org/html/2404.00242v3#bib.bib5">2024</a>），某些节点可能只具有1个令牌（而根节点可能有数千个），从而难以维持负载平衡和高效计算。</li></ul><blockquote><p>方法针对的点是看“多级联前缀中共同的部分”</p><p>这些都是可以去看的相关文章</p><p>这个基于序列解码的K分割策略也可以看看，虽然它说不能用于树结构。从本文标题看它似乎是本文的灵感来源</p><p>树结构的 KV 缓存也需要有效分区？</p><p>不同节点之间的令牌长度可能会有很大差异——动机</p></blockquote><h4 id="解决方案">3.解决方案：</h4><h3 id="kv-guided-groupingkv引导分组">1. KV-GuidedGrouping（KV引导分组）</h3><ul><li><strong>问题背景</strong>：在现有的注意力机制中，通常采用Q-GuidedGrouping（Q引导分组）策略，即每个查询（Query）与其对应的所有键值对（Key-Valuepairs，简称KV）进行分组。这种方法虽然减少了查询的输入输出（IO）冗余，但共享前缀的KV缓存仍然会被多次加载，导致效率不高。</li><li><strong>KV-GuidedGrouping</strong>：DEFT-Flatten提出了一种新的分组策略，即KV引导分组。在这种策略中，共享前缀的KV缓存与所有共享这个前缀的查询分组在一起。这样，共享前缀的KV缓存只被加载一次，显著减少了重复加载的开销。由于查询通常较短（例如，只有几十个token），与每个节点中KV缓存的长度（可能有数百或数千个token）相比，查询的IO开销是微不足道的。</li></ul><h3 id="flattened-tree-kv-splitting扁平树kv分割">2. Flattened Tree KVSplitting（扁平树KV分割）</h3><ul><li><strong>问题背景</strong>：由于LLM推理过程是IO密集型的，每个QKV组的注意力计算开销主要由KV缓存的IO决定。因此，为了提高效率，需要确保不同QKV组的KV长度几乎平衡。</li><li><strong>Flattened Tree KVSplitting</strong>：DEFT-Flatten提出了一种扁平树KV分割策略，通过将扁平化的树结构KV分割成均匀的块来实现平衡的分区。这种分割方法使用位因果掩码（bitcausalmasks）来记录查询和KV缓存之间的因果关系，从而在保持计算效率的同时，确保了不同QKV组之间的负载均衡。</li></ul><blockquote><p>现在看不懂，等看下文看看能不能看懂这两个方法</p></blockquote><h1 id="二.相关工作">二.相关工作</h1><ul><li><strong>基于树的解码：</strong>根据查询和KV的结构特征，可以将基于树的解码分为两种模式：1.通常用于多步推理的，具有并行查询的树结构past KV 2.常用于推测解码的，具有树结构Q+ 序列型Past KV</li></ul><blockquote><p>尽管基于树的搜索算法如 A* (Lu et al., <ahref="https://arxiv.org/html/2404.00242v3#bib.bib24">2022</a> )和Monte-Carlo Tree Search (Liu et al., 2022 )， <ahref="https://arxiv.org/html/2404.00242v3#bib.bib21">2023</a>）已经得到应用，但基于树的解码的效率在很大程度上仍未得到充分探索。</p><p>看了一下这里的MCTS用于的是工作调度</p></blockquote><p><strong>内存高效的注意力算法</strong>：现有的专注于序列，Flash移植到树的解码但是忽略了树结构KV缓存的IO冗余，本文针对了这一点改良。</p><blockquote><p>FlashAttention （Dao et al., <ahref="https://arxiv.org/html/2404.00242v3#bib.bib6">2022</a>）通过平铺和内核融合改进了LLM训练中的自注意力计算，减少了 IO。Flash-Decoding （Dao 等人， <ahref="https://arxiv.org/html/2404.00242v3#bib.bib7">2023</a>）扩展了这一点，通过划分 K 和 V来增强并行性，并引入全局归约来收集部分注意力结果，从而实现长序列的高效解码。</p></blockquote><ul><li><strong>树注意：</strong>树注意力集成到LLM推理中，减少了计算、存储和内核启动开销。本文针对其他论文没有考虑内存访问做了优化</li></ul><blockquote><p>树结构的标记候选进行并行解码，SpecInfer （Miao 等人， <ahref="https://arxiv.org/html/2404.00242v3#bib.bib27">2023</a>）引入了拓扑感知的因果掩码树注意算法，动态更新因果掩码以捕获标记之间的关系。Medusa （Cai et al., <ahref="https://arxiv.org/html/2404.00242v3#bib.bib5">2024</a>）使用了类似的静态因果掩码机制，而其他作品（Zhao et al., <ahref="https://arxiv.org/html/2404.00242v3#bib.bib46">2023</a> ；Liu etal., <a href="https://arxiv.org/html/2404.00242v3#bib.bib22">2024</a>）则采用类似的方法来提高注意力计算效率。</p></blockquote><ul><li><strong>基于树的解码的存储优化：这个或许是我要关注的重点</strong></li></ul><blockquote><p>针对基于树的解码进行优化的LLM框架（Kwon 等人， <ahref="https://arxiv.org/html/2404.00242v3#bib.bib20">2023</a> ；Zheng等人， <a href="https://arxiv.org/html/2404.00242v3#bib.bib47">2023</a>）关注内存存储效率。 vLLM （Kwon 等人， <ahref="https://arxiv.org/html/2404.00242v3#bib.bib20">2023</a> ）提高了GPU 内存利用率，允许来自同一父级的序列共享 KV 缓存存储。 SGLang （Zhenget al., <a href="https://arxiv.org/html/2404.00242v3#bib.bib47">2023</a>）支持与LLMs多轮交互期间的动态KV缓存管理，提高内存效率。</p><hr /></blockquote><ul><li><strong>关于并行工作的讨论：</strong></li></ul><blockquote><p>一些并发作品（Ye et al., <ahref="https://arxiv.org/html/2404.00242v3#bib.bib45">2024</a> ；Juravskyet al., <a href="https://arxiv.org/html/2404.00242v3#bib.bib18">2024</a>；Athiwaratkun et al., <ahref="https://arxiv.org/html/2404.00242v3#bib.bib3">2024</a> ）也认识到IO在LLM<code>推理过程中的重要性</code>。然而，这些作品至少存在以下缺陷之一：i）它们（Yeet al., <a href="https://arxiv.org/html/2404.00242v3#bib.bib45">2024</a>；Juravsky et al., <ahref="https://arxiv.org/html/2404.00242v3#bib.bib18">2024</a>；Athiwaratkun et al., <ahref="https://arxiv.org/html/2404.00242v3#bib.bib3">2024</a>）不能轻易扩展到解码树超过两个级别——它们针对单上下文批量采样场景，这是一般基于树的解码的特殊情况，以系统提示符作为前缀，并在第一深度中使用唯一的后缀；ii）他们（Juravsky et al., <ahref="https://arxiv.org/html/2404.00242v3#bib.bib18">2024</a>；Athiwaratkun et al., <ahref="https://arxiv.org/html/2404.00242v3#bib.bib3">2024</a>）没有考虑解码树中不同节点的长度导致的低效率。DeFT与并行作品的比较见附录<ahref="https://arxiv.org/html/2404.00242v3#A1.SS3">A.3</a> 。</p></blockquote><h1 id="三.方法论">三.方法论</h1><h2 id="初步">3.1 初步</h2><h3 id="llm-推理和瓶颈">LLM 推理和瓶颈：</h3><p>LLM推理涉及两个阶段：(1) 预填充和 (2) 解码。</p><p>在预填充阶段，提示被标记化以初始化LLM 。预填充阶段的输出成为解码阶段的输入。解码阶段是自回归的，上一步的每个输出标记用作下一步的输入标记。</p><p>由于自回归解码的顺序过程，<strong>LLM推理是受内存限制的，其中每次前向传递都需要从内存传输所有模型参数和KV缓存</strong>。较慢但较大的高带宽内存(HBM) 到速度较快但较小的 GPU 共享内存</p><p>另一个<strong>潜在的瓶颈是 GPU利用率低</strong>，当并行性（通常受批量大小限制）远小于 GPU上的流式多处理器 (SM) 数量时，就会发生这种情况，该操作仅使用 GPU的一小部分。</p><blockquote><p>之前没接触过这个流式多处理器SM，比较底层，暂时可以忽略</p></blockquote><h3 id="gpu-上注意力算法的执行模式">GPU 上注意力算法的执行模式</h3><p>我们可以将注意力算法的执行分为两个主要阶段：（1）QKV准备阶段：逻辑上对查询、键和值（QKV）进行分组，以分区并将QKV组映射到GPU的不同流式多处理器（SM）；(2)注意力计算阶段：将QKV分区加载到不同SM的共享内存中，并对每个组应用注意力算法以获得最终的注意力结果。</p><h3 id="具有分段注意力的-qkv-分区">具有分段注意力的 QKV 分区</h3><p>一个SM本来可以处理一个batch，A100有108个batch，通常batch&lt;108导致没有充分利用并行性能，所以需要<strong>QKV分区</strong></p><p>这里使用的Flash-Decoding提出的并行解码（另一篇论文，有时间再细看它是怎么实现切割KV分区并行计算的），以及另一篇论文提出的OnlineSoftmax合并切割计算的注意力（Dao 等人， <ahref="https://arxiv.org/html/2404.00242v3#bib.bib6">2022</a> ； <ahref="https://arxiv.org/html/2404.00242v3#bib.bib7">2023</a> ）</p><p>在<em>QKV 准备阶段</em>（参见第<ahref="https://arxiv.org/html/2404.00242v3#S3.SS3">3.3</a>节），QKV将逻辑分组到具有共享前缀 KV 缓存和负载平衡 IO感知的分区。这些分区将指导QKV在<em>注意力计算阶段</em>的加载（参见附录<ahref="https://arxiv.org/html/2404.00242v3#A1.SS4">A.4</a>），其中将执行注意力计算。</p><img src="/2024/12/23/DeFT%EF%BC%9A%E4%BD%BF%E7%94%A8Flash%E6%A0%91%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E8%A7%A3%E7%A0%81%EF%BC%8C%E4%BB%A5%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%95%88%E7%9A%84%E6%A0%91%E7%BB%93%E6%9E%84LLM%E6%8E%A8%E7%90%86/image-20241226220126266.png" class="" title="image-20241226220126266"><blockquote><p><strong>左侧结构：QKV准备阶段</strong>：</p><ul><li>树状KV缓存被逻辑分组，例如，节点KV0和其共享的查询（Qa、Qb）被分配到一个分组（G0）。</li><li>分组逻辑：<ul><li>KV-Guided Grouping：确保共享前缀只加载一次。</li><li>Flattened Tree KVSplitting：通过深度优先展开和块划分，均衡GPU负载。</li></ul></li></ul><p><strong>右侧结构：注意力计算阶段</strong>：</p><ul><li>不同分组（如G0、G1）被映射到GPU的不同流多处理器（StreamingMultiprocessors, SM）中。</li><li>GPU利用率优化：<ul><li>每个分组在对应的SM中独立完成计算。</li><li>使用<strong>树拓扑感知的全局归约</strong>（Tree-Topology-AwareGlobal Reduction）融合分组结果，生成最终的注意力输出。</li></ul></li></ul><p><strong>IO优化：</strong></p><ul><li>通过减少KV缓存的冗余加载显著优化了IO性能。</li><li>共享前缀（如KV0）的加载频率由传统方法的多次降为一次。</li></ul></blockquote><h2 id="deft概述">3.2 DeFT概述</h2><p><strong>QKV分区的重要性：</strong>对于<strong>基于树的解码</strong>，逻辑分区QKV对于高并行性的注意力计算是必要的。当树形KV缓存中的令牌数量较多时，由于内存容量的限制，树形生成请求的分支数量可能不足以充分利用GPU。比如一个推理类的任务要排序128个数，Llama2-&amp;B中涉及曰40K个token，KV缓存占用20GB。</p><p><strong>DeFT的动机：</strong>DeFT旨在解决LLM推理在处理树结构KV序列时的两个潜在瓶颈（即IO和GPU利用率）</p><p>(1)比如两个查询，对应的键有共同部分，希望消除共享前缀的KV缓存的冗余内存，来最小化IO（图中的K0V0)，为了Qa和Qb</p><p>(2)确保工作负载均衡以获得高GPU利用率，从而减少计算每个分段注意力开销Ai保持几乎相同。由于方程1中的全局约简需要全部部分注意力，如果计算开销Ai明显大于Aj，SM负责计算Aj会经历长时间的闲置。</p><p>DeFT技术概述：</p><p>(1)QKV阶段：使用了KV引导分组策略以重用共享前缀的KV缓存的IO，并且引入扁平树KV分割以实现高GPU利用率，详见3.3</p><p>(2)注意力计算阶段：设计DeFT AttentionKernel3来加载QKV分割，并按QKV准备阶段来进行逻辑分组，然后执行注意力计算。</p><blockquote><p>具体来说：</p><ul><li>使用了Kenel Fusion, Tiling strategies(Flash Attention中的融合)</li><li>Tree-Topology-Aware GlobalReduction，这是本文基于Flash-Decoding的缩减机制的一个升级，考虑了树结构来做聚合注意力以有效计算每个Q的最终注意力</li></ul></blockquote><h2id="prefix-aware-and-balanced-tree-structured-kv-cache-partitions">3.3Prefix-aware and Balanced Tree-structured KV Cache Partitions</h2><img src="/2024/12/23/DeFT%EF%BC%9A%E4%BD%BF%E7%94%A8Flash%E6%A0%91%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E8%A7%A3%E7%A0%81%EF%BC%8C%E4%BB%A5%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%95%88%E7%9A%84%E6%A0%91%E7%BB%93%E6%9E%84LLM%E6%8E%A8%E7%90%86/image-20241229193159493.png" class="" title="image-20241229193159493"><blockquote><p>个人对图的解释：</p><p>先提一下几个关键方法</p><ul><li>前缀感知--从旧有的Q-Guided Grouping改为KV-Guided Grouping</li><li>为了负载平衡怎么做到好的KV缓存切割--Flattened Tree KV Splitting</li></ul><p>首先关注最左上角的Notations，图中的方块这里都做了解释。</p><p>左边第二个部分是Vanilla TreeAttention，即这张图里面最原始的版本。我们在DCM中可以看到一个易于理解的方式：如果横着来划分KV缓存，一行一行的来分就是Q-GuidedGrouping，如果竖着来划分KV缓存，一列一列的来分就是KV-GuidedGrouping。</p><p>接下来我们通过右边的箭头来看一下QKV准备阶段，即要载入到SM的时候不同的策略（也是本文的<strong>重点</strong>）</p><p>先从箭头往右上走，看<strong>Q-GuidedGrouping</strong>，这里的典型就是Flash-Attention，它会给Qa和Qb分配自己的，没有把重复的给省掉；接着(因为考虑<strong>负载均衡</strong>所以进一步优化)往右走，看SequenceKVSpliting，因为每个Q有单独的对应KV，不用担心指向问题所以直接裁成块，这样可以均匀分配给SM</p><p>然后回到分割左右的箭头，这次往右下走，看<strong>KV-GuidedGrouping(标红重点)</strong>，这里的例子是DeFT-Node，可以看到它正是Vanilla中按竖着划分KV的结果，具备前缀感知（不同的Q如果有相同的KV会指向相同的KV块），但是此时KV划分大小不一，没有负载均衡。接着往右走我们通过NodeKV Spliting，跟上面的裁块逻辑是一样的。</p><p>然后我们回溯往下走，即本文重点方法<strong>Flattened Tree KVSplitting</strong>。我们先来看这张左边关于Flattened Tree KVSplitting的解释（即Remark 3.1中提到的三个组件的简单解释）。</p><ul><li>1.首先扁平化处理，通过深度优先将树结构转成顺序来处理。</li><li>2.其次将顺序的KV进行均匀切分KV得到长度相等的块。因为原来token长度不一的KV，在均分切块后的每块里面很容易存在不止有自身的内容，这就要用到第三个组件。</li><li>3.最后使用Bitmask，上面的每个均匀切分块，都有自己的一个或多个BitMask，指向均匀切分块里面的一部分区域。这里面每个Bitmask通过1or0来告诉每个查询是否该访问这个。论文指出它的开销会远小于DCM，可以忽略不计。<ul><li>ps：每一个bitmsak里面有64位，(我的理解是最多能供64个query来确认是否应该访问)</li><li>ps：我觉得对于一个切好的chunk，如KVb1，里面有几个Bitmask可能是取决于它里面包含了几个KV1,KV2,KV3...（下面举例会再次提到）</li></ul></li></ul><p>以右边这个DeFT-Flatten为实例，来解释一下这个<strong>Flattened Tree KVSplitting</strong>。(原始情况来自左边的Details for...)</p><ul><li>首先看第一个G0，对应的是第一个块KVb0，它的KV-BCM0只有一个，应该是因为它只包含了KV0的内容，值为11，表明Qa和Qb都可以访问。</li><li>然后看第二个G1，对应第二块KVb1，它的KV-BCM1有两个，应该是因为它包含了KV0和KV1，值分别为11和10，对应KV0和KV1，即KV0给Qa和Qb都可以访问，而KV1只允许Qa访问。</li><li>第三个与第二个同理。</li></ul></blockquote><p>本节深入研究QKV准备阶段的细节，这是DeFT的一个关键设计方面。注意力计算阶段的讨论推迟到附录<ahref="https://arxiv.org/html/2404.00242v3#A1.SS4">A.4</a> 。</p><h3 id="q-guided-vs.-kv-guided-grouping">Q-Guided vs. KV-GuidedGrouping</h3><p>用我的话来讲：Q-Guided说白了就是给每个Q的KV都做缓存，KV-Guided就是把指向了相同KV的Q合在一起指向同一个KV缓存。</p><p>前者没有前缀感知，而采用后者产生的查询的额外IO成本可以忽略不计。</p><h3 id="tree-kv-splitting-and-load-balancing.">Tree KV Splitting andLoad-Balancing.</h3><p>得益于<em>KV 引导分组</em>， DeFT -Node 可以识别 KV 缓存 IO的前缀。然而，它引入了一个潜在的瓶颈：不同SM之间的工作负载不平衡。</p><p>接下来把目光放在如何平衡工作负载：</p><img src="/2024/12/23/DeFT%EF%BC%9A%E4%BD%BF%E7%94%A8Flash%E6%A0%91%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E8%A7%A3%E7%A0%81%EF%BC%8C%E4%BB%A5%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%95%88%E7%9A%84%E6%A0%91%E7%BB%93%E6%9E%84LLM%E6%8E%A8%E7%90%86/image-20250112162321820.png" class="" title="image-20250112162321820"><h4 id="remark-3.1-techniques-of-flattened-tree-kv-splitting">Remark 3.1(Techniques of Flattened Tree KV Splitting)</h4><p>有<strong>三个</strong>关键组件</p><ul><li>Depth-first Flatten strategy:深度优先扁平化而非广度优先可以最大化查询重叠</li><li>Evenly block-wise strategy:拆分核心，确保每个QKV组中的KV长度相等，来平衡GPU中SM的工作负载。</li><li>Bit mask( <em>(Miao et al., <ahref="https://arxiv.org/html/2404.00242v3#bib.bib27">2023</a>).</em>):一组64位的整数，用于记录树中标记的因果信息，它的IO开销显著小于densecausal mask.</li></ul><blockquote><p>Bit mask是另一篇论文的成果，<strong>未看</strong></p></blockquote><h3 id="remark-3.2-discussion-on-tree-attention-algorithms">Remark 3.2(Discussion on Tree Attention Algorithms)</h3><p>几种现有的注意力算法是为推测解码而设计的，是针对整个树结构查询计算注意力。然而，这些方法的内存效率不高。详细的IO分析请参考附录<ahref="https://arxiv.org/html/2404.00242v3#A1.SS5">A.5</a></p><ul><li>Tree Attention-Medusa: 基于 Vanilla TreeAttention，它的内存效率低下有两个原因：（1）它没有利用Flash-Attention来减少中间结果计算期间的内存访问（例如Softmax）；(2)引入了密集因果掩码，其内存访问非常重要（见图<ahref="https://arxiv.org/html/2404.00242v3#A1.F11">11</a> ）。</li><li>Tree Attention-SpecInfer（Miao 等人，2023），该算法采用基于 VanillaTree Attention 的 Q-Guided Grouping 并通过 Flash-Decoding 对 KV序列进行划分。 为每个查询冗余加载整个树形结构的 KV缓存，内存效率低下（见图<ahref="https://arxiv.org/html/2404.00242v3#A1.F11">11</a> ）。</li></ul><h1 id="四.实验">四.实验</h1><p>在本节中，为了证明DeFT在不同树拓扑下的有效性，我们对三种类型的基于树的解码任务进行了全面的实验，包括：<strong>（1）few-shotPromping</strong> （Mann et al., <ahref="https://arxiv.org/html/2404.00242v3#bib.bib25">2020</a> ）：典型案例研究具有两个级别的树结构交互——一个前缀和多个后缀；<strong>（2）多步推理</strong>（Yao et al., <ahref="https://arxiv.org/html/2404.00242v3#bib.bib44">2023</a> ；Xie etal., <a href="https://arxiv.org/html/2404.00242v3#bib.bib43">2024</a>；Hao et al., <ahref="https://arxiv.org/html/2404.00242v3#bib.bib11">2023</a> ）：以具有并行查询的树结构过去的KV为特征的任务；<strong>（3）推测解码</strong>（Cai et al., <ahref="https://arxiv.org/html/2404.00242v3#bib.bib5">2024</a> ；Miao etal., <a href="https://arxiv.org/html/2404.00242v3#bib.bib27">2023</a> ）：涉及过去的 KV 的任务，按顺序使用树结构查询。</p><p>实验其实看的不是很懂</p><p>以下是一些觉得可能值得关注的地方：</p><h4 id="a-trade-off-between-memory-storage-and-memory-operation">Atrade-off between memory storage and memory operation</h4><p>在基于树的解码中，存储每个分支的KV缓存很简单，但缺乏前缀的KV缓存的共享存储。考虑到GPU 内存有限，在 KV共享中不考虑树结构会减少树可以处理的令牌数量。虽然按每个树节点存储 KV缓存显着提高了存储效率，但大多数注意力内核都是为基于序列的解码而设计的（Daoet al., <a href="https://arxiv.org/html/2404.00242v3#bib.bib6">2022</a>; Hong et al., <ahref="https://arxiv.org/html/2404.00242v3#bib.bib15">2023</a> ; Dao etal., <a href="https://arxiv.org/html/2404.00242v3#bib.bib7">2023</a> ）。<strong>要使用这些内核，来自不同节点的 KV缓存必须连接成单个张量，从而导致大量的数据移动成本</strong>（Kwon 等人，<a href="https://arxiv.org/html/2404.00242v3#bib.bib20">2023</a> ）。</p><blockquote><p>最后这句话不太理解</p></blockquote><h4 id="the-benefits-of-paged-memory-for-tree-based-decoding.">Thebenefits of paged memory for tree-based decoding.</h4><p>对于高效的 KV 缓存管理，分页内存（Kwon et al., <ahref="https://arxiv.org/html/2404.00242v3#bib.bib20">2023</a> ；Zheng etal., <a href="https://arxiv.org/html/2404.00242v3#bib.bib47">2023</a>）是当前的主流技术。这些 KV缓存张量存储在不连续的分页布局中，以提供令牌级重用。除了<strong>更高的存储效率</strong>之外，我们还注意到基于树的解码的分页内存管理的另一个好处：内存池中的非连续存储由指针寻址，确保在执行之前不需要将树结构的KV具体化为单个张量注意内核。相反，我们只需要记录每个token的KV缓存的内存池地址。</p><img src="/2024/12/23/DeFT%EF%BC%9A%E4%BD%BF%E7%94%A8Flash%E6%A0%91%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E8%A7%A3%E7%A0%81%EF%BC%8C%E4%BB%A5%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%95%88%E7%9A%84%E6%A0%91%E7%BB%93%E6%9E%84LLM%E6%8E%A8%E7%90%86/image-20250112180601269.png" class="" title="image-20250112180601269"><p>我们观察到，在基于树的解码中使用<strong>未分页的 KV缓存管理</strong>时，瓶颈是实现 KV 缓存所需的数据移动。然而，当我们<strong>使用分页内存管理</strong>时，注意力就成为新的瓶颈，特别是当令牌树很大时。<spanclass="citation" data-cites="毕竟重要的一点">@毕竟重要的一点</span>@</p><h4 id="end-to-end-behaviors-latency-and-ios.">4.3End-to-end Behaviors:Latency and IOs.</h4><p>我们通过测量端到端延迟来评估DeFT在各种基于树的解码任务上的性能（<ahref="https://arxiv.org/html/2404.00242v3#S4.T5">表 5</a>）。请参阅附录<ahref="https://arxiv.org/html/2404.00242v3#A1.SS7">A.7</a>中的注意力延迟（<ahref="https://arxiv.org/html/2404.00242v3#A1.T16">表 16</a> ）、IO（<ahref="https://arxiv.org/html/2404.00242v3#A1.T17">表 17</a>）和推理精度（<a href="https://arxiv.org/html/2404.00242v3#A1.T15">表15</a> ）。该评估证明了DeFT对树注意力的优化及其对挂钟时间的加速。</p><p>附录</p><h3 id="a.1-components-of-system-support-for-deft">A.1 Components ofSystem Support for DeFT</h3><img src="/2024/12/23/DeFT%EF%BC%9A%E4%BD%BF%E7%94%A8Flash%E6%A0%91%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E8%A7%A3%E7%A0%81%EF%BC%8C%E4%BB%A5%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%95%88%E7%9A%84%E6%A0%91%E7%BB%93%E6%9E%84LLM%E6%8E%A8%E7%90%86/image-20250113205524615.png" class="" title="image-20250113205524615"><p>DeFT系统的四个组件</p><ul><li>Branch Controller：它使树解码过程由用户定义的函数强制执行</li><li>Sequence TreeManager：它根据来自分支控制器的针对树的操作和令牌维护解码树的拓扑。诸如修剪和分支之类的树操作将由该组件中的Tree Handler执行。分支结果存储将记录解码树中所有分支的令牌生成结果，并在解码停止时输出。</li><li><strong>KV cache Manager</strong>：它将以树形结构维护KV缓存。保留解码树中的序列ID和KV缓存索引之间的映射，该映射将根据KV操作进行更新。这里提供了分页和非分页内存管理，以适应不同的注意力内核。</li><li>Model Interface：将输入元数据传递给DeFTAttention内核和MLP模块，然后返回logits和更新后的KV缓存指针。</li></ul><h3 id="a.2discussion-of-tree-based-decoding">A.2Discussion ofTree-based Decoding</h3><img src="/2024/12/23/DeFT%EF%BC%9A%E4%BD%BF%E7%94%A8Flash%E6%A0%91%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E8%A7%A3%E7%A0%81%EF%BC%8C%E4%BB%A5%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%95%88%E7%9A%84%E6%A0%91%E7%BB%93%E6%9E%84LLM%E6%8E%A8%E7%90%86/image-20250113210014715.png" class="" title="image-20250113210014715"><blockquote><p>还不是很理解这个Query并行是什么情况</p><p>Tree KV倒是比较理解（DeFT前面图3就是处理这种）</p></blockquote><img src="/2024/12/23/DeFT%EF%BC%9A%E4%BD%BF%E7%94%A8Flash%E6%A0%91%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E8%A7%A3%E7%A0%81%EF%BC%8C%E4%BB%A5%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%95%88%E7%9A%84%E6%A0%91%E7%BB%93%E6%9E%84LLM%E6%8E%A8%E7%90%86/image-20250113210240581.png" class="" title="image-20250113210240581"><p>要结合上面这张图看的左边来看，Query Token Tree之间的因果关系</p><p>这里提了一些<strong>相关工作</strong>：</p><ul><li>基于树的解码可以具有树结构的 KV 缓存，用于存储并了解共享前缀 (Zhenget al., <ahref="https://arxiv.org/html/2404.00242v3#bib.bib47">2023</a>),</li><li>或并行/推测解码中的树结构查询 (Miao et al., <ahref="https://arxiv.org/html/2404.00242v3#bib.bib27">2023</a>; Cai etal., <a href="https://arxiv.org/html/2404.00242v3#bib.bib5">2024</a>),<a href="https://arxiv.org/html/2404.00242v3#A1.F6">如图6</a>所示</li><li>通用解码可以同时使用树KV和树查询，这可以减少共享前缀的冗余（例如IO、存储、计算等），并且增加每次解码迭代生成的令牌。</li></ul><blockquote><p>我需要关注Query Token Tree吗？</p></blockquote><h3 id="analysis-of-speedup-potential-in-tree-based-decoding.">Analysisof speedup potential in tree-based decoding.</h3><p>在基于树的解码中，<strong>KV缓存和查询</strong>可以以树的形式构建。我们不仅可以将KV缓存存储在树中，还可以在注意力计算期间加载<strong>具有树拓扑意识</strong>的QKV，以最大限度地减少 HBM 和 GPU 片上共享内存之间昂贵的IO。我们通过两个具有树结构交互的复杂场景的案例研究来解释它：（1）<strong>多步推理</strong>（Yaoet al., <a href="https://arxiv.org/html/2404.00242v3#bib.bib44">2023</a>；Xie et al., <ahref="https://arxiv.org/html/2404.00242v3#bib.bib43">2024</a> ） ；(2)<strong>推测解码</strong>(Cai et al., <ahref="https://arxiv.org/html/2404.00242v3#bib.bib5">2024</a> ; Miao etal., <a href="https://arxiv.org/html/2404.00242v3#bib.bib27">2023</a>)</p><img src="/2024/12/23/DeFT%EF%BC%9A%E4%BD%BF%E7%94%A8Flash%E6%A0%91%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E8%A7%A3%E7%A0%81%EF%BC%8C%E4%BB%A5%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%95%88%E7%9A%84%E6%A0%91%E7%BB%93%E6%9E%84LLM%E6%8E%A8%E7%90%86/image-20250113211410946.png" class="" title="image-20250113211410946"><h5 id="case-study-1-multi-step-reasoning.">Case study 1: multi-stepreasoning.</h5><p>如图7左侧所示，我们可以将多步推理过程 (Hao et al.,2023;Yao etal.,2023;Besta et al.,2023)总结为三个阶段：</p><ul><li><ol type="1"><li><strong>思想生成</strong>：根据<spanclass="math inline">\(P_\mathrm{g}\)</span>和之前的步骤S，来生成下一步思考步骤的k 个候选者</li></ol></li><li>(2)<strong>思想评估</strong>：当面临各种思想的前沿时，LLM作为评估者基于评估提示<spanclass="math inline">\(P_e\)</span>衡量以前的想法 S对解决问题有多大帮助。这种评估充当搜索算法的启发式指导其进一步追求哪些状态以及探索它们的顺序；</li><li><ol start="3" type="1"><li><strong>基于树搜索的扩展</strong>：发挥不同的搜索算法(Luetal,2022;Liu et al., 2023 ; Xie et al., 2024 )来探索搜索空间，从而影响未来的树拓扑。在 (1) 和 (2)中，我们在树注意力计算期间可以共享KV缓存的<spanclass="math inline">\(\mathcal{IO}P_{g}/P_{e}\)</span>和<spanclass="math inline">\(S\)</span>。</li></ol></li></ul><blockquote><p>MCTS在这里用作一种方式？</p></blockquote><h5 id="case-study-2-speculative-decoding.">Case study 2: speculativedecoding.</h5><p>如图7右半部分所示，我们可以将推测解码(Caiet al.,2024;Miao etal.,2023)的过程总结为树阶段：</p><ul><li>(1)令牌树生成：多个 draft 模型 (Miao et al., 2023 )或微调头 (Cai etal., 2024 )根据提示 P 生成多个token序列,然后将它们合并到推测的令牌树中<spanclass="math inline">\(T_t\)</span>,速度非常快(例如在Speclnfer中的时间开销中只占1%(Miao 等人，2023);</li><li>(2)令牌验证：基于这些树形结构的令牌候选<spanclass="math inline">\(T_t\)</span>,根据LLM的输出验证其标记的正确性，其中树注意力计算是该过程的瓶颈(Miao等人， 2023)。在 (2) 中，我们在树注意力计算期间可以共享KV缓存的<spanclass="math inline">\(P_{g}/P_{e}的IO\)</span>和<spanclass="math inline">\(S\)</span>。</li></ul><h5 id="why-existing-tree-attention-algorithms-are-not-enough">Whyexisting tree-attention algorithms are not enough?</h5><p>现有的树注意力算法要么在内存访问方面效率低下（Cai et al., <ahref="https://arxiv.org/html/2404.00242v3#bib.bib5">2024</a> ；Miao etal., <a href="https://arxiv.org/html/2404.00242v3#bib.bib27">2023</a> ），要么不适合超过 64 个树的通用解码（Miao et al., <ahref="https://arxiv.org/html/2404.00242v3#bib.bib27">2023</a>）。令牌树中的令牌。</p><h3 id="a.3discussion-of-concurrent-works">A.3Discussion of ConcurrentWorks</h3><blockquote><p>这部分不是很理解，关于深度、</p></blockquote><img src="/2024/12/23/DeFT%EF%BC%9A%E4%BD%BF%E7%94%A8Flash%E6%A0%91%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E8%A7%A3%E7%A0%81%EF%BC%8C%E4%BB%A5%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%95%88%E7%9A%84%E6%A0%91%E7%BB%93%E6%9E%84LLM%E6%8E%A8%E7%90%86/image-20250114160004900.png" class="" title="image-20250114160004900"><p>这里罗列了一些对比方案</p><p>现有的单上下文大批量采样工作对于一般的基于树的解码来说硬件效率不高，原因有两个，如<ahref="https://arxiv.org/html/2404.00242v3#A1.T9">表 9</a>所示：</p><p>它们被设计用<strong>于解码只有两层的树</strong>——根部的前缀和深度为1的后缀（没理解到）。对于具有多层前缀的解码树，他们的算法只能减少树根处的提示的IO。然而，在多步推理等场景中（Yaoet al., 2023; Besta et al., 2023;hao et al., 2023），非根前缀的 token长度也可能很长（例如，数千个）的令牌），并且它们的KV缓存的IO没有被重用。<strong>DeFT可以复用通用解码树中所有非叶前缀的KVIO</strong>，提供更大的加速潜力。</p><h3id="a.4discussion-of-techniques-in-efficient-attention-algorithm-design">A.4Discussionof Techniques in Efficient Attention Algorithm Design</h3><img src="/2024/12/23/DeFT%EF%BC%9A%E4%BD%BF%E7%94%A8Flash%E6%A0%91%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E8%A7%A3%E7%A0%81%EF%BC%8C%E4%BB%A5%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%95%88%E7%9A%84%E6%A0%91%E7%BB%93%E6%9E%84LLM%E6%8E%A8%E7%90%86/image-20250114160520401.png" class="" title="image-20250114160520401"><p>这张图提了Tree Attention-Medusa 这篇论文中的处理方式</p><img src="/2024/12/23/DeFT%EF%BC%9A%E4%BD%BF%E7%94%A8Flash%E6%A0%91%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E8%A7%A3%E7%A0%81%EF%BC%8C%E4%BB%A5%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%95%88%E7%9A%84%E6%A0%91%E7%BB%93%E6%9E%84LLM%E6%8E%A8%E7%90%86/image-20250114160908893.png" class="" title="image-20250114160908893"><p>这张图提了DeFT<strong>使用的和推荐的方法</strong>，</p><p><strong><em>*Remark A.2*</em></strong> <strong>(The effects ofintroducing a causal mask).</strong></p><p>推测性解码工作中引入了<em>树拓扑感知因果掩模</em>，以方便使用单个解码树内的所有查询计算注意力GPU内核。它通过在解码树中记录查询和 KV 缓存之间的因果关系来实现这一点。<ahref="https://arxiv.org/html/2404.00242v3#A1.F6">如图 6</a>所示，whileoriginally designed for tree-based decoding with KV cache for a sequenceof tokens and tree-structured queries, the <em>Causal Mask</em> can alsobe adapted to tree decoding with tree-structured KV cache and parallelqueries—a configuration targeted by DeFT to enhance efficiency.</p><p><strong><em>Remark A.4</em> (IO in Radix Attention)</strong></p><p><em>Radix Attention</em> （Zheng et al., <ahref="https://arxiv.org/html/2404.00242v3#bib.bib47">2023</a>）<em>本质上是利用<strong>分页和树结构内存</strong>管理的Flash-Decoding</em> （Dao et al., <ahref="https://arxiv.org/html/2404.00242v3#bib.bib7">2023</a>）<em>的实现。因此，IO 行为与Flash Attention的行为相同，如</em><ahref="https://arxiv.org/html/2404.00242v3#A1.T12">表12</a><em>所示。</em></p><blockquote><p>Radix Attention重要性++</p></blockquote><h3 id="a.5analysis-io-complexity-of-deft">A.5Analysis: IO Complexity ofDeFT</h3><img src="/2024/12/23/DeFT%EF%BC%9A%E4%BD%BF%E7%94%A8Flash%E6%A0%91%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E8%A7%A3%E7%A0%81%EF%BC%8C%E4%BB%A5%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%95%88%E7%9A%84%E6%A0%91%E7%BB%93%E6%9E%84LLM%E6%8E%A8%E7%90%86/image-20250116212335589.png" class="" title="image-20250116212335589"><p><strong>这张图非常有助于理解这个优化过程</strong></p><h1 id="五.结论">五.结论</h1><blockquote><p>我们的<strong>消融研究</strong>强调：(1) 平衡分区至关重要，(2) DeFT-Flatten 在各种LLM模型和 GPU 架构中提供显着的加速，(3) DeFT -Flatten在更大的令牌大小（例如，更长的提示）以及更多树形结构请求下可以提供更大的加速提示</p></blockquote><h1 id="六.个人理解">六.个人理解</h1><h3 id="相关重点论文">相关重点论文：</h3><ul><li><p><ahref="https://zhuanlan.zhihu.com/p/691038809">图解大模型计算加速系列之：vLLM核心技术PagedAttention原理</a>讲得很好</p></li><li><p><a href="https://arxiv.org/abs/2402.15220">ChunkAttention:Efficient Self-Attention with Prefix-Aware KV Cache and Two-PhasePartition</a> 前缀树，共享，<strong>重点看看</strong></p></li><li><p>PageAttention、vattention：关注的是内存存储优化，减小用的开支，应该用不到树<strong>经典但是相关度存疑</strong></p></li><li><p>Streaming LLM基于窗口型的升级，但是每次计算不都要用到前面所有KV吗？<strong>还没看</strong></p><blockquote><p>功能是使之可以用于长文本</p><p>本文中指出在auto-regressive LLM中，大量的attentionscore会位于几个initialtokens上，即使它们在语义上并不重要。这些tokens称为attentionsinks。直观上，在auto-regressiveLLM中，初始的token会对后面所有的token产生影响。基于该观察，StreamingLLM除了保留最近的token对应的KVCache，还保留initial tokens的KVCache。它可以让LLM在训练时使用有限长的attentionwindow，而在推理时无需fine-tuning便可以用于无限长的情况。实验证明在序列长度达到120Ktokens时仍能保持合理的准确率。</p></blockquote></li><li><p><a href="https://arxiv.org/abs/2407.11550">Ada-KV: Optimizing KVCache Eviction by Adaptive Budget Allocation for Efficient LLMInference</a><strong>不同注意力头关注度存在差异后</strong>，对其进行适配性压缩预算分配。这个是用<strong>压缩</strong>来做的，可能有一定启示意义。</p></li><li><p><a href="https://arxiv.org/abs/2305.09781">SpecInfer:Accelerating Generative Large Language Model Serving with Tree-basedSpeculative Inference and Verification</a><strong>树状注意力和KVCache?这个是Token树但是KVCache暂时好像还没有树，相关度感觉一般</strong></p><p>相关解释文章：<ahref="https://cloud.tencent.com/developer/article/2293462?utm_source=chatgpt.com">机器之心</a>（精炼/简略）、</p><blockquote><p>这是一个通过基于树的推测推理和验证来加速生成式大语言模型 ( LLM )的系统。 SpecInfer背后的关键思想是利用小型推测模型来预测LLM的输出；预测被组织为令牌树，每个节点代表一个候选令牌序列。使用一种新颖的基于树的并行解码机制，针对LLM并行验证由令牌树表示的所有候选令牌序列的正确性。SpecInfer使用LLM作为令牌树验证器而不是增量解码器，这显着减少了服务生成LLMs端到端延迟和计算要求，同时可证明保持模型质量。我们的评估表明，SpecInfer的分布式LLM推理性能比现有LLM服务系统高 1.5-2.8倍，基于卸载的LLM推理性能比现有 LLM 服务系统高 2.6-3.5倍，同时保持相同的生成性能。</p></blockquote><img src="/2024/12/23/DeFT%EF%BC%9A%E4%BD%BF%E7%94%A8Flash%E6%A0%91%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E8%A7%A3%E7%A0%81%EF%BC%8C%E4%BB%A5%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%95%88%E7%9A%84%E6%A0%91%E7%BB%93%E6%9E%84LLM%E6%8E%A8%E7%90%86/image-20241225173657238.png" class="" title="image-20241225173657238"></li><li><p><strong>准备一看</strong>：RadixAttention(复用前缀)、ChunkAttention(前缀树),Tree Attention-Medusa(Deft说它在这篇基础上优化减少了IO操作)</p></li></ul><h5 id="动机目的">动机/目的：</h5><ul><li>减少内存消耗</li><li>减少IO次数？增快推理速度</li></ul><p><strong>手段本质：</strong></p><ul><li>裁剪/压缩/稀疏化？选择性丢掉一些？</li><li>类似PageAttention做分配？</li><li>共享？能比PageAttention做的更好吗？</li></ul><h3 id="相关prefix-kvcache论文">相关prefixkvcache<code>论文</code>：</h3><h4 id="vllmpage-attention"><strong>1.vLLM(page-attention)</strong></h4><p>Block级别的KV Cache共享</p><p>它在Parallelsampling时也可以把共享前缀进行相同分配，只不过是针对序列的。</p><img src="/2024/12/23/DeFT%EF%BC%9A%E4%BD%BF%E7%94%A8Flash%E6%A0%91%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E8%A7%A3%E7%A0%81%EF%BC%8C%E4%BB%A5%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%95%88%E7%9A%84%E6%A0%91%E7%BB%93%E6%9E%84LLM%E6%8E%A8%E7%90%86/image-20250113143912432.png" class="" title="image-20250113143912432"><img src="/2024/12/23/DeFT%EF%BC%9A%E4%BD%BF%E7%94%A8Flash%E6%A0%91%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E8%A7%A3%E7%A0%81%EF%BC%8C%E4%BB%A5%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%95%88%E7%9A%84%E6%A0%91%E7%BB%93%E6%9E%84LLM%E6%8E%A8%E7%90%86/image-20250113143926670.png" class="" title="image-20250113143926670"><h4 id="cacheblend">2. CacheBlend</h4><p><a href="https://zhuanlan.zhihu.com/p/7781800255">详解CacheBlend：RAG 场景 KV 复用，打破前缀相同的限制</a></p><p>针对RAG场景进行KV复用，打破了前缀相同才能使用<strong>KVCache</strong>的限制</p><blockquote><p>问题1: 非RAG场景呢？</p></blockquote><p>RAG 有以下特点：</p><ol type="1"><li>KV Cache 复用以文本块为<ahref="https://zhida.zhihu.com/search?content_id=250579055&amp;content_type=Article&amp;match_order=1&amp;q=粒度&amp;zhida_source=entity">粒度</a></li><li>数据库包含的文档数量大，但是一小部分热文档会在请求中重复出现</li></ol><p>因此，如果能够在请求间复用相同文本块的 KV Cache，并构建一个<ahref="https://zhida.zhihu.com/search?content_id=250579055&amp;content_type=Article&amp;match_order=1&amp;q=多级缓存架构&amp;zhida_source=entity">多级缓存架构</a>，把热文本块的KV Cache 存储在GPU，冷文本块卸载到主存和磁盘，需要时再加载回来，能够大大提升 RAG的效率和性能。</p><p><strong>挑战</strong></p><p>如果缓存下某个文本块的 KVCache，在之后的请求中如果包含了这个文本块就复用缓存好的 KVCache，这样做看上去很简单有效，但这种方法会有两个问题：</p><ol type="1"><li><strong>KV Cache 的位置编码错误</strong>：RAG拼接文本块时是有顺序的，这个顺序不能直接打破，因此同一个文本块在请求当中的位置可以是不同的，这意味着这些token 虽然相同，但是位置编码不同，Q/K 也是不同的。</li><li><strong>文本块之间的交叉注意力（cross-attention）缺失</strong>：在原始的prefill 计算当中，后面文本块中的 token 会注意到前面文本块中的token（Causal Attention）。可是如果在一个请求中，例如包含两个文本块[chunk1、chunk2]，chunk2 使用缓存的 KV Cache，这些 KV 是没有与 chunk1 中token 计算过 attention score 的，因为只有请求产生了才知道自己前面有哪些token，chunk2 的 KV Cache 在被缓存的时候还不知道谁是 chunk 1 呢。</li></ol><p>有了这两个问题，缓存和复用文本块的 KV Cache就难了，只有前面所有文本块都相同的 KV Cache 块可以复用。</p><p><strong>前提工作</strong></p><img src="/2024/12/23/DeFT%EF%BC%9A%E4%BD%BF%E7%94%A8Flash%E6%A0%91%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E8%A7%A3%E7%A0%81%EF%BC%8C%E4%BB%A5%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%95%88%E7%9A%84%E6%A0%91%E7%BB%93%E6%9E%84LLM%E6%8E%A8%E7%90%86/image-20250115101014721.png" class="" title="image-20250115101014721"><img src="/2024/12/23/DeFT%EF%BC%9A%E4%BD%BF%E7%94%A8Flash%E6%A0%91%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E8%A7%A3%E7%A0%81%EF%BC%8C%E4%BB%A5%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%95%88%E7%9A%84%E6%A0%91%E7%BB%93%E6%9E%84LLM%E6%8E%A8%E7%90%86/image-20250115102102089.png" class="" title="image-20250115102102089"><p>CacheBlend 的方案主要分为两部分：先拼接起不同文本块的 KVCache、挑选一部分重要的 token 重新计算它们的注意力（<strong>Selectivelyrecomputing KV cache</strong>）</p><p><strong>拼接 KV Cache 块</strong></p><p>如何解决 Full KV reuse 位置编码错误的问题呢？这里用到了 <ahref="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2104.09864">RotaryPosition Embedding(RoPE)</a>编码的特性。</p><p>但是实际上我们不能知道 KV 偏差，我们不知道真实的 KV是多少，因为我们不会真的做 Full KV Re-compute。这里 CacheBlend 利用了<ahref="https://zhida.zhihu.com/search?content_id=250579055&amp;content_type=Article&amp;match_order=2&amp;q=相邻层&amp;zhida_source=entity">相邻层</a>之间的相似性：<strong>上一层KV 偏差大的 token 大概率也是下一层 KV 偏差大的</strong></p><p><strong>交叉注意力恢复</strong></p><p>如何评估缺失交叉注意力对它的影响大不大？让注意力产生差别的根源在于pre-computed KV 与真实的 KV 不同，所以我们可以衡量 <strong>KV 偏差：一个token 在有无文本块交叉注意力的情况下 k、v 的差别</strong>，也就是在 FullKV Reuse 和 Full KV Re-compute情况下的区别。显然，偏差越大越容易影响注意力的结果。因此，我们可以挑选前r% 个 KV 偏差大的 token，然后重新计算它们的 attention。</p><p>但是实际上我们不能知道 KV 偏差，我们不知道真实的 KV是多少，因为我们不会真的做 Full KV Re-compute。这里 CacheBlend 利用了<ahref="https://zhida.zhihu.com/search?content_id=250579055&amp;content_type=Article&amp;match_order=2&amp;q=相邻层&amp;zhida_source=entity">相邻层</a>之间的相似性：<strong>上一层KV 偏差大的 token 大概率也是下一层 KV 偏差大的</strong>，如下图：</p><img src="/2024/12/23/DeFT%EF%BC%9A%E4%BD%BF%E7%94%A8Flash%E6%A0%91%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E8%A7%A3%E7%A0%81%EF%BC%8C%E4%BB%A5%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%95%88%E7%9A%84%E6%A0%91%E7%BB%93%E6%9E%84LLM%E6%8E%A8%E7%90%86/image-20250115103911930.png" class="" title="image-20250115103911930"><p>根据这个特性，CacheBlend 给出的方法是，在第一层 Layer 1 完整计算所有token，这样得到的 Layer 2 的 KV Cache 将和 pre-computed Layer 2 KV Cache不同，从中筛选出 KV 偏差大的 r1 个 token 重新计算，这样在 Layer 3中将至多有 r1 个 token 的 KV 与 pre-computed 的不同，再其中再挑选出 KV偏差大的 r2 个 token 重新计算，以此类推……作者形象地把这个方法称为gradual filtering scheme（渐进过滤方案），下一层选择进行 reompute 的token 是上一层 recompute token 的子集。</p><h4 id="raddix-attention">3.Raddix Attention</h4><p><ahref="https://cloud.tencent.com/developer/article/2424704">原理&amp;图解vLLMAutomatic Prefix Cache(RadixAttention)</a></p><p>带有LRU策略的RadixAttention操作示例↓</p><img src="/2024/12/23/DeFT%EF%BC%9A%E4%BD%BF%E7%94%A8Flash%E6%A0%91%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E8%A7%A3%E7%A0%81%EF%BC%8C%E4%BB%A5%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%95%88%E7%9A%84%E6%A0%91%E7%BB%93%E6%9E%84LLM%E6%8E%A8%E7%90%86/image-20250116144446066.png" class="" title="image-20250116144446066"><p>0x02部分讲到通过hash作为cache blokc的唯一标识的方式</p><img src="/2024/12/23/DeFT%EF%BC%9A%E4%BD%BF%E7%94%A8Flash%E6%A0%91%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%BF%9B%E8%A1%8C%E8%A7%A3%E7%A0%81%EF%BC%8C%E4%BB%A5%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%95%88%E7%9A%84%E6%A0%91%E7%BB%93%E6%9E%84LLM%E6%8E%A8%E7%90%86/image-20250116145713300.png" class="" title="image-20250116145713300"><p><strong>重点理解</strong>：由于所有当前block的hash码都依赖于此前所有block的token_ids，因此，这个唯一的hash码实际上也代表着一种唯一的前缀关系。这个需要仔细理解，因为只有确保了唯一的前缀关系，才能确保从PrefixCache KVBlocks中拿到的一系列block是具有<strong>相同上下文</strong>的。也就是说这种hash编码的实现，实际上具备<strong>前缀树（PrefixTree）</strong>的功能。并且这种前缀树是以PhysicalTokenBlock为单位的，树上的每一个node就是代表一个实际的PhysicalTokenBlock，每一个node的内容就是这个PhysicalTokenBlock的hash码，这个hash码又代表着从树的根节点到当前PhysicalTokenBlock的唯一路径。</p><blockquote><p>一个比较有意思的点： <strong>（1）只有PrefixCaching的优化，多轮对话分析。</strong>如下图所示，只有PrefixCaching时，每个新的轮次对话中，总是会有2个片段的prompt需要在prefill阶段进行计算。其中一个frag是上一轮对话的输出，另一个frag是当前轮对话的输入。此时，上一轮对话的输出由于没有被Caching，需要在本轮对话的prefill阶段进行recompute，这个recompute的耗时取决于上一轮生成的token数，根据ChunkPrefills论文（SARATHI: Efficient LLM Inference by Piggybacking Decodeswith Chunked Prefills）中的一个观察，"at small batch sizes, the decodecost per token can be as high as ∼ 200 times the prefill cost pertoken"，也就是说，prefill中计算200tokens的耗时大约等于generate阶段计算一个token的耗时。<strong>因此，如果上一轮生成了200个tokens，本轮prefillrecompute增加的耗时相当于generate阶段多生成一个token的耗时。</strong></p></blockquote><h4 id="vattention">4.vattention</h4><p><ahref="https://cloud.tencent.com/developer/article/2429149">vAttention：用于在没有PagedAttention的情况下Serving LLM</a> 非常详细的文章，很清晰。</p><p>5.<a href="https://arxiv.org/html/2412.19442v2#S1">A Survey on LargeLanguage Model Acceleration based on KV Cache Management</a></p><p>最新KV综述，梳理了逻辑，超赞。</p>]]></content>
    
    
    
    <tags>
      
      <tag>科研, KV Cache</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>可微分集成多智能体交互式预测和运动规划框架</title>
    <link href="/2024/12/23/%E5%8F%AF%E5%BE%AE%E5%88%86%E9%9B%86%E6%88%90%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E4%BA%A4%E4%BA%92%E5%BC%8F%E9%A2%84%E6%B5%8B%E5%92%8C%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92%E6%A1%86%E6%9E%B6/"/>
    <url>/2024/12/23/%E5%8F%AF%E5%BE%AE%E5%88%86%E9%9B%86%E6%88%90%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E4%BA%A4%E4%BA%92%E5%BC%8F%E9%A2%84%E6%B5%8B%E5%92%8C%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92%E6%A1%86%E6%9E%B6/</url>
    
    <content type="html"><![CDATA[<p><strong>计划：</strong></p><blockquote><p>主要分成三遍来阅读</p><p>1.title 2.abstract 3.introduction 4.method 5.experiments6.conclusion</p><p>第一遍：标题、摘要、结论。可以看一看方法和实验部分重要的图和表。这样可以花费十几分钟时间了解到论文是否适合你的研究方向。</p><p>第二遍：确定论文值得读之后，可以快速的把整个论文过一遍，不需要知道所有的细节，需要了解重要的图和表，知道每一个部分在干什么，圈出相关文献。觉得文章太难，可以读引用的文献。</p><p>第三遍：第三遍是最后一遍了，也是最详细的一遍，这里就需要知道每一句话在干什么，每一段在说什么，一边读，可以一边在脑子里面思考一些问题：比如说，如果要是我来写这篇文章，我会如何组织这个结构？读实验部分的时候，可以思考一下，作者是如何描述自己的实验的，你可以思考，如果换自己来做的话，能不能比作者做得更好？这一遍读的时候，一定要明白作者每句话，每个字在说什么，并且最好可以脑补出它整个流程是什么样子的，似乎是自己在做实验，写论文一样。如果有困难的话，可以借助思维导图或者流程图这样的工具，把他的整个流程以可视化的形式展现出来，帮助自己理解。</p></blockquote><p><strong>如果我有更多时间：</strong></p><ol type="1"><li>整个设计的框架和思路，每个的输入输出</li><li>具体的公式使用</li><li>跑一遍实验看看（如果有条件）</li></ol><p><strong>关键词：</strong></p><p>Agent/智能体 -&gt;智能体</p><p><strong>相关文章阅读：</strong></p><ol type="1"><li><a href="https://www.helloxiaobai.cn/article/e2e/1">Imitation Is NotEnough - 在运动规划中克服模仿学习的局限性</a> 未读</li><li>GameFormer也是联合预测，在模拟智能体交互部分用level-k层次化博弈</li><li>公众号文章：<strong>预测决策规划可以分三种方法，三种阶段：消极性交互（预测首先对其他车辆产生一个轨迹，自车通过costfunction做一个反应）；第二是一般交互（自车轨迹也会影响其他车辆）；第三是强交互（你影响我我影响你）</strong>预测规划解耦的本质上是被动的，不能表示自车和其他智能体连续的交互</li></ol><p><strong>自己的思考：</strong></p><ul><li>一个关键点是如何规划，如何提取信息资源</li><li>我这个过程中学习的思路是什么：像编织一张网去覆盖这篇论文，我看别人的综述，了解分类，就知道怎么去概括它</li></ul><h1 id="零.前人见解">零.前人见解</h1><p>1.<ahref="https://www.cnblogs.com/zjucys/p/17966398">深度学习自动驾驶系统中预测与规划的集成：综述</a></p><ul><li><p>包含感知、预测、规划和控制模块</p></li><li><p>常见的参数：自车EV、他车SV、历史状态X、未来状态Y、环境I、预测P（分每个车单独预测、考虑所有车两两组合，会维度爆炸、只考虑和他车高度交互的车辆）</p></li><li><p>规划中自车的意图已知，而预测中他车意图未知</p></li><li><p>我们可以关注以下几个版块的问题：预测：场景表征、交互建模、轨迹解码、基准测试规划：输入表征、输出表征、目标调节、规划范式、基准测试集成预测和规划：集成规则、自车和智能体的关系、安全和应急、可能的组合</p></li><li><p><strong>手动集成（Manualintegration）</strong>还有完全的端到端和可解释的端到端</p><ul><li>规划和预测使用独立的子系统，二者的交互基于专业领域的知识而设计</li><li>集成方式：<ul><li>先预测+后规划</li><li>规划生成候选方案+对候选方案进行预测+选择最终方案</li><li>迭代或同时进行预测和规划</li></ul></li></ul></li><li><p>DIPP</p><ul><li>联合预测所有智能体的轨迹（gameformer也做了）</li><li>选择最高概率的轨迹作为微分非线性运动规划器（differential nonlinearmotion planner）的输入</li><li>规划器根据运动学模型和损失函数进行局部迭代优化</li></ul></li><li></li></ul><ol start="2" type="1"><li></li></ol><h1 id="一.概要">一.概要</h1><ul><li><p><strong>研究背景</strong>:自动驾驶中预测周围交通参与者的未来状态并据此规划安全、平滑且符合社会规范的轨迹至关重要。现有自动驾驶系统存在<strong>两个主要问题:预测模块与规划模块分离,且规划的成本函数难以指定和调整。</strong></p></li><li><p><strong>动机补充：</strong></p><ul><li><strong>直接优化最终任务性能</strong>：DIPP的目标是训练神经网络预测器和可学习的成本函数，以直接优化最终任务的性能。</li><li><strong>模仿人类驾驶员的计划</strong>：通过塑造其他代理的预测来反映他们的“心理状态”，DIPP旨在模仿人类驾驶员的计划。</li></ul></li><li><p><strong>研究目标</strong>:提出一个可学习的、可微分的集成预测-规划框架(DIPP),能够从数据中学习成本函数。</p></li><li><p><strong>方法</strong>:框架使用可微分的非线性优化器作为运动规划器,输入由神经网络给出的周围智能体的预测轨迹,优化自动驾驶车辆的轨迹,使得所有操作(包括成本函数权重)可微分。</p></li></ul><blockquote><p>可微分的非线性优化器？运动规划期？神经网络给出周围智能体的预测轨迹？所有操作可微分？我想我需要了解一下自动驾驶大模型相关的架构</p></blockquote><ul><li><p><strong>实验结果</strong>:在大规模真实驾驶数据集上训练,通过开环和闭环测试验证了框架的有效性。开环测试结果表明,所提方法在多个指标上优于基线方法,并能输出接近人类驾驶员的轨迹。闭环测试中,所提方法展现出处理复杂城市驾驶场景的能力和对分布偏移的鲁棒性。</p></li><li><p>创新点：</p><blockquote><p><strong>可微分集成框架：</strong></p><p>提出了一个可微分的集成预测-规划框架（DIPP），将预测和规划模块结合在一起，使得整个系统可以端到端地学习。这种设计使得预测结果能够直接影响规划过程，从而提高了系统的整体性能。</p><ul><li>前人研究（如文献[27]）提出了交互式预测和规划框架，该框架可以根据自我车辆的规划状态序列来预测其他智能体的未来状态。然而，他们的方法中的规划器使用交叉熵方法对大量动作序列进行采样，并且必须迭代查询预测模型以获得其他智能体的响应，这可能会显著减慢规划过程。相比之下，<strong>DIPP框架使预测模型能够输出规划感知的预测结果，并隐式捕获训练过程中自我车辆未来行为的影响。这种设计使得规划器能够在测试期间根据预测结果生成类似人类的计划，而无需对预测模型进行迭代查询。</strong></li><li><strong>可微分的运动规划器</strong>：<ul><li>传统的运动规划方法往往依赖于静态环境或假设完美的预测，而DIPP框架采用可微分的非线性优化器作为运动规划器，使得所有操作（包括成本函数权重）都是可微分的。这一创新使得规划器能够在训练过程中直接从预测结果中学习，优化过程更加灵活和高效。</li></ul></li></ul><p><strong>学习成本函数：</strong></p><ul><li>以往的研究通常依赖于手动调整成本函数的权重，而DIPP框架通过将成本函数的权重作为可学习的参数，允许其从数据中自动调整。这种方法提高了系统的适应性，使得在不同驾驶场景中能够更好地平衡安全性、舒适性和效率。</li></ul><p><strong>多智能体交互建模：</strong></p><ul><li>DIPP框架使用基于Transformer的神经网络进行多智能体的未来轨迹预测，能够捕获智能体之间的交互关系。与传统方法（单智能体预测）相比，这种方法提高了预测的准确性，使得规划器能够更好地理解周围环境的动态变化。</li></ul><p><strong>显式规划与优化：</strong></p><ul><li>DIPP框架通过显式的运动规划步骤和隐式的预测-规划交互，提供了一种新的方法来处理复杂的交通场景。这种设计使得框架能够在生成轨迹时考虑到周围智能体的行为，从而提高了决策的合理性和安全性。</li></ul><p><strong>端到端的学习能力</strong>：</p><ul><li>DIPP框架的设计允许整个系统从真实驾驶数据中进行端到端的学习，优化最终的规划性能。这种能力使得框架能够在实际应用中更好地适应复杂的驾驶环境。</li></ul></blockquote></li></ul><h1 id="二.相关工作">二.相关工作</h1><ul><li><strong>多智能体预测</strong>:综述了基于深度神经网络的轨迹预测方法,特别是基于Transformer的网络。指出现有模型大多独立预测每个智能体的未来轨迹,计算效率低且可能产生不一致的结果。多智能体联合预测能够以一致的方式生成所有智能体的未来轨迹,捕获智能体间的交互,有助于规划任务。</li></ul><blockquote><p>这里提到单智能体预测和多智能体预测，现在大多数是前者，论文认为后者更有前途</p></blockquote><ul><li><strong>运动规划</strong>:运动规划是研究较为充分的领域,包括轨迹优化、图搜索、随机采样等方法。学习型方法直接从传感器输入或感知结果生成动作或轨迹,但缺乏可解释性和泛化能力。本文采用基于优化的方法,灵活且能实现对轨迹的最大控制。</li></ul><blockquote><p>这里提到有新的方法（学习型方法，使用神经网络）和经典方法，为了可解释性和安全性选择了经典方法。“采用可微最小二乘非线性优化器作为运动规划器，以便优化器可以集成到神经框架中，并且可以同时学习其参数（例如，成本函数权重）。”这句话那是看不懂一点</p></blockquote><ul><li><strong>联合预测和规划</strong>:在密集且交互的交通中行驶需要联合推理其他智能体的未来行为和自动驾驶车辆的计划。<strong>一种有前景的框架是生成自动驾驶车辆的候选轨迹,并条件化地预测其他智能体的未来轨迹。</strong>例如， [ <ahref="https://ar5iv.labs.arxiv.org/html/2207.10422?_immersive_translate_auto_translate=1#bib.bib27">27</a>]中提出了一种交互式预测和规划框架，它可以根据自我车辆的规划状态序列来预测其他智能体的未来状态。他们的方法中的规划器使用交叉熵方法对大量动作序列进行采样，并且必须迭代查询预测模型以获得其他智能体的响应，这可能会显着减慢规划过程。相比之下，我们的方法使预测模型能够输出规划感知的预测结果，并隐式捕获训练过程中自我车辆未来行为的影响。这使得规划器能够在测试期间根据预测结果生成类似人类的计划，而无需对预测模型进行迭代查询。<strong>另一种方法是端到端的整体神经网络模型,隐式捕获预测-规划交互,但缺乏安全保证。</strong></li></ul><blockquote><p>这个可能是重点，看看<strong>同期其他的交互式预测和规划框架</strong></p><p>本文采用了主一但是采用了第二种的方法。它使用基于Transformer的神经网络来预测周围智能体的未来轨迹，为规划器提供输入。同时没有做纯粹的端到端，没用“隐式的捕获预测”，增加了可解释性和安全性。</p></blockquote><h1 id="三.方法论">三.方法论</h1><h3 id="概述">1.概述</h3><img src="/2024/12/23/%E5%8F%AF%E5%BE%AE%E5%88%86%E9%9B%86%E6%88%90%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E4%BA%A4%E4%BA%92%E5%BC%8F%E9%A2%84%E6%B5%8B%E5%92%8C%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92%E6%A1%86%E6%9E%B6/image-20241223163032565.png" class="" title="image-20241223163032565"><blockquote><p>反向训练具体方式？如果是创新就要看，是通用的那我就不关心了</p><p>可微分具体是怎么做的？</p><p>答，我先不关心，但我知道<strong>它的意义是允许反向传播算法来训练网络</strong></p><p>代价函数以及返回结果进行训练，具体？</p><p>答：我们采用可微的非线性优化器来解决优化问题，因此规划器的梯度（规划损失）可以在训练阶段反向传播到预测模块和成本函数，使整个框架端到端可学习。神经网络预测器和可微运动规划器的细节分别在<ahref="https://ar5iv.labs.arxiv.org/html/2207.10422?_immersive_translate_auto_translate=1#S3.SS2">第III-B</a>节和第<ahref="https://ar5iv.labs.arxiv.org/html/2207.10422?_immersive_translate_auto_translate=1#S3.SS3">III-C</a>节中给出。</p></blockquote><h3 id="问题表述">2.问题表述</h3><img src="/2024/12/23/%E5%8F%AF%E5%BE%AE%E5%88%86%E9%9B%86%E6%88%90%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E4%BA%A4%E4%BA%92%E5%BC%8F%E9%A2%84%E6%B5%8B%E5%92%8C%E8%BF%90%E5%8A%A8%E8%A7%84%E5%88%92%E6%A1%86%E6%9E%B6/image-20241223163502170.png" class="" title="image-20241223163502170"><p>Predictor:根据（自己？）、地图和周围的Agents历史进行预测(自车?)、周围Agents联合分布的轨迹</p><p>Planner:根据自车初始计划、周围Agents的预测轨迹以及当前状态，进行计算（优化）得到最佳路线。具体建模为最小二乘优化问题</p><p>优点之一：采用了可微非线性优化器，来实现梯度可以反向传播。</p><blockquote><p>有更好的优化方法吗？cost函数应该有很多</p></blockquote><h3 id="多智能体预测和初始计划">3.多智能体预测和初始计划</h3><p>首先明确这个是多智能体预测</p><p><strong>输入部分：</strong>对于每个智能体，考虑最近该智能体的动态特征（这个应该是通用不细究），并且构建一个本地场景上下文，对于地图元素，也用向量（这个应该是也是通用的），<strong>我觉得这些都会是其余通用的，我不关心。</strong></p><blockquote><p>对其他车的预测方法？有更好的吗？</p></blockquote><p><strong>编码智能体历史和场景上下文：</strong>为了对观察到的智能体的历史状态编码，将状态序列输入到LSTM</p><ul><li><strong>智能体历史编码</strong>：使用LSTM网络对每个智能体的历史状态进行编码。LSTM是一种特殊的循环神经网络（RNN），擅长处理和预测基于时间序列的数据。</li><li><strong>场景上下文编码</strong>：使用MLP对场景上下文中的车道和人行横道进行编码。MLP是一种前馈神经网络，能够学习输入数据的非线性表示。</li></ul><blockquote><p><strong>“使用LSTM是因为我们发现在处理短期时间序列时它会比Transformers带来更好的最终预测性能和计算效率”</strong>重要的点</p><p><strong>忘了LSTM具体的相关内容了，但是没时间细究。</strong></p><p>网上有人说这和遗忘的一些基于transformer的大同小异，那就跳过。</p></blockquote><p><strong>对代理-代理和代理-场景交互进行建模</strong>：做了两个图，<ahref="https://ar5iv.labs.arxiv.org/html/2207.10422?_immersive_translate_auto_translate=1#bib.bib7">heterogeneousedge-enhanced graph attention network</a>，<strong>没时间看</strong></p><h1 id="四.实验">四.实验</h1><h1 id="五.结论">五.结论</h1><p>我们提出了一种<strong>新颖的可微分集成多智能体交互式预测和运动规划框架</strong>，该框架是<strong>根据现实世界的驾驶数据进行端到端训练的</strong>。建立基于Transformer的<strong>预测器</strong>来预测周围智能体的联合未来轨迹，并为<strong>规划器</strong>提供初始猜测。预测轨迹、初始计划和可学习成本函数被引导至基于优化的可微分运动规划器，从而允许框架中的每个组件都是可微分的。该框架在大规模城市驾驶数据集上通过开环和闭环测试进行了验证。开环测试结果表明，我们提出的方法在规划和预测性能、乘坐舒适度和安全指标方面优于传统的管道方法和基于IL的方法。闭环测试结果表明，基于规划的方法显着优于基于IL的方法（尽管在开环测试中与人类轨迹具有更好的相似性），这表明我们的方法克服了离线学习中常见的分布偏移问题。此外，该方法在闭环测试中优于管道方法，强调了预测和规划模块联合训练的好处。在消融研究中，我们发现框架中的所有可学习组件对于维持优化器的稳定性和所需的规划性能都是不可或缺的。所提出的框架有可能通过使所有组件都能从现实世界的数据中学习来加速自动驾驶决策系统的开发。</p><h1 id="六.如果我来讲dipp">六.如果我来讲DIPP</h1><h2 id="特点">1.特点：</h2><ul><li>联合预测所有智能体的轨迹，</li><li>选择最高概率的轨迹作为微分非线性运动规划器（differential nonlinearmotion planner）的输入</li><li>规划器根据运动学模型和损失函数进行局部迭代优化</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>科研</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>方向探索</title>
    <link href="/2024/12/19/%E7%A7%91%E7%A0%94%E6%96%B9%E5%90%91%E6%8E%A2%E7%B4%A2/"/>
    <url>/2024/12/19/%E7%A7%91%E7%A0%94%E6%96%B9%E5%90%91%E6%8E%A2%E7%B4%A2/</url>
    
    <content type="html"><![CDATA[<h1 id="三.杂乱学习">三.杂乱学习</h1><h2 id="相关文章">相关文章</h2><h3 id="模仿探索与自我提升慢思考推理系统的复现之路">1.<ahref="https://mp.weixin.qq.com/s/x0a9QKcvM1uxNqcpDunzwQ">模仿、探索与自我提升：慢思考推理系统的复现之路</a></h3><p>Why: 工业界关于慢思考推理系统技术保密，作者在尝试完成一个推理系统</p><p>What:提出了一个“模仿、探索与自我提升”的三阶段训练框架，旨在模仿思维链中的思考模式。</p><p>模仿：用长思维链数据对模型进行了微调，目的在于让大模型在回答问题之前内部生成详细的推理步骤<strong>#真的能吗？内部真的是在做推理？#</strong>。看着有Prompt模板</p><p>探索：采用简单的搜索策略，对问题生成多个解答轨迹，直到找到包含正确答案的解答（说的比较宽泛）</p><p>自我提升：采用了监督微调（自监督？）；采用直接偏好优化（DPO）<strong>#第一次听说#</strong></p><blockquote><p><a href="https://arxiv.org/abs/2305.18290">Direct PreferenceOptimization: Your Language Model is Secretly a RewardModel</a>简单了解了一下，这是23年提出的一个方法，相比RLHF，计算量更轻，无需拟合奖励模型，也无需在微调期间从LM采样或执行显著的超参数调整。值得注意的是，DPO在情绪控制的能力上超越了RLHF，提高了总结和单轮对话的响应质量，同时大大简化了实现和训练。</p></blockquote><h3 id="长文干货如何从头训练大语言模型">2.<ahref="https://mp.weixin.qq.com/s/HDAW5CnbPjMqThsaeNuAfw">长文干货！如何从头训练大语言模型</a></h3><p>新词汇：Deepseek家的MLA架构、MoE（混合专家模型）、MQA → GQA →MLA（多查询注意力-&gt;分组查询注意力-&gt;多层注意力） ...太多了</p><p>近似于一篇综述，贯穿模型训练前后，只是比较个人，现在了解度太低了，<strong>可能值得再读</strong></p><p>作者强推了两篇论文，与<strong>预训练和模型规模</strong>有关，待之后有相关接触再来看</p><p><a href="https://arxiv.org/abs/2404.06395">MiniCPM: Unveiling thePotential of Small Language Models with Scalable TrainingStrategies</a></p><p><a href="https://arxiv.org/abs/2001.08361">Scaling Laws for NeuralLanguage Models</a></p><p>文末有关于当前<strong>大模型推理的挑战和方法论</strong>的综述翻译，做<strong>CoT</strong>时可以回顾。</p><h3 id="kag来了rag慌了">3.<ahref="https://mp.weixin.qq.com/s/B6ex95ZKFRvy3jyfaCHiWQ">KAG来了，RAG慌了！</a></h3><blockquote><p>OpenSPG 开源了KAG框架，通过利用知识图谱和向量检索的优势，在四个方面双向增强LLM和知识图谱，以解决<strong>RAG 存在的挑战（RAG存在着向量相似度与知识推理相关性差距大、对知识逻辑（如数值、时间关系、专家规则等）不敏感等问题</strong>，这些都阻碍了专业知识服务的落地。）</p></blockquote><p>简要介绍了KAG，<strong>日后要做再细看</strong></p><h3 id="利用树状推理路径中的错误探索信息提升使用外部工具的大模型">5.<ahref="https://mp.weixin.qq.com/s/32O2AlfgKJ3155rjSYIIFA">利用树状推理路径中的错误探索信息提升使用外部工具的大模型</a></h3><h3 id="选择你的道路llm-时代指南">6.<ahref="https://mp.weixin.qq.com/s/vfsB5t3r5dBACKQx6FshVw">选择你的道路：LLM时代指南</a></h3><p>23年8月21日的一篇文章，梳理了一部分当时LLM研究者从事的道路</p><p>有整体把握、理解的价值，可以借此考虑从哪里切入</p><p>作者分为向上和向下+艰难之路（理论、探求可解释性）</p><ol type="1"><li>向下可以再分为：<strong>Capability，Deployment</strong>，Infrastructure</li><li>向上可以再分为：Alignment，<strong>Agents</strong>，Application</li></ol><h3id="a-comprehensive-survey-of-retrieval-augmented-generation-rag-evolution-current-landscape-and-future-directions">7.<ahref="https://arxiv.org/abs/2410.12837">A Comprehensive Survey ofRetrieval-Augmented Generation (RAG): Evolution, Current Landscape andFuture Directions</a></h3><p>RAG 2024.10的综述</p><p>1.预备知识：RAG两个关键组件：检索器和生成器</p><p>2.当前的技术：</p><ol type="1"><li>检索机制：BM25（完善的老方法，对关键词匹配很有效，在理解语义方面有局限性）、DPR（密集通道检索，擅长捕获查询和文档之间的语义相似性）、REALM（检索增强语言模型，即利用LLMS来帮助判断什么更好）</li><li>生成器机制：T5（Text-to-Text TransferTransformer），BART（idirectional and Auto-Regressive Transformer)<strong>两种模型，暂时不细究</strong></li><li><strong>多模态的一些相关新研究，如果确定做RAG再回头看</strong></li></ol><p>3.当前的挑战：</p><ol type="1"><li>检索机制虽然强大，但仍然难以检索最相关的文档，特别是在处理不明确的查询或专业知识领域时。</li><li>检索和生成之间的<strong>集成</strong>虽然在理论上是无缝的，但在实践中有时会失败。例如，生成模块可能并不总是有效地将检索到的信息合并到其响应中，导致检索到的事实和生成的文本之间不一致或不连贯。</li><li>RAG模型的<strong>计算开销</strong>也是一个问题，因为它们需要为每个查询执行检索和生成步骤。这种双重过程可能会占用大量资源，特别是对于大规模应用而言</li><li>道德风险，可能放大检索来源中存在的偏见</li></ol><p>4.当前研究的方向：</p><ol type="1"><li>需要努力改进<strong>检索技术</strong>，包括结合更复杂的查询扩展和上下文消歧，以提高这些领域的性能。</li><li>研究更好的<strong>对齐机制</strong>，例如改进的注意力模型或分层融合技术</li></ol><h3 id="deepmindcot推理无需prompt也可进行一文回顾cot推理及其发展上">8.<ahref="https://www.53ai.com/news/LargeLanguageModel/2024111646023.html">DeepMind：CoT推理无需prompt也可进行，一文回顾CoT推理及其发展（上）</a></h3><p>这张图记录了一些关键进程</p><figure><imgsrc="https://api.ibos.cn/v4/weapparticle/accesswximg?aid=93864&amp;url=aHR0cHM6Ly9tbWJpei5xcGljLmNuL3N6X21tYml6X2pwZy9QMEthMVdOUndqVTBZUW5oZVFSZDZuM0ZqZnhEMGJIeUNrYjZPSlVNUXVvdXZvcWVrUzhoQ2JNSjVYa1Bwc1A1ak9ONURUWTBFaEJaZjlTMm9vdGowUS82NDA/d3hfZm10PWpwZWcmYW1w;from=appmsg"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><h1 id="四.强化性学习">四.强化性学习</h1><h3 id="kv-cache">1.KV Cache</h3><blockquote><h3 id="最新进展">最新进展</h3><ol type="1"><li><strong>缓存压缩与量化</strong>：研究者提出了多种方法来压缩 KV缓存的大小，以减少内存占用。例如，CacheGen 使用自定义张量编码器对 KV缓存进行压缩和流式处理，从而加快 LLM 系统的上下文加载速度。 <ahref="https://dl.acm.org/doi/10.1145/3651890.3672274?utm_source=chatgpt.com">ACMDigital Library</a></li><li><strong>自适应缓存压缩</strong>：一些方法根据模型的内部结构，自适应地压缩KV 缓存。例如，Adaptive KV Cache Compression方法通过分析注意力模块的结构，选择性地丢弃或压缩特定层的缓存，以减少内存占用。<ahref="https://arxiv.org/pdf/2310.01801?utm_source=chatgpt.com">arXiv</a></li><li><strong>低秩空间注意力</strong>：Eigen Attention方法在低秩空间中执行注意力操作，从而减少 KV 缓存的内存需求。<ahref="https://aclanthology.org/2024.findings-emnlp.899.pdf?utm_source=chatgpt.com">ACLAnthology</a></li><li><strong>关键令牌选择</strong>：Keyformer方法在推理时选择关键令牌，以减少 KV 缓存的大小，从而提高推理效率。<ahref="https://proceedings.mlsys.org/paper_files/paper/2024/file/48fecef47b19fe501d27d338b6d52582-Paper-Conference.pdf?utm_source=chatgpt.com">MLSysProceedings</a></li><li><strong>量化技术</strong>：KIVI 方法提出了一种无须调优的非对称 2位量化技术，用于减少 KV 缓存的内存占用，同时保持模型性能。<ahref="https://arxiv.org/abs/2402.02750?utm_source=chatgpt.com">arXiv</a><a href="https://arxiv.org/abs/2310.19102">Atom: Low-bit Quantizationfor Efficient and Accurate LLM Serving</a> 低位量化方法<strong>未看</strong></li><li><a href="https://arxiv.org/abs/2407.01527">KV Cache Compression, ButWhat Must We Give in Return? A Comprehensive Benchmark of Long ContextCapable Approaches</a>这是一篇有关评估长上下文能力的基准的论文，这里面指出了当前的一些KV优化方法：只关注了它提到的<strong>KVCache最新优化方法（可以再看）</strong> token dropping, promptcompression, linear-time sequence models, and hybrid architectures</li></ol><h3 id="面临的挑战">面临的挑战</h3><ul><li><strong>长上下文处理</strong>：随着上下文长度的增加，KV缓存的大小线性增长，导致内存瓶颈，影响推理效率。</li><li><strong>批处理大小限制</strong>：在大批处理情况下，KV缓存的内存需求显著增加，限制了模型的并行处理能力。</li><li><strong>缓存管理策略</strong>：如何有效地管理和压缩 KV缓存，以在减少内存占用的同时，保持模型性能，是一个关键问题。</li></ul><h3 id="代表性的优化方式架构">代表性的优化方式/架构</h3><ol type="1"><li><strong>MiniCache</strong>：通过在深度维度上压缩 KV缓存，减少内存占用，同时保持模型性能。 <ahref="https://arxiv.org/abs/2405.14366?utm_source=chatgpt.com">arXiv</a></li><li><strong>混合精度量化</strong>：通过对重要的 KV对采用高精度表示，次要的采用低精度表示，实现缓存压缩与性能的平衡。 <ahref="https://arxiv.org/abs/2402.18096?utm_source=chatgpt.com">arXiv</a></li><li><strong>自适应缓存合并</strong>：根据模型的需求，自适应地合并或丢弃部分KV 缓存，以减少内存占用。arXiv](https://arxiv.org/abs/2407.08454?utm_source=chatgpt.com)</li><li><strong>CXL 内存扩展</strong>：利用 Compute ExpressLink（CXL）技术，将 KV 缓存存储在扩展内存中，以缓解内存压力。 <ahref="https://mlforsystems.org/assets/papers/neurips2024/paper17.pdf?utm_source=chatgpt.com">MLforSystems</a></li></ol><h3 id="正在研究的方向">正在研究的方向</h3><ul><li><strong>缓存压缩算法</strong>：开发更高效的压缩算法，以在最小化内存占用的同时，保持或提升模型性能。</li><li><strong>硬件加速</strong>：探索专用硬件或硬件加速技术，以提高 KV缓存管理和访问的效率。</li><li><strong>动态缓存管理</strong>：研究动态调整 KV缓存大小和策略的方法，以适应不同的输入和任务需求。</li><li><strong>跨层信息共享</strong>：探索在不同层之间共享或重用 KV缓存的可能性，以减少冗余存储。</li></ul></blockquote><p>我现在的个人理解：KVCache是在注意力模块中，进行QKV矩阵计算时为了加快，将KV缓存到显存里面的一种方法。</p>]]></content>
    
    
    
    <tags>
      
      <tag>科研</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>暑期想法总结记录--幻觉</title>
    <link href="/2024/09/08/%E6%9A%91%E6%9C%9F%E6%96%87%E7%AB%A0idea%E6%80%BB%E7%BB%93/"/>
    <url>/2024/09/08/%E6%9A%91%E6%9C%9F%E6%96%87%E7%AB%A0idea%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<p>大方向：</p><ol type="1"><li>大语言模型幻觉的子问题及其背后的机制是不一样的，<strong>针对某一子问题尝试解决【细粒度】</strong>【SelfCheckGPT句子级】</li><li>定义幻觉指标：两个分类：有参考（基于经过验证的知识库/参考文献）/无参考（基于令牌概率的<strong>不确定性度量</strong>或自我一致性检查）【很多】【语义熵，F2，】</li><li>针对数据集进行优化(清理或者增强)</li><li>针对具体的任务考虑优化呢？内容概括、对话生成、生成问答、数据到文本生成、机器翻译和视觉语言生成</li><li>幻觉缓解：大方向：数据&amp;建模和模型推理数据：预训练数据、指令调整、奖励模型、<strong>RAG</strong>（效果好但偏应用没idea）建模和模型推理：RLHF、编辑模型、解码方法、思想链、后处理、合奏（多模型辩论）</li><li>多模态（视觉尤其）</li><li>长尾问题（有没有什么把高度组合的关键词结合到一起）</li></ol><p>别人走过的路：</p><ol type="1"><li>看过有两篇论文有相似的方式：通过其他模型给样本插入错误来帮助学习</li><li>溯本追源，找到大语言模型生成文本的证据，通过某种方式验证</li></ol><p>不好走的路：</p><ol type="1"><li>数字、逻辑推理类的错误（不了解内在关系）（没什么想法）</li><li>编码、<strong>解码方法</strong>的优化（缺乏实践基础）</li></ol><p>劣势：</p><ol type="1"><li>没有实践或实验，主要靠主观臆测来找切入点</li></ol><p>幻觉的分类（来自Fine-grained Hallucination Detection and Editing forLanguage Models)</p><img src="/2024/09/08/%E6%9A%91%E6%9C%9F%E6%96%87%E7%AB%A0idea%E6%80%BB%E7%BB%93/image-20240910214531242.png" class="" title="image-20240910214531242"><ul><li>是否与事实相矛盾？</li><li>是否可以基于网络知识验证？</li><li>是否涉及虚构的实体？</li><li>是否是主观的或基于事实的？</li><li>是否可以被验证？</li></ul><p>1(a).事实性矛盾</p><p>1(b).关系型矛盾</p><p>1(c).句子级矛盾</p><p>2.虚构实体</p><p>3.主观陈述</p><p>4.无法验证的陈述</p><h3 id="界定边界与不确定性">界定边界与不确定性</h3><p>不确定性估计方法的分类：基于令牌分布的（黑盒没法使用）和基于采样的[比如selfCheckGPT]</p><p>从不确定性级别看的分类：序列级[比如selfCheckGPT]、令牌级、词级</p><p>IDEA：</p><p>已走过的路：</p><ul><li><ahref="https://openreview.net/forum?id=VD-AYtP0dve">用语义熵捕获含义而非序列的不确定性</a></li><li>有一些低重要度的词却有高<strong>语义熵</strong>，和OPERA发现锚点很相似<ahref="https://arxiv.org/abs/2307.01379">将注意力转向相关性：自由形式大型语言模型的预测不确定性量化</a></li><li>"概念级"框架，将输出序列的关键含义提取出来</li></ul><img src="/2024/09/08/%E6%9A%91%E6%9C%9F%E6%96%87%E7%AB%A0idea%E6%80%BB%E7%BB%93/image-20240912123358195.png" class="" title="image-20240912123358195"><ul><li>弃权放弃回答 效果还不错<ahref="https://arxiv.org/abs/2404.10960">基于不确定性的弃权LLMs提高安全性并减少幻觉</a></li></ul><p>​</p><h3 id="不平等问题">不平等问题</h3><p>数据集的时候处理？</p><p>目前局限于黑盒，采样的方式有什么类似的可行方案吗？</p>]]></content>
    
    
    
    <tags>
      
      <tag>幻觉</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>刷算法题笔记</title>
    <link href="/2024/09/03/%E5%88%B7%E7%AE%97%E6%B3%95%E9%A2%98%E7%AC%94%E8%AE%B0/"/>
    <url>/2024/09/03/%E5%88%B7%E7%AE%97%E6%B3%95%E9%A2%98%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="历程记录">历程记录：</h1><p>使用平台：力扣</p><p>使用语言：Python(2024.9.3起)</p><p>刷题来源：</p><ol type="1"><li>LeetCode 热题 100</li></ol><p>刷题顺序：按难度从简单-到困难刷</p><h1 id="刷题笔记">刷题笔记：</h1><h1 id="python基础">Python基础</h1><h2 id="引入库">1.引入库</h2><h3 id="deque">1.1 deque</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> deque<br></code></pre></td></tr></table></figure><ol type="1"><li>创建实例 queue = collections.deque([maxlen = 5])</li><li>右侧入队 queue.append('a') 左侧入队 queue.appendleft('A')</li><li>将可迭代对象右侧入队：queue.extend(['D', 'E'])将可迭代对象右侧入队：queue.extendleft(['D','E'])</li><li>插入一个值 queue.insert(3,'T')</li><li>右边出队 queue.pop() 左边出队 queue.popleft()</li><li>复制，深拷贝 queue_b = queue.copy()</li><li>计算 queue.count('b') 找到其所在的位置 queue.index('T')</li><li>反转 queue.reverse() 每个数字=做一次右边出去然后从左边再进来queue.retate(3)</li><li>移出某个元素 queue.remove('T') 全部清理 queue.clear()</li></ol><h1 id="算法题">算法题</h1><h2 id="哈希">1.哈希</h2><h3 id="两数之和2024.9.5"><ahref="https://leetcode.cn/problems/two-sum/">1.两数之和</a>2024.9.5</h3><blockquote><p>用到enumerate函数，指出了列表中下标和值</p><p>每次如果差值在哈希表里就返回结果，不在就把值和下标加进哈希表（注意是值为哈希表的下标）</p></blockquote><h3 id="字母异位词分组-2024.9.19">2.<ahref="https://leetcode.cn/problems/group-anagrams/">字母异位词分组</a>2024.9.19</h3><p>想使用元组把字母异位词判成一类，但不好判断，还是字典好。</p><blockquote><ol type="1"><li>创建一个空字典来存储字母异位词组。</li><li>遍历字符串数组中的每个单词：<ul><li>对每个单词进行排序sorted，将排序后的字符串作为键，原始单词作为值，存储到字典中。</li></ul></li><li>遍历完成后，将字典中的每个值（列表）添加到结果列表中。</li></ol></blockquote><h3 id="最长连续序列-2024.9.19">3.<ahref="https://leetcode.cn/problems/longest-consecutive-sequence/">最长连续序列</a>2024.9.19</h3><p>想排序后再整个函数用于统计，但很明显时间复杂度大于n了，没有什么想法</p><blockquote><p>这里值得记住的是这个<strong>遍历起点优化和集合去重加速</strong>：</p><ul><li>我们只尝试从 <strong>序列的起点</strong> 开始向后扩展。</li><li>如果一个数字 <code>x</code> 是序列的起点，那么它的前一个数字<code>x-1</code> 一定不在集合中。</li><li>通过这个性质，我们可以避免不必要的计算，确保每个数字只被访问一次。</li></ul><p>集合去重加速也很重要，还可以用in快速查找(O(1))</p><p>解题思路：</p><ol type="1"><li>创建一个哈希表（例如Python中的字典），用于存储数组中每个数字的出现情况。</li><li>遍历数组中的每个数字，对于每个数字，检查它的前一个数字是否已经在哈希表中：<ul><li>如果不在，检查它的前一个数字（当前数字 -1）是否在哈希表中。如果在，说明我们可以扩展一个序列，更新哈希表，包括当前数字。</li><li>如果当前数字已经在哈希表中，那么不需要再次添加，因为我们已经考虑过它了。</li></ul></li><li>在遍历过程中，维护一个变量来记录遇到的最大序列长度。</li><li>最后返回最大序列长度。</li></ol></blockquote><h2 id="双指针">2.双指针</h2><h3 id="移动零">1.<ahref="https://leetcode.cn/problems/move-zeroes/">移动零</a></h3><p>把0移到末尾，不如<strong>角度反转</strong>，把非0移到最前面</p><h3 id="盛最多水的容器-2024.9.19">2.<ahref="https://leetcode.cn/problems/container-with-most-water/">盛最多水的容器</a>2024.9.19</h3><p>只想到遍历，更优解是双指针短的继续走</p><blockquote><p><strong>记住：关键是中间没有抵挡物，并且每次都是移动矮的一根可以遍历到最大值</strong></p><h3 id="算法步骤">算法步骤</h3><ol type="1"><li><strong>初始化</strong>：设置两个指针 <code>left</code> 和<code>right</code>，分别指向数组的开始和结束。同时，定义一个变量<code>max_area</code> 来存储遍历过程中找到的最大水容量，初始值为0。</li><li><strong>计算面积</strong>：在每一步中，计算两个指针所指的线段与x轴构成的容器的面积。面积可以通过<code>min(height[left], height[right]) * (right - left)</code>计算得出。</li><li><strong>移动指针</strong>：<ul><li>如果 <code>height[left] &lt; height[right]</code>，则<code>left</code> 指针向右移动一位（即<code>left += 1</code>），因为增加较短线的高度对总面积的增加更有效。</li><li>否则，<code>right</code> 指针向左移动一位（即<code>right -= 1</code>），因为增加较长线的高度对总面积的增加更有效。</li></ul></li><li><strong>更新最大面积</strong>：在每次计算面积后，更新<code>max_area</code>。</li><li><strong>重复</strong>：重复步骤2-4，直到两个指针相遇。</li></ol></blockquote><h3 id="三数之和-2024.9.19">3. <ahref="https://leetcode.cn/problems/3sum/">三数之和</a> 2024.9.19</h3><p><strong>记住：这个需要排序后再双指针往中间走，这样可以排除重复的并且让双指针知道怎么走</strong></p><p>只想到遍历（按一个数然后再在另外的数里面凑两个合为0的，没有想到怎么进一步优化）</p><blockquote><p>没想着排序，不排序双指针往中间走指望不上</p><ol type="1"><li><strong>排序</strong>：首先对数组进行排序，这样可以通过双指针法有效地处理重复元素，并简化双指针的移动。</li><li><strong>遍历</strong>：遍历排序后的数组，对于每个元素<code>nums[i]</code>，使用两个指针，一个指向 <code>i+1</code>（称为<code>left</code>），另一个指向数组的末尾（称为<code>right</code>）。</li><li><strong>双指针法</strong>：<ul><li>对于每个 <code>nums[i]</code>，初始化 <code>left</code> 和<code>right</code> 指针，然后使用 <code>left</code> 和<code>right</code> 指针向中间移动来寻找两个数，使得<code>nums[i] + nums[left] + nums[right] = 0</code>。</li><li>如果找到满足条件的三元组，记录下来，然后移动指针继续寻找下一个可能的三元组。</li><li>为了避免重复的三元组，每次找到三元组后，将 <code>left</code>指针向右移动一位（跳过所有相等的元素），将 <code>right</code>指针向左移动一位（跳过所有相等的元素）。</li></ul></li><li><strong>跳过重复元素</strong>：在遍历和移动指针的过程中，如果<code>nums[i]</code>与前一个元素相同，则跳过这个元素，以避免重复的三元组。</li><li><strong>返回结果</strong>：将所有找到的三元组存储在一个列表中，并返回。</li></ol></blockquote><p><strong>拓展，K数之和</strong></p><blockquote><h3 id="思路"><strong>思路</strong></h3><ol type="1"><li><strong>排序数组</strong>：<ul><li>对数组排序，方便处理重复和使用双指针。</li><li>排序的目的是让相同数字相邻，便于跳过重复的组合。</li></ul></li><li><strong>递归解决问题</strong>：<ul><li>从数组中选择一个数字，将问题转化为 <strong>K-1数之和</strong>的问题，直到简化为两数之和（用双指针解决）。</li><li>每次递归时，缩小数组范围，避免重复使用数字。</li></ul></li><li><strong>去重</strong>：<ul><li>跳过重复的数字，防止出现相同的组合。</li></ul></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">kSum</span>(<span class="hljs-params">self, nums: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], target: <span class="hljs-built_in">int</span>, k: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]]:<br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">findKSum</span>(<span class="hljs-params">nums, target, k</span>):<br>            n = <span class="hljs-built_in">len</span>(nums)<br>            res = []<br>            <span class="hljs-comment"># 如果数组长度不足k或最小值大于target或最大值小于target，直接返回空</span><br>            <span class="hljs-keyword">if</span> n &lt; k <span class="hljs-keyword">or</span> nums[<span class="hljs-number">0</span>] * k &gt; target <span class="hljs-keyword">or</span> nums[-<span class="hljs-number">1</span>] * k &lt; target:<br>                <span class="hljs-keyword">return</span> res<br><br>            <span class="hljs-comment"># 两数之和的特殊处理（用双指针解决）</span><br>            <span class="hljs-keyword">if</span> k == <span class="hljs-number">2</span>:<br>                left, right = <span class="hljs-number">0</span>, n - <span class="hljs-number">1</span><br>                <span class="hljs-keyword">while</span> left &lt; right:<br>                    total = nums[left] + nums[right]<br>                    <span class="hljs-keyword">if</span> total == target:<br>                        res.append([nums[left], nums[right]])<br>                        <span class="hljs-comment"># 跳过重复值</span><br>                        <span class="hljs-keyword">while</span> left &lt; right <span class="hljs-keyword">and</span> nums[left] == nums[left + <span class="hljs-number">1</span>]:<br>                            left += <span class="hljs-number">1</span><br>                        <span class="hljs-keyword">while</span> left &lt; right <span class="hljs-keyword">and</span> nums[right] == nums[right - <span class="hljs-number">1</span>]:<br>                            right -= <span class="hljs-number">1</span><br>                        left += <span class="hljs-number">1</span><br>                        right -= <span class="hljs-number">1</span><br>                    <span class="hljs-keyword">elif</span> total &lt; target:<br>                        left += <span class="hljs-number">1</span><br>                    <span class="hljs-keyword">else</span>:<br>                        right -= <span class="hljs-number">1</span><br>                <span class="hljs-keyword">return</span> res<br><br>            <span class="hljs-comment"># k数之和的递归处理</span><br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>                <span class="hljs-comment"># 跳过重复的数字</span><br>                <span class="hljs-keyword">if</span> i &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> nums[i] == nums[i - <span class="hljs-number">1</span>]:<br>                    <span class="hljs-keyword">continue</span><br>                <span class="hljs-comment"># 递归求解k-1数之和</span><br>                <span class="hljs-keyword">for</span> subset <span class="hljs-keyword">in</span> findKSum(nums[i + <span class="hljs-number">1</span>:], target - nums[i], k - <span class="hljs-number">1</span>):<br>                    res.append([nums[i]] + subset)<br>            <span class="hljs-keyword">return</span> res<br><br>        nums.sort()<br>        <span class="hljs-keyword">return</span> findKSum(nums, target, k)<br></code></pre></td></tr></table></figure></blockquote><h2 id="滑动窗口">3.滑动窗口</h2><p>滑动方式总结：</p><ol type="1"><li>整个窗口是固定大小，一步一步挪动，关注最后一位并且删掉最前面一位（如1）或是关注这一个整体（如2）</li><li>窗口是不固定大小，右边一直挪动直到不满足条件，让左边一直挪来恢复条件（如3）</li></ol><h3 id="无重复字符的最长子串-2024.9.19">1.<ahref="https://leetcode.cn/problems/longest-substring-without-repeating-characters/">无重复字符的最长子串</a>2024.9.19</h3><blockquote><p>思路差不多，用集合来判断是否重复，循环条件不用right一直走容易混淆，用forend in range(len(s))</p><ol type="1"><li>初始化两个指针 <code>start</code> 和 <code>end</code>分别指向字符串的起始位置，以及一个用于存储字符的集合（例如Python中的字典或集合）。</li><li>同时初始化一个变量 <code>max_len</code>来记录所遇到的最大长度。</li><li>遍历字符串，移动end指针：<ul><li>将 <code>end</code> 指针所指向的字符添加到集合中。</li><li>如果字符已经在集合中（表示重复），则移动 <code>start</code>指针（将其向右移动一位），直到该字符从集合中移除且窗口内所有字符都是唯一的。</li><li>每次移动 <code>end</code> 指针后，都更新 <code>max_len</code>，即<code>max_len = max(max_len, end - start + 1)</code>。</li></ul></li><li>重复步骤3，直到 <code>end</code> 指针遍历完整个字符串。</li></ol></blockquote><h3 id="找到字符串中所有字母异位词-2024.9.22">2.<ahref="https://leetcode.cn/problems/find-all-anagrams-in-a-string/">找到字符串中所有字母异位词</a>2024.9.22</h3><p>问题：如果p中有重复的字母，我没法用集合判等</p><blockquote><p>用字典统计出现次数，用Counter函数初始化，dels_+count[s[i-1]]来删除</p><p>关键是2</p><ol type="1"><li><strong>初始化</strong>：创建一个哈希表（字典）来存储字符串<code>p</code> 中每个字符的出现次数。</li><li><strong>滑动窗口</strong>：遍历字符串<code>s</code>，对于每个可能的起始索引，尝试找到一个长度为<code>len(p)</code> 的子串，使得该子串中字符的出现次数与 <code>p</code>相同。</li><li><strong>字符计数</strong>：对于每个子串，使用另一个哈希表来计数子串中字符的出现次数。</li><li><strong>比较哈希表</strong>：如果子串的字符计数哈希表与<code>p</code>的哈希表相等，说明找到了一个异位词，记录下这个子串的起始索引。</li><li><strong>移动窗口</strong>：移动窗口的起始索引，重复步骤3和4，直到遍历完整个字符串<code>s</code>。</li></ol></blockquote><h3 id="最大连续1的个数-iii-2024.12.4">3.<ahref="https://leetcode.cn/problems/max-consecutive-ones-iii/">最大连续1的个数III</a> 2024.12.4</h3><p>思想是接近正确的，但是没有做好滑动，我是针对每个数都要往右从这个数开始走，相当于失去了“记忆缓存”，正确的做法如上面总结。</p><blockquote><p><strong>滑动窗口的两个指针</strong>：</p><ul><li>使用 <code>left</code> 和 <code>right</code>两个指针来表示滑动窗口。<code>right</code>用于扩展窗口，<code>left</code> 用于收缩窗口。</li></ul><p><strong>更新 <code>k</code> 的逻辑</strong>：</p><ul><li>每次遇到一个 <code>0</code>，减少 <code>k</code>。</li><li>如果 <code>k</code> 变得小于 0，表示窗口内有太多的0，需要收缩窗口，移动左指针，并恢复 <code>k</code>。</li></ul><p><strong>计算窗口大小</strong>：</p><ul><li>每次窗口扩展时，计算当前窗口的大小，并更新最大长度<code>max_len</code>。</li></ul><p><strong>时间复杂度</strong>：</p><ul><li>时间复杂度是 O(n)，其中 <code>n</code> 是数组 <code>nums</code>的长度。每个元素最多被左右指针访问一次。</li></ul></blockquote><h2 id="子串">4.子串</h2><h3 id="和为-k-的子数组-2024.9.23">1.<ahref="https://leetcode.cn/problems/subarray-sum-equals-k/">和为 K的子数组</a> 2024.9.23</h3><p>我想排序后用滑动窗口的角度一路划过去，但子数组是连续的；不排序又没法一直前滑</p><blockquote><p>新概念：前缀和，便于计算子串和</p></blockquote><blockquote><p>用了哈希表来统计<strong>前缀和</strong>和出现的次数，只需遍历一次</p><p>这种方法的思路是，对于每个子数组，其和可以表示为两个前缀和之差。具体步骤如下：</p><ol type="1"><li>初始化一个变量 <code>count</code> 来记录和为 <code>k</code>的子数组数量，以及一个哈希表 <code>prefix_sum</code>来存储前缀和出现的次数，键是前缀和的值，值是该前缀和出现的次数。</li><li>遍历数组 <code>nums</code>，计算从数组开始到当前位置的前缀和。</li><li>对于每个前缀和，检查 <code>prefix_sum[prefix - k]</code>的值。如果存在，那么表示我们找到了一个和为 <code>k</code>的子数组。我们将这个值加到 <code>count</code> 上，因为这意味着从索引<code>0</code> 到当前索引 <code>i</code> 的子数组和，与从某个索引<code>j</code>（<code>j &lt; i</code>）到当前索引 <code>i</code>的子数组和的差为 <code>k</code>。</li><li>更新 <code>prefix_sum</code> 哈希表，记录当前前缀和的出现次数。</li><li>最后返回 <code>count</code>。</li></ol></blockquote><h2 id="普通数组">5.普通数组</h2><h3 id="最大子数组和-2024.9.25">1.<ahref="https://leetcode.cn/problems/maximum-subarray/">最大子数组和</a>2024.9.25</h3><p>没法用前缀和</p><blockquote><p>卡登算法（用于找最大子数组和），关键是看是继续现有的数组还是新开一个。算是动态规划</p><ol type="1"><li><p><strong>初始化</strong>：首先，我们初始化两个变量，<code>max_current</code>和<code>max_global</code>。这两个变量都设置为数组的第一个元素。<code>max_current</code>表示包含当前元素的最大子数组和，<code>max_global</code>表示全局找到的最大子数组和。</p></li><li><p><strong>遍历数组</strong>：然后，算法遍历数组，从第二个元素开始。对于每个元素，我们考虑两个选择：</p><ul><li>从当前元素开始一个新的子数组。</li><li>将当前元素添加到前一个元素的子数组中。</li></ul><p>这可以用数学公式表示为：max_current=max⁡(nums[i],max_current+nums[i])max_current=max(nums[<em>i</em>],max_current+nums[<em>i</em>])其中 <code>nums[i]</code> 是当前遍历到的数组元素。</p></li><li><p><strong>更新全局最大值</strong>：接着，我们使用<code>max_current</code> 更新 <code>max_global</code>：max_global=max⁡(max_global,max_current)max_global=max(max_global,max_current)</p></li><li><p><strong>继续遍历</strong>：重复步骤2和3，直到遍历完整个数组。</p></li><li><p><strong>返回结果</strong>：遍历完成后，<code>max_global</code>将包含整个数组的最大子数组和。</p></li></ol></blockquote><h3 id="合并区间-2024.9.25">2.<ahref="https://leetcode.cn/problems/merge-intervals/description/?envType=study-plan-v2&amp;envId=top-100-liked">合并区间</a>2024.9.25</h3><p>先排序，新的如果有合并再改</p><h3 id="轮转数组-2024.9.26">3.<ahref="https://leetcode.cn/problems/rotate-array/">轮转数组</a>2024.9.26</h3><p>没想到空间复杂度为O(1)的方法</p><blockquote><p>常规想不到的方法：三步反转法</p><ol type="1"><li><strong>反转整个数组</strong>：首先反转整个数组。</li><li><strong>反转数组的前 k 个元素</strong>：然后反转数组的前 k个元素。</li><li><strong>反转数组剩余的元素</strong>：最后反转数组剩余的元素。</li></ol><p>如果 k 大于数组长度，我们只需要对 k 取余。</p></blockquote><h3 id="除自身以外数组的乘积-2024.9.27">4.<ahref="https://leetcode.cn/problems/product-of-array-except-self/">除自身以外数组的乘积</a>2024.9.27</h3><blockquote><p>用到了前缀和（乘积版本的）记录每个数左右两边的乘积和，在基础版本上有把空间复杂度优化为O(1)的方法</p><p><strong>前缀积和后缀积：</strong>我们可以分别计算每个元素的“前缀积”和“后缀积”。</p><ul><li>前缀积：数组中每个元素左侧所有元素的乘积。</li><li>后缀积：数组中每个元素右侧所有元素的乘积。</li></ul><p><strong>构建结果数组：</strong>利用前缀积和后缀积的思想，我们可以用两次遍历实现。</p><ul><li>第一次遍历，计算并存储每个位置的前缀积。</li><li>第二次遍历，计算并存储每个位置的后缀积，同时更新结果数组。</li></ul><p><strong>空间优化：</strong>因为我们不可以使用额外空间，所以需要直接在 <code>answer</code>数组上操作，利用它来保存中间的计算结果。</p></blockquote><h3 id="section">5.</h3><h2 id="矩阵">6.矩阵</h2><h3 id="矩阵置零-2024.9.27">1.<ahref="https://leetcode.cn/problems/set-matrix-zeroes/">矩阵置零</a>2024.9.27</h3><p>idea: <del>直接遍历</del></p><blockquote><p>关键在标志要处理的行和列；其次是第三部处理的时候的顺序</p><p><strong>使用首行和首列作为标记：</strong></p><ul><li>我们不使用额外的标记矩阵，而是利用矩阵的首行和首列来记录哪些行和列需要被置零。</li><li>首先遍历整个矩阵，如果元素为<code>0</code>，则将对应的行首和列首位置为<code>0</code>，作为标记。</li></ul><p><strong>检查标记，置零行和列：</strong></p><ul><li>再次遍历矩阵，根据首行和首列的标记来将对应的行和列置零。</li></ul><p><strong>处理首行和首列：</strong></p><ul><li>因为首行和首列被用作标记，在前两步处理时可能会改变它们。我们需要单独记录首行和首列是否需要置零，最后再处理它们。</li></ul></blockquote><h3 id="螺旋矩阵-2024.9.27">2.<ahref="https://leetcode.cn/problems/spiral-matrix/">螺旋矩阵</a>2024.9.27</h3><p>idea:再设置一个同意=一大小的标记矩阵，每次走到头或者遇到标记就转向<del>(一个大for四个while)</del></p><blockquote><p>GPT补充：可以用这个来判断方向 directions = [(0, 1), (1, 0), (0, -1),(-1, 0)] # (row_change, col_change)</p><p>next_row = row + directions[dir_index][0] next_col = col +directions[dir_index][1]</p></blockquote><h3 id="旋转图像-2024.9.28">3.<ahref="https://leetcode.cn/problems/rotate-image/">旋转图像</a>2024.9.28</h3><p>idea: 第一次做，想不到</p><blockquote><p>矩阵顺时针旋转90°的方法：</p><p><strong>矩阵的转置：</strong>将矩阵的行和列互换，这样可以将元素的相对位置调整为顺时针旋转的基础。</p><p><strong>水平翻转：</strong>在转置之后，反转每一行，从而完成顺时针的旋转。</p></blockquote><h2 id="链表">7.链表</h2><h3 id="环形链表">1.<ahref="https://leetcode.cn/problems/linked-list-cycle/">环形链表</a></h3><blockquote><p>判断是否有环用到了快慢指针法</p></blockquote><h3 id="环形链表-ii-2024.9.28">2.<ahref="https://leetcode.cn/problems/linked-list-cycle-ii/">环形链表II</a> 2024.9.28</h3><blockquote><p>没想到快慢指针法还是能做</p><p>这个算法的核心是使用两个指针：一个快指针每次移动两步，另一个慢指针每次移动一步。如果链表中存在环，快慢指针最终会相遇。</p><p>一旦相遇，我们可以通过以下步骤找到环的起始节点：</p><ol type="1"><li><strong>相遇后，重置一个指针到链表头节点。</strong></li><li><strong>两个指针以相同的速度（每次移动一步）移动，直到它们再次相遇。</strong></li></ol><p>这个相遇点就是环的起始节点。</p></blockquote><h3 id="两数相加-2024.9.28">3.<ahref="https://leetcode.cn/problems/add-two-numbers/">两数相加</a>2024.9.28</h3><blockquote><p>更好的处理：循环条件中加入进位，提高了代码的简洁性，结果返回头结点.next</p></blockquote><h3 id="删除链表的倒数第-n-个结点-2024.9.28">4.<ahref="https://leetcode.cn/problems/remove-nth-node-from-end-of-list/">删除链表的倒数第N 个结点</a> 2024.9.28</h3><p>Idea:遍历知道数量后处理，删除头结点需要特殊处理</p><blockquote><p>更好的方法：双指针</p><p>这种方法的核心是使用两个指针来遍历链表，确保在到达链表末尾时，第一个指针比第二个指针多移动n个节点。这样，当第一个指针到达链表末尾时，第二个指针正好指向要删除的节点的前一个节点。</p></blockquote><h3 id="两两交换链表中的节点-2024.9.28">5.<ahref="https://leetcode.cn/problems/swap-nodes-in-pairs/">两两交换链表中的节点</a>2024.9.28</h3><p>idea:我想只用两个指针进行交换，但实际上需要第三个指针留在上一组交换的最后一个来让他指向新的这组的第二个</p><h3 id="随机链表的复制-2024.10.10">6.<ahref="https://leetcode.cn/problems/copy-list-with-random-pointer/">随机链表的复制</a>2024.10.10</h3><p>idea:准备直接复制，但是random其实指向的是原来的点</p><blockquote><p>特别方法：三次遍历</p><p>新概念：深拷贝</p><p>为了深拷贝一个带有随机指针的链表，我们可以通过三次遍历链表来实现深拷贝：</p><ol type="1"><li><strong>第一遍遍历：复制节点</strong>在原链表的每个节点后面插入一个复制的新节点。例如，假设原链表有节点<code>A -&gt; B -&gt; C</code>，则在这一遍遍历之后，我们将得到<code>A -&gt; A' -&gt; B -&gt; B' -&gt; C -&gt; C'</code>。其中每个节点的副本紧挨在它的后面。</li><li><strong>第二遍遍历：复制随机指针</strong> 遍历链表，将原节点的<code>random</code> 指针复制给对应副本节点的 <code>random</code>指针。也就是对于每个原节点 <code>A</code>，其副本节点 <code>A'</code> 的<code>random</code> 指针指向 <code>A.random.next</code>。</li><li><strong>第三遍遍历：拆分链表</strong>将复制的链表与原链表分离，形成一个新的链表，并返回新链表的头节点。</li></ol></blockquote><h3 id="排序链表-2024.10.10">7.<ahref="https://leetcode.cn/problems/sort-list/">排序链表</a>2024.10.10</h3><p>idea:准备还是用排序算法，只是用于链表</p><blockquote><p>GPT指出链表排序最常用的是归并排序，一种递归方法，空间/时间复杂度：<spanclass="math inline">\(nlog^{n} 和 log^n\)</span></p><ol type="1"><li>利用快慢指针找到链表的中点，并将链表分成两半。</li><li>对每一半递归地进行归并排序。</li><li>最后将两半排好序的链表合并，返回合并后的有序链表。</li></ol></blockquote><h3 id="lru-缓存-2024.10.11">8.<ahref="https://leetcode.cn/problems/lru-cache/">LRU 缓存</a>2024.10.11</h3><p>idea: 准备使用字典，但是该怎么标记来实现LRU呢？</p><blockquote><p>通过双向链表维护键值对顺序，方便在O(1)时间内移动访问和插入</p><p><strong>双向链表</strong>：</p><ul><li>使用虚拟头节点 <code>head</code> 和尾节点<code>tail</code>，使得插入和删除节点更加方便，无需处理空链表的特殊情况。</li><li><code>_remove(node)</code>：从链表中删除某个节点。</li><li><code>_add_to_head(node)</code>：将某个节点插入到链表的头部。</li></ul><p><strong><code>get(key)</code> 操作</strong>：</p><ul><li>通过哈希表在 O(1) 时间内找到 <code>key</code> 对应的节点。</li><li>访问后将该节点移动到链表头部（因为它是最近使用的节点）。</li><li>如果 <code>key</code> 不存在，返回 -1。</li></ul><p><strong><code>put(key, value)</code> 操作</strong>：</p><ul><li>如果 <code>key</code>已经存在，更新节点的值，并将该节点移动到链表头部。</li><li>如果 <code>key</code>不存在，检查缓存是否已满，如果满了则删除链表尾部的最久未使用的节点，再将新节点插入到链表头部。</li></ul></blockquote><h2 id="二叉树">8.二叉树</h2><h3 id="二叉树的中序遍历">1.二叉树的中序遍历</h3><p>递归比较简单</p><p>非递归通过栈实现，两个while循环，外层是当前节点和栈都空的时候结束，内层是当前节点为空时结束，当内层结束的时候从栈中抛出一个节点。</p><blockquote><ul><li>使用一个栈 <code>stack</code> 来存储未访问的节点。</li><li>使用一个指针 <code>current</code> 来遍历树。</li><li>当 <code>current</code> 非空时，一直沿着左子树向下遍历，直到<code>current</code> 为空，然后将这些节点依次压入栈中。</li><li>当 <code>current</code>为空时，从栈中弹出一个节点，访问它，然后转向它的右子树继续遍历。</li></ul></blockquote><h3 id="二叉树的最大深度">2.二叉树的最大深度</h3><p>自己的想法：中序遍历+外面一个记录量加一个列表记录每个节点深度</p><p>优化：</p><ul><li>不用nonlocal关键字而是把记录量作为参数传递</li><li>不用记录每个节点深度，直接在递归过程中计算最大深度</li></ul><p>简洁版：递归计算max(左右子树)的最深根 非递归版：队列BFS</p><h3 id="翻转二叉树-2024.9.10">3.翻转二叉树 2024.9.10</h3><p>递归：简单</p><p>非递归：栈/队列 用collections中的deque代替queue有更好的效果？</p><h3 id="检查二叉树是否对称-2024.9.11">4.检查二叉树是否对称2024.9.11</h3><p>递归：注意判断条件，左右值相等时递归看左子树的左边和右子树的右边and左子树的右边和右子树的左边</p><p>非递归：用栈/队列，每个元素是一个元组把左右节点放在一起，while stack:内部思路和递归一样</p><h3 id="二叉树直径-2024.9.12">5.二叉树直径 2024.9.12</h3><p>没什么想法。。。</p><blockquote><p>子函数先递归找左右最长路径，然后更新max，返回的时候返回当前节点最大深度（注意要加一）</p></blockquote><h3 id="将有序数组转换为二叉搜索树-2024.9.18">6.<ahref="https://leetcode.cn/problems/convert-sorted-array-to-binary-search-tree/">将有序数组转换为二叉搜索树</a>2024.9.18</h3><blockquote><p>以中间为分割，左右递归调用</p></blockquote><h3 id="二叉树的层序遍历-2024.10.15">7.<ahref="https://leetcode.cn/problems/binary-tree-level-order-traversal/">二叉树的层序遍历</a>2024.10.15</h3><p>使用队列+BFS</p><h3 id="验证二叉搜索树-2024.10.15">8.<ahref="https://leetcode.cn/problems/validate-binary-search-tree/">验证二叉搜索树</a>2024.10.15</h3><p>我想递归调用自己，但是无法记录上面节点的情况；我考虑用队列来遍历，仍然无法解决上面的情况</p><blockquote><p>通过一个辅助函数来维护当前节点值的上下界限</p><p>def helper(node, low, high)</p><p>Ps：队列也可以，把上下限一起存为一个元组</p></blockquote><h3 id="二叉搜索树中第-k-小的元素-2024.10.15">9.<ahref="https://leetcode.cn/problems/kth-smallest-element-in-a-bst/">二叉搜索树中第K 小的元素</a> 2024.10.15</h3><p>思路没问题，中序遍历自然产生了一个升序的数组</p><h3 id="二叉树的右视图-2024.10.15">10.<ahref="https://leetcode.cn/problems/binary-tree-right-side-view/">二叉树的右视图</a>2024.10.15</h3><p>idea:BFS遍历把每一层最后一个打印出来</p><blockquote><p>优化1.只记录每一层最后一个，不需要都记录</p><p>方法2 DFS，优先遍历右子树</p></blockquote><h3 id="二叉树展开为链表-2024.10.15">11.<ahref="https://leetcode.cn/problems/flatten-binary-tree-to-linked-list/">二叉树展开为链表</a>2024.10.15</h3><p>idea：想按顺序（前序遍历）但没想好该怎么连接</p><blockquote><p>用栈来连接</p></blockquote><h3 id="从前序与中序遍历序列构造二叉树-2024.10.17">12. <ahref="https://leetcode.cn/problems/construct-binary-tree-from-preorder-and-inorder-traversal/">从前序与中序遍历序列构造二叉树</a>2024.10.17</h3><p>idea:没有想法</p><blockquote><h4 id="确定根节点">(1) <strong>确定根节点</strong></h4><ul><li><strong>前序遍历</strong>中的第一个元素一定是当前树的根节点。因此，首先可以通过<code>preorder[0]</code> 找到根节点。</li></ul><h4 id="划分左右子树">(2) <strong>划分左右子树</strong></h4><ul><li>在 <strong>中序遍历</strong>中，根节点将左右子树划分开。根节点左边的部分是左子树，右边的部分是右子树。</li><li>找到 <strong>中序遍历</strong>中根节点的位置后，<code>inorder[:root_index]</code>是左子树，<code>inorder[root_index + 1:]</code> 是右子树。</li></ul><h4 id="递归构造左右子树">(3) <strong>递归构造左右子树</strong></h4><ul><li>根据 <strong>中序遍历</strong> 划分出的左右子树的大小，可以从<strong>前序遍历</strong> 中提取对应的左右子树。</li><li>对左子树：前序遍历中的下一个元素到左子树长度的位置是左子树的节点，即<code>preorder[1:len(left_inorder)+1]</code>。</li><li>对右子树：剩下的部分是右子树，即<code>preorder[len(left_inorder)+1:]</code>。</li><li>递归地重复这个过程，构造左子树和右子树。</li></ul><h4 id="递归终止条件">(4) <strong>递归终止条件</strong></h4><ul><li>当 <strong>前序遍历</strong> 或 <strong>中序遍历</strong>为空时，说明子树为空，返回 <code>None</code>，表示没有子节点。</li></ul></blockquote><h3 id="路径总和-iii-2024.10.17">13.<ahref="https://leetcode.cn/problems/path-sum-iii/">路径总和 III</a>2024.10.17</h3><p>没有想法</p><blockquote><p>前缀遍历+前缀和</p><p><strong>前缀和哈希表</strong>：</p><ul><li>我们使用一个字典 <code>prefix_sum_count</code>来记录每一个前缀和出现的次数。</li><li>初始时，前缀和为 0 的路径出现过一次，即<code>prefix_sum_count = &#123;0: 1&#125;</code>，因为从根节点到自身是一个默认的路径。</li></ul><p><strong>DFS 递归遍历</strong>：</p><ul><li>对每一个节点，我们更新当前路径的前缀和<code>current_sum</code>。</li><li>然后通过查找 <code>prefix_sum_count[current_sum - targetSum]</code>来判断以当前节点为终点的路径中，是否存在前缀和等于<code>current_sum - targetSum</code> 的路径，这意味着这段路径的和正好为<code>targetSum</code>。</li><li>遍历完左右子树后，回溯时要将当前的 <code>current_sum</code>从前缀和表中移除，防止影响其他分支的计算。</li></ul><p><strong>递归终止条件</strong>：</p><ul><li>当 <code>node</code> 为 <code>None</code> 时，返回0，表示没有路径。</li></ul></blockquote><ol start="14" type="1"><li><h3 id="二叉树的最近公共祖先-2024.10.17"><ahref="https://leetcode.cn/problems/lowest-common-ancestor-of-a-binary-tree/">二叉树的最近公共祖先</a>2024.10.17</h3></li></ol><p>没有想法</p><blockquote><p>我根据提出的思路写的代码，我写了四个判等式，可以缩成一个</p><figure class="highlight coq"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs coq"><span class="hljs-keyword">return</span> <span class="hljs-built_in">left</span> <span class="hljs-keyword">if</span> <span class="hljs-built_in">left</span> <span class="hljs-keyword">else</span> <span class="hljs-built_in">right</span><br></code></pre></td></tr></table></figure><p>时间效率也大大提升</p><p>如果当前节点为 <code>None</code>，返回 <code>None</code>。</p><p>如果当前节点是 <code>p</code> 或<code>q</code>，直接返回当前节点。</p><p>递归左右子树，检查左右子树是否包含 <code>p</code> 或<code>q</code>。</p><ul><li>如果左子树返回的值不为 <code>None</code>，而右子树返回的值也不为<code>None</code>，则说明当前节点 <code>root</code> 是 <code>p</code> 和<code>q</code> 的最近公共祖先。</li><li>如果只有左子树返回非空，说明最近公共祖先在左子树，直接返回左子树的结果。</li><li>如果只有右子树返回非空，说明最近公共祖先在右子树，直接返回右子树的结果。</li></ul></blockquote><h2 id="图论">9.图论</h2><h3 id="岛屿数量-2024.10.18">1.<ahref="https://leetcode.cn/problems/number-of-islands/">岛屿数量</a>2024.10.18</h3><p>没有想法</p><blockquote><p>看到思路后我自己的想法是新设一个同样大小的二维网格visited,访问过的做个标记</p><p>gpt给出的更便捷的方法，修改了原网格但是空间复杂度减小</p><p>遍历网格的每个位置，当遇到 '1' 时，开始一次 DFS 或 BFS，把所有连接的'1' 变成 '0'（标记为已访问），并增加岛屿计数器。</p><p>如果遇到 '0'，直接跳过。</p><p>遍历完成后，返回岛屿计数器。</p></blockquote><h3 id="腐烂的橘子-2024.10.18">2.<ahref="https://leetcode.cn/problems/rotting-oranges/">腐烂的橘子</a>2024.10.18</h3><p>思路错了，用dfs很麻烦，易错（也是我DFS写错了），该用bfs，适合这种层级扩散的</p><blockquote><p>BFS：</p><p><strong>初始化</strong>：将所有腐烂的橘子（值为2）作为起点放入队列，并记录所有新鲜橘子的数量。</p><p><strong>BFS扩展</strong>：对于每分钟的每个腐烂橘子，检查它的四个相邻方向（上、下、左、右）。如果有新鲜橘子（值为1），它会变成腐烂的橘子，同时将其加入队列，记录这个橘子变腐烂的分钟数。</p><p><strong>终止条件</strong>：当队列为空时，表示所有可以腐烂的橘子都已处理完毕。如果还有新鲜橘子未被腐烂，则返回-1；否则返回所用的分钟数。</p><h3 id="用-dfs-实现的思路">用 DFS 实现的思路：</h3><ol type="1"><li><strong>腐烂传播的递归</strong>：每次从一个腐烂橘子出发，尝试递归地向四周扩散到相邻的新鲜橘子，让它们腐烂。</li><li><strong>计时逻辑</strong>：需要记录每个新鲜橘子腐烂所需的时间。在DFS 中，每深入一次递归，都需要记录腐烂的深度，代表经过的时间。</li><li><strong>提前退出条件</strong>：在所有橘子腐烂完成后，提前返回结果。</li></ol></blockquote><h3 id="课程表-2024.11.20">3.<ahref="https://leetcode.cn/problems/course-schedule/">课程表</a>2024.11.20</h3><p>想到图，如果存在一条路径能产生环，则说明不满足</p><p>但不知道怎么转为代码表示</p><blockquote><p>思路是正确的，dfs方法比较难</p><h3 id="方法-1dfs-检测环">方法 1：DFS 检测环</h3><ul><li>将课程和它们的先修关系表示为一个有向图（邻接表）。</li><li>对图中的每个节点进行 <strong>DFS</strong>，检测是否存在<strong>环</strong>。</li><li>如果访问一个节点时再次遇到正在访问中的节点（递归栈中的节点），说明存在环。</li><li>如果没有环，则可以完成所有课程。</li></ul><p>关键： from collections import defaultdict</p><p>​ # 构建邻接表</p><p>​ graph = defaultdict(list)</p><p>​ for course, prereq in prerequisites:</p><p>​ graph[prereq].append(course)</p><h3 id="方法-2bfs拓扑排序">方法 2：BFS（拓扑排序）</h3><ul><li>使用入度表记录每门课程的前置课程数。</li><li>每次将入度为 0的课程加入队列，并移除它对其他课程的影响（即减少其他课程的入度）。</li><li>如果最后所有课程都被学习过，则返回 <code>True</code>，否则返回<code>False</code>。</li></ul></blockquote><h3 id="实现-trie-前缀树-2024.11.21">4.<ahref="https://leetcode.cn/problems/implement-trie-prefix-tree/">实现Trie (前缀树)</a> 2024.11.21</h3><p>没什么想法，要快速查找肯定是字典（哈希表），但找前缀只想到挨着找</p><blockquote><p>可以利用嵌套字典或专门的节点类</p><p>使用了节点类的方法，每个节点都有一个字典，和一个is_end属性</p></blockquote><h2 id="回溯">10.回溯</h2><h3 id="全排列-2024.11.21">1.<ahref="https://leetcode.cn/problems/permutations/">全排列</a>2024.11.21</h3><p>想树型+dfs，但不知道怎么实现和递归以及回溯</p><blockquote><h3id="没有想到这样递归和回溯交换位置">没有想到这样递归和回溯（交换位置）</h3><h3 id="代码设计思路">代码设计思路</h3><ol type="1"><li><strong>递归的定义</strong>：<ul><li>每次递归尝试固定当前的位置（<code>start</code>）。</li><li>当所有位置都固定好（<code>start == len(nums)</code>），记录当前排列。</li></ul></li><li><strong>交换元素</strong>：<ul><li>将 <code>nums[start]</code> 与 <code>nums[i]</code>交换，把当前位置的元素换到合适的地方。</li></ul></li><li><strong>回溯</strong>：<ul><li>每次递归完成后，恢复原状态（交换回去），以便探索其他可能性。</li></ul></li><li><strong>复杂度分析</strong>：<ul><li>时间复杂度：O(n×n!)O(n n!)O(n×n!)，其中 n!n!n! 是排列的总数，额外的nnn 用于深拷贝。</li><li>空间复杂度：O(n)O(n)O(n)，递归栈的深度。</li></ul></li></ol><p>递归是一种非常强大的编程方法，但其设计需要一些通用的思路和步骤来确保代码正确且高效。以下是递归设计的通用思路以及实现递归算法时需要遵循的框架：</p><hr /><h3 id="递归的通用思路"><strong>递归的通用思路</strong></h3><p>递归的核心是将问题分解为<strong>更小的子问题</strong>，然后通过<strong>解决子问题来解决整体问题</strong>。它通常包括以下几个部分：</p><h4 id="确定递归的目标"><strong>1. 确定递归的目标</strong></h4><ul><li>问题是什么？最终结果是什么？</li><li>例如，计算斐波那契数列，目标是找到第 n 项。</li></ul><h4 id="确定递归的-最小子问题base-case"><strong>2. 确定递归的</strong><strong>“最小子问题”（base case）</strong></h4><ul><li>什么情况下可以直接返回结果（无需进一步递归）？</li><li>例如，求解二叉树的深度，空树直接返回 0。</li></ul><h4 id="找出递归关系"><strong>3. 找出递归关系</strong></h4><ul><li>当前问题与更小问题之间的联系是什么？</li><li>例如，斐波那契数列：<code>F(n) = F(n-1) + F(n-2)</code>。</li></ul><h4 id="确定递归参数"><strong>4. 确定递归参数</strong></h4><ul><li>每次递归需要传递什么参数？参数如何随着递归变化？</li><li>例如，路径问题中传递当前位置、剩余步数。</li></ul><h4 id="确定返回值"><strong>5. 确定返回值</strong></h4><ul><li>每次递归的返回值是什么？如何利用子问题的返回值组合成最终结果？</li><li>例如，路径计数问题中，返回从当前点到终点的路径数量。</li></ul><hr /><h3 id="递归的设计框架"><strong>递归的设计框架</strong></h3><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs applescript">python复制代码def recursive_function(parameters):<br>    <span class="hljs-comment"># 1. 确定递归结束条件（Base Case）</span><br>    <span class="hljs-keyword">if</span> some_condition:<br><span class="hljs-built_in">        return</span> some_value<br><br>    <span class="hljs-comment"># 2. 处理当前层逻辑（与子问题相关联）</span><br>    <span class="hljs-comment"># 拆分成更小的问题</span><br><br>    <span class="hljs-comment"># 3. 递归调用下一层（处理更小的子问题）</span><br>    <span class="hljs-literal">result</span> = recursive_function(smaller_parameters)<br><br>    <span class="hljs-comment"># 4. 合并子问题结果</span><br>    <span class="hljs-comment"># 根据返回的子问题结果更新或计算</span><br><span class="hljs-built_in"></span><br><span class="hljs-built_in">    return</span> <span class="hljs-literal">result</span><br></code></pre></td></tr></table></figure><hr /><h3 id="递归的具体应用举例"><strong>递归的具体应用举例</strong></h3><h4 id="斐波那契数列"><strong>1. 斐波那契数列</strong></h4><p>目标：计算第 n 项斐波那契数。</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">python</span>复制代码def fibonacci(n):<br>    <span class="hljs-comment"># Base Case: 最小子问题</span><br>    <span class="hljs-attribute">if</span> n == <span class="hljs-number">0</span>:<br>        <span class="hljs-attribute">return</span> <span class="hljs-number">0</span><br>    <span class="hljs-attribute">if</span> n == <span class="hljs-number">1</span>:<br>        <span class="hljs-attribute">return</span> <span class="hljs-number">1</span><br><br>    <span class="hljs-comment"># Recursive Relation: 当前问题的解由子问题组合而成</span><br>    <span class="hljs-attribute">return</span> fibonacci(n - <span class="hljs-number">1</span>) + fibonacci(n - <span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure><h4 id="二叉树的最大深度-1"><strong>2. 二叉树的最大深度</strong></h4><p>目标：计算二叉树的最大深度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">python复制代码<span class="hljs-keyword">def</span> <span class="hljs-title function_">maxDepth</span>(<span class="hljs-params">root</span>):<br>    <span class="hljs-comment"># Base Case: 如果是空树，深度为 0</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> root:<br>        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span><br><br>    <span class="hljs-comment"># Recursive Relation: 树的深度 = 左右子树的最大深度 + 1</span><br>    left_depth = maxDepth(root.left)<br>    right_depth = maxDepth(root.right)<br><br>    <span class="hljs-comment"># 合并结果</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">max</span>(left_depth, right_depth) + <span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><h4 id="子集问题"><strong>3. 子集问题</strong></h4><p>目标：给定一个数组，返回所有可能的子集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python">python复制代码<span class="hljs-keyword">def</span> <span class="hljs-title function_">subsets</span>(<span class="hljs-params">nums</span>):<br>    result = []<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backtrack</span>(<span class="hljs-params">index, path</span>):<br>        <span class="hljs-comment"># Base Case: 当前路径是一个子集</span><br>        result.append(path[:])<br><br>        <span class="hljs-comment"># Recursive Relation: 遍历剩余的数字</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(index, <span class="hljs-built_in">len</span>(nums)):<br>            path.append(nums[i])  <span class="hljs-comment"># 做选择</span><br>            backtrack(i + <span class="hljs-number">1</span>, path)  <span class="hljs-comment"># 递归</span><br>            path.pop()  <span class="hljs-comment"># 撤销选择</span><br><br>    backtrack(<span class="hljs-number">0</span>, [])<br>    <span class="hljs-keyword">return</span> result<br></code></pre></td></tr></table></figure><hr /><h3 id="递归设计的注意事项"><strong>递归设计的注意事项</strong></h3><h4 id="必须有明确的递归终止条件"><strong>1.必须有明确的递归终止条件</strong></h4><ul><li>如果递归没有合适的结束条件，就会陷入死循环。</li><li>例如，<code>factorial(n)</code> 中必须明确<code>if n == 0: return 1</code>。</li></ul><h4 id="避免重复计算"><strong>2. 避免重复计算</strong></h4><ul><li>对于一些问题（如斐波那契数列），同一个子问题可能被多次求解。</li><li>解决方法：<ul><li><strong>记忆化递归</strong>：用缓存记录已经计算过的结果。</li><li><strong>动态规划</strong>：用迭代替代递归。</li></ul></li></ul><h4 id="注意参数变化方向"><strong>3. 注意参数变化方向</strong></h4><ul><li>每次递归调用时，确保参数在向终止条件收敛。</li><li>如果参数变化方向错误，会导致无限递归。</li></ul><h4 id="考虑递归深度"><strong>4. 考虑递归深度</strong></h4><ul><li>如果问题规模较大，递归可能导致栈溢出。</li><li>解决方法：<ul><li>转换为迭代。</li><li>使用尾递归优化（Python 不支持真正的尾递归优化）。</li></ul></li></ul><h4 id="分析复杂度"><strong>5. 分析复杂度</strong></h4><ul><li>递归的时间和空间复杂度取决于递归树的大小。</li><li>确保递归的规模和层数不会导致效率低下。</li></ul></blockquote><h3 id="子集-2024.11.22">2.<ahref="https://leetcode.cn/problems/subsets/">子集</a> 2024.11.22</h3><p>不知道怎么构造出不重复的</p><blockquote><p>可以通过索引，向右推进，保证不会重复选择之前的元素</p><p>遍历始终向后推进（通过<code>for i in range(index, len(nums))</code>）。</p><p>每次路径的状态通过回溯（<code>path.pop()</code>）及时清理，防止干扰。</p><p>子集生成顺序由递归逻辑控制，不会重复计算或插入。</p></blockquote><h3 id="电话号码的字母组合-2024.11.23">3.<ahref="https://leetcode.cn/problems/letter-combinations-of-a-phone-number/">电话号码的字母组合</a>2024.11.23</h3><p>通过</p><p>和上一个子集有点像，传递index和path</p><p>这里我是通过列表转成字符串的，用的 ''.join(path)</p><h3 id="组合总和-2024.11.23">4.<ahref="https://leetcode.cn/problems/combination-sum/">组合总和</a>2024.11.23</h3><p>感觉会用一个记忆化递归，每次尝试后用一个字典记录结果和对应的组合</p><p><del>不可能通过遍历所有组合的可能，因为一个数字能选多次</del>（正确方案）</p><p>所以只能拿一个数然后看差值，但接下来怎么选择呢？</p><p>用index逐步推进吗？</p><blockquote><p>真是遍历所有在sum和小于target</p><p>参数传了remaining, start, path 避免了重复</p><p>注意用result.append(path[:]) 深拷贝</p><h3 id="解题思路">解题思路</h3><ol type="1"><li><p><strong>目标</strong>：在 <code>candidates</code>数组中选择一些数字，其和等于 <code>target</code>。</p></li><li><p>特点</p><p>：</p><ul><li>数字可以重复使用。</li><li>解的顺序无关（只要组合是不同的即可）。</li></ul></li><li><p>算法设计</p><p>：</p><ul><li>使用回溯法，逐步选择数字加入路径，尝试所有可能的组合。</li><li>如果当前路径的和等于目标值，将路径加入结果集。</li><li>如果当前路径的和大于目标值，则终止递归。</li><li>保证组合的唯一性：从当前数字开始，后续只能选择当前及后面的数字（避免重复排列导致结果重复）。</li></ul></li><li><p><strong>剪枝</strong>：在递归时，如果路径和超过<code>target</code>，提前返回以减少不必要的计算。</p></li></ol></blockquote><h3 id="括号生成-2024.11.24">5.<ahref="https://leetcode.cn/problems/generate-parentheses/">括号生成</a>2024.11.24</h3><p>通过</p><p>从递归回溯的角度分析，传递参数为左括号、右括号的个数以及path</p><p>同时设置右括号个数不能多于左括号个数来剪枝</p><p>不过GPT写的比我的效率更高，判断条件更简化</p><h3 id="单词搜索-2024.11.24">6.<ahref="https://leetcode.cn/problems/word-search/">单词搜索</a>2024.11.24</h3><p>思路差不多，只是用的是布尔值二维数组但代码没成功实现，绕进去了</p><blockquote><p>result用or合并分支结果</p><h3 id="思路-1"><strong>思路</strong></h3><ol type="1"><li><strong>递归设计：</strong><ul><li>对网格中的每个字符进行搜索，尝试从该字符开始寻找单词。</li><li>每次搜索时，判断当前位置的字符是否与目标单词当前字符匹配。</li><li>如果匹配，则继续从相邻的四个方向（上、下、左、右）递归搜索。</li></ul></li><li><strong>回溯：</strong><ul><li>如果搜索失败，则需要回退到上一步，恢复现场状态，以便进行其他方向的搜索。</li></ul></li><li><strong>终止条件：</strong><ul><li>如果找到完整的单词，返回 <code>True</code>。</li><li>如果越界或遇到已访问的单元格，返回 <code>False</code>。</li></ul></li><li><strong>辅助变量：</strong><ul><li>用一个布尔值二维数组或修改当前网格来标记访问过的单元格，避免重复使用。</li></ul></li></ol></blockquote><h3 id="分割回文串-2024.11.24">7.<ahref="https://leetcode.cn/problems/palindrome-partitioning/">分割回文串</a>2024.11.24</h3><p>没什么想法，不知道该怎么递归</p><blockquote><p>其实和之前子集的很像，传递的是start(基本上等于索引)，path</p><p>遍历了所有组合</p><h3 id="思路-2"><strong>思路</strong></h3><ol type="1"><li><p><strong>分割的核心思路</strong>：</p><ul><li>每次从字符串 <code>s</code>的开头尝试切分，选择一个前缀，判断它是否是回文串。</li><li>如果是回文串，则递归处理剩下的部分。</li><li>如果到字符串的末尾，则将当前路径加入结果集。</li></ul></li><li><p><strong>回文判断</strong>：</p><ul><li>用一个辅助函数检查当前的子串是否是回文。</li></ul></li><li><p><strong>递归和回溯</strong>：</p><ul><li>在递归时，记录当前路径。</li><li>通过回溯撤销选择，尝试其他切分方式。</li></ul></li></ol></blockquote><h3 id="n-皇后-2024.11.24">8.<ahref="https://leetcode.cn/problems/n-queens/">N 皇后</a> 2024.11.24</h3><blockquote><p>按照国际象棋的规则，皇后可以攻击与之处在同一行或同一列或同一斜线上的棋子。</p><p><strong>n 皇后问题</strong> 研究的是如何将 <code>n</code>个皇后放置在 <code>n×n</code>的棋盘上，并且使皇后彼此之间不能相互攻击。</p><p>给你一个整数 <code>n</code> ，返回所有不同的 <strong>n皇后问题</strong> 的解决方案。</p><p>每一种解法包含一个不同的 <strong>n 皇后问题</strong>的棋子放置方案，该方案中 <code>'Q'</code> 和 <code>'.'</code>分别代表了皇后和空位。</p></blockquote><p>没做出来。不知道如何检验合法性。</p><blockquote><p>采用了cols,row-col表示对角线1，row+col表示对角线2的方式来处理合法性</p><p>每次传递只需要传递row即可</p><p>注意：这里考虑性能开销以及这是全局状态（检查合法性），这里board（效果等效于之前的path）放在了外面。当然也可以作为参数，在函数里（用到了zip检查合法性）。</p></blockquote><h2 id="二分查找">11.二分查找</h2><h3 id="搜索插入位置-2024.9.18">1.<ahref="https://leetcode.cn/problems/search-insert-position/">搜索插入位置</a>2024.9.18</h3><blockquote><p>给定一个排序数组和一个目标值，在数组中找到目标值，并返回其索引。如果目标值不存在于数组中，返回它将会被按顺序插入的位置。</p><p>请必须使用时间复杂度为 <code>O(log n)</code> 的算法。</p></blockquote><p>二分法 <span class="math inline">\(Log^{n}复杂度\)</span></p><h3 id="搜索二维矩阵-2024.11.25">2.<ahref="https://leetcode.cn/problems/search-a-2d-matrix/">搜索二维矩阵</a>2024.11.25</h3><p>通过，简单，用了二分法在横向搜索</p><h3 id="在排序数组中查找元素的第一个和最后一个位置-2024.11.25">3.<ahref="https://leetcode.cn/problems/find-first-and-last-position-of-element-in-sorted-array/">在排序数组中查找元素的第一个和最后一个位置</a>2024.11.25</h3><p>通过，二分法找到后在左右查看</p><h3 id="搜索旋转排序数组-2024.11.25">4.<ahref="https://leetcode.cn/problems/search-in-rotated-sorted-array/">搜索旋转排序数组</a>2024.11.25</h3><blockquote><p>整数数组 <code>nums</code> 按升序排列，数组中的值<strong>互不相同</strong> 。</p><p>在传递给函数之前，<code>nums</code> 在预先未知的某个下标<code>k</code>（<code>0 &lt;= k &lt; nums.length</code>）上进行了<strong>旋转</strong>，使数组变为<code>[nums[k], nums[k+1], ..., nums[n-1], nums[0], nums[1], ..., nums[k-1]]</code>（下标<strong>从 0 开始</strong> 计数）。例如， <code>[0,1,2,4,5,6,7]</code>在下标 <code>3</code> 处经旋转后可能变为 <code>[4,5,6,7,0,1,2]</code>。</p><p>给你 <strong>旋转后</strong> 的数组 <code>nums</code> 和一个整数<code>target</code> ，如果 <code>nums</code> 中存在这个目标值<code>target</code> ，则返回它的下标，否则返回 <code>-1</code> 。</p><p>你必须设计一个时间复杂度为 <code>O(log n)</code>的算法解决此问题。</p></blockquote><p>感觉应该有种特殊的方法处理旋转，目前我知道的logn的方法都是要排序好的</p><blockquote><p>idea:还是二分查找，左右两边一定有一边有序，在有序的那边找，如果在那边很好，不在那边就缩小往另一边区间看</p><p>可以通过以下的策略来利用二分查找的特性：</p><ol type="1"><li><strong>判断数组的一半是有序的</strong>：<ul><li>如果 <code>nums[mid] &gt;= nums[left]</code>，说明 <code>left</code>到 <code>mid</code> 这一段是有序的。</li><li>如果 <code>nums[mid] &lt; nums[left]</code>，说明 <code>mid</code>到 <code>right</code> 这一段是有序的。</li></ul></li><li><strong>确定目标值在哪一边</strong>：<ul><li>如果 <code>left</code> 到 <code>mid</code> 这一段是有序的，且<code>target</code>在这一段内，则缩小范围，<code>right = mid - 1</code>。</li><li>如果 <code>mid</code> 到 <code>right</code> 这一段是有序的，且<code>target</code>在这一段内，则缩小范围，<code>left = mid + 1</code>。</li></ul></li></ol></blockquote><h3 id="寻找旋转排序数组中的最小值-2024.11.25">5.<ahref="https://leetcode.cn/problems/find-minimum-in-rotated-sorted-array/">寻找旋转排序数组中的最小值</a>2024.11.25</h3><p><del>和上面一道题背景一样，只是找最小值，但我不知道如何设置条件来找最小值，每次记录什么值作为tempmin?</del></p><p>通过，当我想到每次只需要把有序边的最小值和当前最小值比较，只要更小就更新，反正这样可以遍历所有，最后一定有最小值</p><blockquote><p><strong>关键判断</strong>：</p><ul><li>如果数组的右侧是有序的（即<code>nums[mid] &lt;= nums[right]</code>），那么最小值一定在左侧（包括<code>mid</code>）。</li><li>如果数组的左侧是有序的（即<code>nums[left] &lt;= nums[mid]</code>），那么最小值一定在右侧。</li></ul></blockquote><h3 id="寻找两个正序数组的中位数-2024.11.25">6.<ahref="https://leetcode.cn/problems/median-of-two-sorted-arrays/">寻找两个正序数组的中位数</a>2024.11.25</h3><blockquote><p>给定两个大小分别为 <code>m</code> 和 <code>n</code>的正序（从小到大）数组 <code>nums1</code> 和<code>nums2</code>。请你找出并返回这两个正序数组的<strong>中位数</strong> 。</p><p>算法的时间复杂度应该为 <code>O(log (m+n))</code> 。</p></blockquote><p>想合并数组再找中位数，但时间复杂度不允许</p><blockquote><p>有点复杂</p><p>关键是通过<strong>二分查找</strong>来确定中位数的位置</p><img src="/2024/09/03/%E5%88%B7%E7%AE%97%E6%B3%95%E9%A2%98%E7%AC%94%E8%AE%B0/image-20241126003302856.png" class="" title="image-20241126003302856"><img src="/2024/09/03/%E5%88%B7%E7%AE%97%E6%B3%95%E9%A2%98%E7%AC%94%E8%AE%B0/image-20241126003334293.png" class="" title="image-20241126003334293"><h4 id="边界条件的处理">2. 边界条件的处理：</h4><ul><li>当 <code>partition1 == 0</code> 时，表示 <code>nums1</code>的左半部分为空，此时 <code>maxLeft1 = -\infty</code>。</li><li>当 <code>partition1 == len(nums1)</code> 时，表示 <code>nums1</code>的右半部分为空，此时 <code>minRight1 = +\infty</code>。</li></ul></blockquote><h2 id="栈">12.栈</h2><h3 id="有效的括号-2024.9.18">1.<ahref="https://leetcode.cn/problems/valid-parentheses/">有效的括号</a>2024.9.18</h3><p>简单，很久以前做过类似的</p><h3 id="最小栈-2024.11.27">2.<ahref="https://leetcode.cn/problems/min-stack/">最小栈</a>2024.11.27</h3><p>构造的不是很好，列表用的不太行</p><p>错误：pop方法忘了更新最小值；pop和top方法都没检查是否stack存在；min用一个数来记录初始化不方便；top方法用pop直接删了</p><blockquote><p>一种更酷的方法，只用一个栈，存一个元组(val,min_val)进去</p></blockquote><h3 id="字符串解码-2024.11.28">3.<ahref="https://leetcode.cn/problems/decode-string/">字符串解码</a>2024.11.28</h3><blockquote><p>给定一个经过编码的字符串，返回它解码后的字符串。</p><p>编码规则为: <code>k[encoded_string]</code>，表示其中方括号内部的<code>encoded_string</code> 正好重复 <code>k</code> 次。注意<code>k</code> 保证为正整数。</p><p>你可以认为输入字符串总是有效的；输入字符串中没有额外的空格，且输入的方括号总是符合格式要求的。</p></blockquote><figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs abnf">输入：s <span class="hljs-operator">=</span> <span class="hljs-string">&quot;3[a2[c]]&quot;</span><br>输出：<span class="hljs-string">&quot;accaccacc&quot;</span><br></code></pre></td></tr></table></figure><p>想用队列来处理，但是没处理好括号嵌套时的情况</p><blockquote><p>1.它在从字符串读“300”这种子串的时候采用了移位的想法</p><p>​ if char.isdigit(): # 如果是数字</p><p>​ current_num = current_num * 10 + int(char) # 计算多位数字</p><p>2.在处理嵌套的括号的问题，它遇到左括号会把当前num和str入栈并且重置。.它只用了一个字符串，一直关注当前构造的，每次出现右括号就出去一层。我的代码没有做对嵌套的处理</p></blockquote><h3 id="每日温度-2024.11.29">4.<ahref="https://leetcode.cn/problems/daily-temperatures/">每日温度</a>2024.11.29</h3><blockquote><p>给定一个整数数组 <code>temperatures</code>，表示每天的温度，返回一个数组 <code>answer</code> ，其中<code>answer[i]</code> 是指对于第 <code>i</code>天，下一个更高温度出现在几天后。如果气温在这之后都不会升高，请在该位置用<code>0</code> 来代替。</p></blockquote><p>我的代码的问题：栈底不需要一个初始化无限大（确保栈不空即可），days计数有逻辑问题，必须得手动处理最后一天而且无法处理倒数第二、三个。</p><p>做得好的点：意识到出栈要while一直检查</p><p>关于天数的计算：</p><blockquote><ol type="1"><li><strong>存储栈中的是“天数索引”</strong>，而不是温度或天数差。</li><li><strong>每当栈顶的温度小于当前温度时</strong>，弹出栈顶元素，计算当前温度与该元素对应天数之间的天数差。</li><li><strong>当前天数索引入栈</strong>，等待未来可能的更高温度。</li></ol></blockquote><h3 id="柱状图中最大的矩形-2024.11.29">5.<ahref="https://leetcode.cn/problems/largest-rectangle-in-histogram/">柱状图中最大的矩形</a>2024.11.29</h3><blockquote><p>给定 <em>n</em>个非负整数，用来表示柱状图中各个柱子的高度。每个柱子彼此相邻，且宽度为 1。</p><p>求在该柱状图中，能够勾勒出来的矩形的最大面积。</p></blockquote><p>不知道怎么用栈来做，感觉做过一道类似的贪心算法，但和这道题有点不一样</p><blockquote><p>思路切入点：最大的面积一定是涵盖了某个柱子的全部，所以遍历涵盖每个柱子的最大面积，其中最大值就是答案</p><p>具体实施：单调栈，里面存放的是柱子的索引。</p><p>​ 可以想象每次拿到一根柱子只要往右边扫到第一个比自己小的就够了</p><p>易错点/边界处理：加入哨兵柱子（两边的<code>0</code>），统一处理边界。</p><p>其实这个和上道题很像</p><ol type="1"><li><strong>保持单调性</strong>：<ul><li>栈中存放的柱子索引保持<strong>从小到大的顺序</strong>（单调递增栈）。</li><li>为什么要递增？因为当当前柱子的高度小于栈顶的柱子时，说明找到右边界了，栈顶柱子需要出栈计算面积。</li></ul></li><li><strong>栈的作用</strong>：<ul><li>栈中的元素（索引）记录了未确定右边界的柱子。</li></ul></li><li><strong>计算宽度和面积</strong>：<ul><li>宽度 = <strong>右边界索引</strong> - <strong>左边界索引</strong> -1。</li><li>高度 = 栈顶柱子高度。</li></ul></li></ol></blockquote><h2 id="堆">13.堆</h2><h3 id="数组中的第k个最大元素-2024.12.1">1.<ahref="https://leetcode.cn/problems/kth-largest-element-in-an-array/">数组中的第K个最大元素</a>2024.12.1</h3><blockquote><p>给定整数数组 <code>nums</code> 和整数 <code>k</code>，请返回数组中第<code>**k**</code> 个最大的元素。</p><p>请注意，你需要找的是数组排序后的第 <code>k</code>个最大的元素，而不是第 <code>k</code> 个不同的元素。</p><p>你必须设计并实现时间复杂度为 <code>O(n)</code> 的算法解决此问题。</p></blockquote><p>难</p><blockquote><p>两种方法</p><p>1.基于快速排序的快速选择</p><h4 id="整体结构"><strong>整体结构</strong></h4><ol type="1"><li><strong><code>findKthLargest(nums, k)</code> 方法</strong>：<ul><li>主函数，接收输入数组 <code>nums</code> 和整数 <code>k</code>。</li><li>调用 <code>quickselect</code> 方法处理。</li></ul></li><li><strong><code>partition</code> 方法</strong>：<ul><li>类似快速排序中的分区操作。</li><li>以 <code>pivot</code> 为参考，将数组中 <strong>大于<code>pivot</code> 的元素</strong> 移到 <code>pivot</code>左边，其余元素在右边。</li><li>返回 <code>pivot</code> 最终的索引，称为“分区点”。</li></ul></li><li><strong><code>quickselect</code> 方法</strong>：<ul><li>递归实现快速选择。</li><li>利用 <code>partition</code> 将数组划分，根据目标 k<em>k</em>的索引，决定递归处理左半部分还是右半部分。</li></ul></li></ol><p>2.堆：</p><p>补充，堆的常用方法</p><p>import heapq</p><p>可以直接处理列表，heapq的方法会保证堆的性质</p><ol type="1"><li><code>heapq.heappush(heap, x)</code>：将元素 <code>x</code>插入到堆中，时间复杂度为 O(log n)。</li><li><code>heapq.heappop(heap)</code>：移除并返回堆顶最小值，时间复杂度为O(log n)。</li><li><code>heapq.heapify(list)</code>：将一个列表转化为堆，时间复杂度为O(n)。</li></ol></blockquote><h3 id="前-k-个高频元素-2024.12.1">2.<ahref="https://leetcode.cn/problems/top-k-frequent-elements/">前 K个高频元素</a> 2024.12.1</h3><p>1.哈希表能做，不过我在导出的实现上犯错了，用错了enumerate,该用dict.items()</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python3">#map = list(enumerate(hashmap)).sort(key = takecount,reverse = True) 自己写的错误行<br>sorted_list = sorted(hashmap.items(), key=lambda x: x[1], reverse=True)<br></code></pre></td></tr></table></figure><p>另外可以用Counter(from collections import Counter)来简化读取</p><p>2.堆也能做，读取完频率后，根据频率构造大小为k的堆</p><p>Ps:使用最小堆，来删去偏小的那些，而非最大堆。</p><p>另外注意堆不是全局有序，pop()一直删倒是可以保证有序</p><h3 id="数据流的中位数-2024.12.1">3.<ahref="https://leetcode.cn/problems/find-median-from-data-stream/">数据流的中位数</a>2024.12.1</h3><blockquote><p><strong>中位数</strong>是有序整数列表中的中间值。如果列表的大小是偶数，则没有中间值，中位数是两个中间值的平均值。</p><ul><li>例如 <code>arr = [2,3,4]</code> 的中位数是 <code>3</code> 。</li><li>例如 <code>arr = [2,3]</code> 的中位数是<code>(2 + 3) / 2 = 2.5</code> 。</li></ul><p>实现 MedianFinder 类:</p><ul><li><code>MedianFinder()</code>初始化 <code>MedianFinder</code>对象。</li><li><code>void addNum(int num)</code> 将数据流中的整数 <code>num</code>添加到数据结构中。</li><li><code>double findMedian()</code>返回到目前为止所有元素的中位数。与实际答案相差 <code>10-5</code>以内的答案将被接受。</li></ul></blockquote><p>如果是单纯构建一个最小堆，应该没法保证中间的值为正确的，我不知道如何保证完全有序（除去排序算法）</p><blockquote><h3 id="设计思路"><strong>设计思路</strong></h3><ol type="1"><li><strong>数据结构选择</strong>：<ul><li>使用一个最大堆保存较小的一半数字，堆顶是较小部分的最大值。</li><li>使用一个最小堆保存较大的一半数字，堆顶是较大部分的最小值。</li></ul></li><li><strong>插入规则</strong>：<ul><li>插入的数字先放入最大堆或最小堆，然后根据堆的大小和堆顶的关系进行调整，确保：<ol type="1"><li>最大堆的所有数字 ≤ 最小堆的所有数字。</li><li>最大堆的大小和最小堆的大小之差不超过 1。</li></ol></li></ul></li><li><strong>求中位数</strong>：<ul><li>如果两个堆的大小相同，中位数是两个堆顶的平均值。</li><li>如果一个堆的大小大于另一个堆，较大堆的堆顶就是中位数。</li></ul></li></ol></blockquote><h2 id="贪心算法">14.贪心算法</h2><h3 id="买卖股票的最佳时机">1.<ahref="https://leetcode.cn/problems/best-time-to-buy-and-sell-stock/">买卖股票的最佳时机</a></h3><p>自己想的两次遍历复杂度<span class="math inline">\(n^{2}\)</span></p><p>可以一次遍历，记录最低价和最高差价，这样复杂度低不少（基于你每次找到的新的最高差价肯定都是与前面一个最低价的结合）</p><h3 id="跳跃游戏-2024.12.1">2.<ahref="https://leetcode.cn/problems/jump-game/">跳跃游戏</a>2024.12.1</h3><p>错误想法：每次尽可能跳到现在能跳的最远</p><p>正确想法（之一）：遍历一遍数组看能否到终点（if i &gt;max_reach:return False)保证中间发现不行就直接结束</p><h3 id="跳跃游戏-ii-2024.12.1">3.<ahref="https://leetcode.cn/problems/jump-game-ii/">跳跃游戏 II</a>2024.12.1</h3><p>不知道怎么保证选中的一定是最少的次数，</p><p>想法：每次跳到跳到（位置+它能到的最远位置的和是最大）的位置</p><blockquote><p>写不出来正确的，思路是相近的</p><p>不需要递归</p><p>不需要指定特定走哪条路（每次找最大能跳的范围即可）</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">jump</span>(<span class="hljs-params">self, nums: <span class="hljs-built_in">list</span>[<span class="hljs-built_in">int</span>]</span>) -&gt; <span class="hljs-built_in">int</span>:<br>        n = <span class="hljs-built_in">len</span>(nums)<br>        jumps = <span class="hljs-number">0</span>  <span class="hljs-comment"># 跳跃次数</span><br>        current_end = <span class="hljs-number">0</span>  <span class="hljs-comment"># 当前跳跃范围的结束位置</span><br>        farthest = <span class="hljs-number">0</span>  <span class="hljs-comment"># 能到达的最远位置</span><br><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n - <span class="hljs-number">1</span>):  <span class="hljs-comment"># 遍历到倒数第二个位置</span><br>            farthest = <span class="hljs-built_in">max</span>(farthest, i + nums[i])  <span class="hljs-comment"># 更新最远位置</span><br>            <span class="hljs-keyword">if</span> i == current_end:  <span class="hljs-comment"># 到达当前跳跃范围的结束位置</span><br>                jumps += <span class="hljs-number">1</span>  <span class="hljs-comment"># 增加跳跃次数</span><br>                current_end = farthest  <span class="hljs-comment"># 更新当前跳跃范围</span><br><br>        <span class="hljs-keyword">return</span> jumps<br></code></pre></td></tr></table></figure><h3 id="划分字母区间-2024.12.2">4. <ahref="https://leetcode.cn/problems/partition-labels/">划分字母区间</a>2024.12.2</h3><blockquote><p>给你一个字符串 <code>s</code>。我们要把这个字符串划分为尽可能多的片段，同一字母最多出现在一个片段中。</p><p>注意，划分结果需要满足：将所有划分结果按顺序连接，得到的字符串仍然是<code>s</code> 。</p><p>返回一个表示每个字符串片段的长度的列表。</p></blockquote><p>我的想法是在一个while大的循环里面每次读取到一个词就把它从第一个词到最后一个词作为一个句子，然后收集这个句子里面涵盖的单词，再去找这些新的单词的第一个词到最后一个词作为一个句子。</p><p>但是实现上不知道该怎么写</p><blockquote><p>思路靠近了，只需要记录字母的最后一个位置放入字典.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">last_occurrence = &#123;char: idx <span class="hljs-keyword">for</span> idx, char <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(s)&#125;<br><span class="hljs-comment">#这一句实现很经典，因为字典key不重复，所以会覆盖上一次值，最后就得到字母最后出现的位置</span><br></code></pre></td></tr></table></figure><ol type="1"><li><p><strong>记录字母最后出现的位置</strong>：用一个字典记录每个字母在字符串中的最后一次出现位置。</p></li><li><p>划分片段</p><ul><li>用两个变量 <code>start</code> 和 <code>end</code>分别表示当前片段的起点和终点。</li><li>遍历字符串时，更新 <code>end</code> 为当前字母的最后出现位置。</li><li>当当前索引等于 <code>end</code>时，表示当前片段结束，将其长度加入结果，并更新 <code>start</code>为下一个片段的起点。</li></ul></li></ol></blockquote><h3 id="整数转罗马数字-2024.12.3">5.<ahref="https://leetcode.cn/problems/integer-to-roman/">整数转罗马数字</a>2024.12.3</h3><p>用哈希表定义一个符号表，然后贪心算法能减就减</p><h2 id="动态规划">15.动态规划</h2><h3 id="总结">总结：</h3><ol type="1"><li>爬楼梯、杨辉三角基础</li><li>打家劫舍：类似贪心把前面的N家放在一起</li><li>完全平方数：类似爬楼梯，记录的是要到当前位置需要几个数（类似到当前这个台阶需要几种方法）</li><li>零钱兑换：与完全平方数是一样的，只是加了些陷阱</li><li>单词拆分：dp[i]是离散的，不一定连续</li></ol><h3 id="爬楼梯-2024.9.19">1.<ahref="https://leetcode.cn/problems/climbing-stairs/">爬楼梯</a>2024.9.19</h3><p>类似于递归，不过递归是自顶而下，动态规划是自下而上</p><h3 id="杨辉三角2024.9.19">2.<ahref="https://leetcode.cn/problems/pascals-triangle/">杨辉三角</a>2024.9.19</h3><p>类似于上面</p><h3 id="打家劫舍-2024.12.2">3.<ahref="https://leetcode.cn/problems/house-robber/">打家劫舍</a>2024.12.2</h3><blockquote><p>你是一个专业的小偷，计划偷窃沿街的房屋。每间房内都藏有一定的现金，影响你偷窃的唯一制约因素就是相邻的房屋装有相互连通的防盗系统，<strong>如果两间相邻的房屋在同一晚上被小偷闯入，系统会自动报警</strong>。</p><p>给定一个代表每个房屋存放金额的非负整数数组，计算你<strong>不触动警报装置的情况下</strong>，一夜之内能够偷窃到的最高金额。</p></blockquote><p>做不出来，不知道怎么推动这个关系</p><blockquote><p>拘泥于在几间房里做选择，而忘了可以把前面几间房算在一起来做</p><h4 id="状态定义">1. 状态定义</h4><p>用 <code>dp[i]</code> 表示前 <code>i</code>间房屋在不触发警报情况下能偷窃到的最高金额。</p><h4 id="状态转移方程">2. 状态转移方程</h4><p>如果选择偷第 <code>i</code> 间房屋，则总金额为<code>nums[i] + dp[i-2]</code>；如果不偷第 <code>i</code>间房屋，总金额为 <code>dp[i-1]</code>。dp[i]=max⁡(dp[i−1],dp[i−2]+nums[i])<em>d<strong>p<em>[<em>i</em>]=max(</em>d</strong>p</em>[<em>i</em>−1],<em>d<strong>p<em>[<em>i</em>−2]+</em>n</strong>u<strong>m</strong>s</em>[<em>i</em>])</p><h4 id="初始状态">3. 初始状态</h4><ul><li><code>dp[0] = nums[0]</code>：只有一间房时，偷它。</li><li><code>dp[1] = \max(nums[0], nums[1])</code>：两间房时，偷金额较大的那间。</li></ul><h4 id="目标">4. 目标</h4><p>最终答案是 <code>dp[n-1]</code>，其中 <code>n</code> 是房屋数量。</p></blockquote><h3 id="完全平方数-2024.12.2">4.<ahref="https://leetcode.cn/problems/perfect-squares/">完全平方数</a>2024.12.2</h3><p>想不到状态转移方程</p><p>不过这道题用BFS比状态转移方程还好一点</p><blockquote><p>1.动态规划</p><p>有点离谱</p><p>用 <code>dp[i]</code> 表示将整数 <code>i</code>分解为若干完全平方数之和所需的最少数量。</p><p>对于每个整数 <code>i</code>，尝试从每个小于 <code>i</code>的完全平方数减去：</p><p>状态转移方程为：<span class="math inline">\(dp[i] =min(dp[i],dp[i-square]+1)\)</span></p></blockquote><blockquote><p>2.BFS</p><p>可以将问题看作从 <code>n</code> 开始减少平方数到 0 的问题，用 BFS搜索每一层中减去的平方数。</p><ol type="1"><li>初始状态是<code>n</code>，每次从当前状态减去一个平方数，进入下一层。</li><li>一旦某层找到减到 0 的路径，层数就是答案。</li></ol><p>具体实现是用队列，方便出入，每一层全部出然后每一个队列里面的数都要各自减去所有square，所以队列一直是成倍增大的。可以从下面这个例子理解：</p><p>输入 <code>n = 12</code>：</p><ol type="1"><li>初始队列：<code>[12]</code>，<code>level = 0</code>。</li><li>第一层：减去 <code>[1, 4, 9]</code>，队列变为<code>[11, 8, 3]</code>，<code>level = 1</code>。</li><li>第二层：减去 <code>[1, 4, 9]</code>，队列变为<code>[10, 7, 2, 7, 4, 1]</code>，<code>level = 2</code>。</li><li>第三层：发现 <code>0</code>，返回 <code>level = 3</code>。</li></ol></blockquote><h3 id="零钱兑换-2024.12.7">5.<ahref="https://leetcode.cn/problems/coin-change/">零钱兑换</a>2024.12.7</h3><p>照着完全平方数做出来了</p><p>这道题设置了一个需要给coins排序的陷阱；以及因为数据大小的设置这道题不能用BFS</p><p>其余思路一样。</p><h3 id="单词拆分-2024.12.9">6.<ahref="https://leetcode.cn/problems/word-break/">单词拆分</a>2024.12.9</h3><p>没写出来，想用4的BFS来做，麻烦。</p><blockquote><p>1.转为集合可以提高查找效率，O(1)[相比字典的O(n)]</p><p>2.还是经典的动态规划思路，<code>dp[i]</code> 表示字符串<code>s</code> 的前 <code>i</code> 个字符是否可以由<code>wordDict</code> 中的单词组成。</p></blockquote><h3 id="最长递增子序列-2024.12.9">7.<ahref="https://leetcode.cn/problems/longest-increasing-subsequence/">最长递增子序列</a>2024.12.9</h3><p>犯了一个错误是认为最后一个就是最大的，实际上用<code>return max(dp)</code></p><blockquote><p><code>dp</code>数组的作用是存储以每个元素结尾的最长递增子序列的长度。</p><p>有优化方案：使用二分查找</p><h5 id="优化思路动态规划-二分查找">优化思路：动态规划 + 二分查找</h5><ol type="1"><li>使用一个数组<code>sub</code>，表示当前最长递增子序列的最小可能结尾值。<ul><li>如果当前数字比 <code>sub</code> 中所有数字都大，直接将其添加到<code>sub</code>。</li><li>如果当前数字可以替换 <code>sub</code>中某个值（通过二分查找找到的第一个比它大的值），则替换以保持<code>sub</code> 的最小性。</li></ul></li><li>最终，<code>sub</code> 的长度就是最长递增子序列的长度。</li></ol></blockquote><h3 id="乘积最大子数组-2024.12.9">8.<ahref="https://leetcode.cn/problems/maximum-product-subarray/">乘积最大子数组</a>2024.12.9</h3><p>没做出来，没有想到用cur_max,cur_min,result来反应遇到负数的反转情况，局限在dp[i]里了</p><h3 id="分割等和子集-2024.12.9">9.<ahref="https://leetcode.cn/problems/partition-equal-subset-sum/">分割等和子集</a>2024.12.9</h3><p>没做出来</p><ol type="1"><li><p>首先没想到先用sum(nums)判断奇偶数，结果为奇数直接pass，如果偶的话把思路转为在数组里能找到一组数让他们和为一半</p></li><li><p>dp[i]的思路也错了，不同于之前的，这里dp[i]表示的是能不能从target通过减去nums里的数得到<del>（正着来或许也可以？）</del>确定：正着来的动态规划可能改变前面确定下来的状态，只能从后往前</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs cmake"><span class="hljs-keyword">target</span> = total // <span class="hljs-number">2</span>    <br>dp = [<span class="hljs-keyword">False</span>] * (<span class="hljs-keyword">target</span> + <span class="hljs-number">1</span>)<br>dp[<span class="hljs-number">0</span>] = <span class="hljs-keyword">True</span><br></code></pre></td></tr></table></figure></li></ol><h2 id="多维动态规划">16.多维动态规划</h2><h3 id="总结-1">总结</h3><h3 id="不同路径-2024.12.9">1.<ahref="https://leetcode.cn/problems/unique-paths/">不同路径</a>2024.12.9</h3><p>根据只能走右边和走下面，想到了每个方块都继承自左边和上面的路径数目</p><h3 id="最小路径和-2024.12.9">2.<ahref="https://leetcode.cn/problems/minimum-path-sum/">最小路径和</a>2024.12.9</h3><p>思路对的，做错了，row-!写成了col-1还没发现</p><blockquote><p>另外只初始化了第一行而没有初始化第一列（非必须）</p></blockquote><h3 id="最长回文子串-2024.12.9">3.<ahref="https://leetcode.cn/problems/longest-palindromic-substring/">最长回文子串</a>2024.12.9</h3><p>没做出来，没有思路。</p><blockquote><p>1.非动态规划：</p><h3 id="中心扩展法的思路">中心扩展法的思路：</h3><p>回文串的一个特点是对称的。我们可以遍历字符串的每一个字符，尝试以每个字符为中心向两边扩展，检查是否形成回文串。</p><ul><li><strong>奇数长度回文串</strong>：它的中心是一个字符。</li><li><strong>偶数长度回文串</strong>：它的中心是两个字符之间的空隙。</li></ul><h3 id="算法步骤-1">算法步骤：</h3><ol type="1"><li>遍历字符串<code>s</code>，对于每个字符和字符对（即相邻字符）作为回文串的中心，向左右两边扩展。</li><li>判断每次扩展是否满足回文的条件，并更新最大回文子串的长度和起始位置。</li></ol><p>2.动态规划：</p><h3 id="动态规划解法思路">动态规划解法思路：</h3><ol type="1"><li><strong>定义状态：</strong><ul><li><code>dp[i][j]</code> 表示字符串 <code>s</code> 从索引<code>i</code> 到索引 <code>j</code> 的子串是否是回文子串。</li><li>如果 <code>dp[i][j] = True</code>，那么 <code>s[i:j+1]</code>是回文子串。</li></ul></li><li><strong>状态转移：</strong><ul><li>初始化：所有长度为1的子串都是回文的，即<code>dp[i][i] = True</code>。</li><li>对于长度为2的子串，如果 <code>s[i] == s[i+1]</code>，则<code>dp[i][i+1] = True</code>。</li><li>对于长度大于2的子串，如果 <code>s[i] == s[j]</code> 并且<code>dp[i+1][j-1]</code> 是回文的，那么 <code>dp[i][j]</code>也是回文的。</li></ul></li><li><strong>时间复杂度：</strong><ul><li>动态规划的时间复杂度是 <code>O(n^2)</code>，其中 <code>n</code>是字符串的长度。因为我们需要遍历每一对子串的起始和结束位置。</li></ul></li></ol></blockquote><h3 id="最长公共子序列-2024.12.9">4.<ahref="https://leetcode.cn/problems/longest-common-subsequence/">最长公共子序列</a>2024.12.9</h3><p>没做出来，没思路，我好像不擅长回文串和子序列</p><blockquote><h3 id="动态规划状态定义">动态规划状态定义：</h3><ul><li>定义 <code>dp[i][j]</code> 表示字符串 <code>text1[0..i-1]</code> 和<code>text2[0..j-1]</code> 的最长公共子序列的长度。</li><li>初始化：<code>dp[0][j] = 0</code> 和<code>dp[i][0] = 0</code>，表示如果其中一个字符串为空时，最长公共子序列的长度为0。</li><li>状态转移：<ul><li>如果 <code>text1[i-1] == text2[j-1]</code>，则<code>dp[i][j] = dp[i-1][j-1] + 1</code>，因为找到了一个公共字符，可以将它加入当前的最长公共子序列。</li><li>如果 <code>text1[i-1] != text2[j-1]</code>，则<code>dp[i][j] = max(dp[i-1][j], dp[i][j-1])</code>，表示我们可以选择跳过一个字符，以保持当前子序列的最大长度。</li></ul></li></ul><h3 id="动态规划表">动态规划表：</h3><ul><li>表 <code>dp</code> 是一个二维数组，<code>dp[i][j]</code> 存储的是<code>text1[0..i-1]</code> 和 <code>text2[0..j-1]</code>的最长公共子序列的长度。</li></ul></blockquote><h2 id="技巧">17.技巧</h2><h3 id="只出现一次的数字-2024.9.19">1.<ahref="https://leetcode.cn/problems/single-number/">只出现一次的数字</a>2024.9.19</h3><p>全部进行异或运算，最后相同项都会归到0</p><h3 id="多数元素-2024.9.19">2.<ahref="https://leetcode.cn/problems/majority-element/">多数元素</a>2024.9.19</h3><p>只有两种元素的列表，如果数字相同就count+1.不同就-1</p><h1 id="笔记">笔记</h1><ul><li>在def函数内如果要修改一个全局变量需要global n 变量声明;在局部嵌套函数中将外层函数的自由变量绑定到内层函数作用域的变量叫nonlocal，需要声明nonlocaln ps:nonlocal离开内层函数后值不会被修改</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>多模态大语言模型的幻觉增强对比学习</title>
    <link href="/2024/08/29/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%B9%BB%E8%A7%89%E5%A2%9E%E5%BC%BA%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/"/>
    <url>/2024/08/29/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%B9%BB%E8%A7%89%E5%A2%9E%E5%BC%BA%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<p>论文原址：<ahref="https://arxiv.org/abs/2312.06968">多模态大语言模型的幻觉增强对比学习</a></p><h2 id="why-研究背景动机贡献">1.[Why] 研究背景、动机、贡献</h2><p>动机：作者指出：(图1可佐证)</p><ol type="1"><li>尽管有视觉投影，文本和视觉标记之间仍然存在显着的模态差距</li><li>包含幻觉和不包含幻觉的文本的表示是纠缠在一起的，因此很难区分它们</li></ol><p><strong>新方法</strong>：提出了幻觉增强跨模式对比学习（HACL），它增强了视觉和文本表示之间的一致性，以减轻幻觉。具有幻觉的文本被用作图像锚点的硬负例，自然地拉近非幻觉文本和视觉样本的表示，同时推动非幻觉和幻觉文本的表示。具体来说，我们分别将视觉和文本标记序列输入LLMs以获得每种模态的全局表示，用于对比学习。我们使用GPT-4生成幻觉图像说明。与原始图像标题相比，这些幻觉文本包含部分对象属性错误或引入额外的不存在的信息。如图1(b) 所示，将 HACL 引入 LLaVA迫使视觉表示更接近文本表示，并使正确的文本表示和幻觉的文本表示更容易区分。这种有效的排列有助于防止幻觉的产生。</p><img src="/2024/08/29/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%B9%BB%E8%A7%89%E5%A2%9E%E5%BC%BA%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/image-20240830024535770.png" class="" title="image-20240830024535770"><p>贡献：</p><ol type="1"><li>强调视觉和文本表示之间存在显着的跨模式语义差距，以及 MLLM中包含和不包含幻觉的文本之间意外的表示混乱。这些发现暴露了当前方法在有效弥合视觉和文本表示之间的差距方面的不足。</li><li>提出了一种简单而有效的方法，名为幻觉增强跨模态对比学习（HACL）。将对比学习引入MLLM并使用幻觉文本作为硬负样本，可以产生更好的跨模态和更能区分幻觉的表示空间。</li><li>实验表明，为 MLLM 配备 HACL不仅可以减少幻觉，还可以有效提高多个基准评估的性能。</li></ol><p>相关工作：</p><ol type="1"><li>构建多模态大语言基础模型有三种主要方法，每种方法都显示出在视觉语言领域强大的零样本泛化能力的前景。例如，Flamingo[ <a href="https://arxiv.org/html/2312.06968v4#bib.bib1">1</a>]是该领域的先行者，它使用冻结视觉编码器和配备门控交叉注意力的大型语言模型来进行跨模态对齐。相比之下，PaLM-E[ <a href="https://arxiv.org/html/2312.06968v4#bib.bib11">11</a>]将提取的视觉特征直接通过线性层集成到预训练的 PaLM [ <ahref="https://arxiv.org/html/2312.06968v4#bib.bib9">9</a>]模型中，该模型拥有 5200亿个参数，从而在众多实际应用中实现稳健的性能。这种方法已被 LLaVA [ <ahref="https://arxiv.org/html/2312.06968v4#bib.bib33">33</a> ] 、Shikra [<a href="https://arxiv.org/html/2312.06968v4#bib.bib7">7</a>]等模型广泛采用。然而，这种方法的一个显着限制是创建冗长的视觉序列。为了解决这个问题，BLIP-2[ <a href="https://arxiv.org/html/2312.06968v4#bib.bib27">27</a> ]从DETR[ <a href="https://arxiv.org/html/2312.06968v4#bib.bib5">5</a>]中汲取灵感，开发了一种Q-former来有效地减少视觉特征的序列长度。 Kosmos-1[ <a href="https://arxiv.org/html/2312.06968v4#bib.bib17">17</a> ]、mPLUG-Owl [ <ahref="https://arxiv.org/html/2312.06968v4#bib.bib51">51</a> ]和MiniGPT-4 [ <ahref="https://arxiv.org/html/2312.06968v4#bib.bib55">55</a>]也反映了这种设计。</li><li>为了解决 MLLM 中的幻觉问题，研究人员开发了多种方法，大致可分为两类。第一，行涉及限制指令数据的长度，这通常会导致幻觉的减少。例如，LRV-Instruction[ <a href="https://arxiv.org/html/2312.06968v4#bib.bib30">30</a>]通过限制指令的文本长度并构建反事实指令，采用直观的方法。然而，这可能会导致微调模型的描述不太详细。第二利用额外的人工数据或工具来修改模型输出中的幻觉。例如，LLaVA-RLHF [<a href="https://arxiv.org/html/2312.06968v4#bib.bib44">44</a>]使用手动注释的数据作为奖励信号来指导模型生成较少的幻觉响应。虽然有效，但这种方法需要额外的手动注释数据。</li></ol><h2 id="what-提出的新方法">2.[What] 提出的新方法</h2><p>在下面的小节中，我们首先介绍如何在训练过程中结合<strong>跨模态对比学习</strong>。接下来，我们描述如何<strong>通过额外生成的幻觉说明文字来促进对比学习</strong>。最后，我们介绍幻觉增强对比学习<strong>训练范式</strong>。</p><img src="/2024/08/29/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%B9%BB%E8%A7%89%E5%A2%9E%E5%BC%BA%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/image-20240830031823445.png" class="" title="image-20240830031823445"><h3 id="跨模态对比学习">2.1 跨模态对比学习</h3><p>如图2(a)所示，该方法可以应用于任何通过可学习接口将视觉信息映射或抽象到文本表示空间的MLLM。正式地我们假设 MLLM 由一个视觉编码器组成，表示为<spanclass="math inline">\(\mathbf{V}_\theta\)</span>,一个可学习的接口表示为<spanclass="math inline">\(\mathbf{F}_\alpha\)</span>,并且基于仅解码器的大语言模型表示为<spanclass="math inline">\(\mathbf{L}_{\beta}\)</span>，其中<spanclass="math inline">\(\theta,\alpha,\beta\)</span>代表各个模块的参数。此外，我们还有一个无监督预训练数据集，包含N 个图像文本对，表示为<span class="math inline">\(D= \{ I_{i}, T_{i}\} ,i\in [ 1, 2, . . . , N]\)</span> 。</p><p>假设一个图像<spanclass="math inline">\(I_i\)</span>经过视觉编码器处理<spanclass="math inline">\(\mathbf{V}_\theta\)</span>和可学习的界面<spanclass="math inline">\(\mathbf{F}_a\)</span>,它被转化为长度的视觉标记序列<span class="math inline">\(m\)</span>。由于大多数LLMs都是仅解码器模型，以便获得可以捕获全局语义信息的表示。我们加入一个<spanclass="math inline">\(&lt;EOS&gt;\)</span>通标记过嵌入层的<spanclass="math inline">\(\mathbf{L}_\mathrm{\beta}\)</span>获得向量表示<spanclass="math inline">\(e\in\mathbf{R}^D\)</span>并将其附加到该序列中。因此，新的视觉标记序列变为<spanclass="math inline">\(S_v^i=[v_1^i,v_2^i,...,v_m^i,e_v^i]\)</span>,其中<spanclass="math inline">\(\upsilon_k^i\in\mathbb{R}^D,k\in[1,2,/dots,m]\)</span>。同样，对于与该图像配对的说明文字，我们还附加了一个&lt; EOS &gt;token到文本token序列并通过LLM的embedding层得到文本embedding/序列<spanclass="math inline">\(S_t^i=\left[t_1^i,t_2^i,...,t_n^i,e_t^i\right]\)</span>,在哪里<spanclass="math inline">\(t_k^i\in\mathbb{R}^D,k\in[1,2,/dots,n]\)</span>。<spanclass="math inline">\(\bar{\text{随后，视觉嵌入序列 }S_v\text{和文本嵌入序列 }S_v\text{ 分别通过了LLM }\mathbf{L}_\beta\text{得到最后一层的最终输出 }\mathbf{L}_\beta\text{ 如下：}}\)</span> <spanclass="math display">\[H_{t}^{i}=\mathbf{L}_{\beta}\left(s_{t}^{i}\right)\text{(1)}\\H_{v}^{i}=\mathbf{L}_{\beta}\left(s_{v}^{i}\right)\text{(2)}\]</span> 其中有 <span class="math display">\[H_v^i=\begin{bmatrix}\hat{v}_1^i,\hat{v}_2^i,...,\hat{v}_m^i,\hat{e}_v^i\end{bmatrix}\text{and}H_t^i=\begin{bmatrix}\hat{t}_1^i,\hat{t}_2^i,...,\hat{t}_n^i,\hat{e}_t^i\end{bmatrix}.\]</span></p><p>之后我们获得获得全局表示<spanclass="math inline">\(\hat{e}_v^i\)</span>捕获图像的整体语义信息<spanclass="math inline">\(I_i\)</span> ,之后及全球代表性<spanclass="math inline">\(\hat{e}_t^l\)</span>捕获真实标题的整体语义信息<spanclass="math inline">\(T_i\)</span></p><p>然后，与视觉语言预训练领域的许多现有方法类似，我们引入以下对比学习策略。假设训练过程中的批量大小为B，我们计算文本到图像对比学习损失如下： <span class="math display">\[\mathcal{L}_{CL}^t=-\sum_{i=1:B}\frac1Blog\left[\frac{f\left(\hat{e}_t^i,\hat{e}_v^i\right)}{f\left(\hat{e}_t^i,\hat{e}_v^i\right)+\sum_{k\neqi}f\left(\hat{e}_t^i,\hat{e}_v^k\right)}\right] (3)\]</span></p><blockquote><p>分子表示在语义空间中二者的距离</p><p>分母是分子加上其他所有非匹配文本-图像对距离的总和</p><p>这个比值通过一个对数函数进行计算，目的是使得模型更倾向于将正确的文本-图像对的相似度提高，而将错误的对的相似度降低。</p></blockquote><h3 id="利用幻觉说明文字提高对比学习">2.2利用幻觉说明文字提高对比学习</h3><p>总结：通过引入模仿 MLLM生成的幻觉文本的硬负样本来提高对比学习的有效性</p><p>产生幻觉说明文字：通过给GPT4提示，产生一些粗粒度/细粒度的文字</p><p>假设我们对于图像<spanclass="math inline">\(I_i\)</span>,，基于原始说明文字题<spanclass="math inline">\(T_i\)</span>生成了一个幻觉说明文字<spanclass="math inline">\(T_i\)</span> 并获得全局表示 <spanclass="math inline">\(\dot{e}_t^i\)</span>使用2.1小节中描述的方法对幻觉说明文字进行分析，我们可以将其视为图文对比学习中的负样本。因此，图像到文本对比学习的新公式变为：<span class="math display">\[\begin{aligned}&amp;\mathcal{L}_{CL}^{\upsilon}=\\&amp;-\sum_{i=1:B+1}\frac1{B+1}log\left[\frac{f\left(\hat{e}_v^i,\hat{e}_t^i\right)}{f\left(\hat{e}_v^i,\hat{e}_t^i\right)+f\left(\hat{e}_v^i,\hat{e}_t^i\right)+\sum_{k\neqi}f\left(\hat{e}_v^i,\hat{e}_t^k\right)}\right]&amp;\text{(5)}\end{aligned}\]</span>对于文本到图像的对比学习，我们没有做出改变，并与2.1小节中呈现的内容保持一致。</p><h2 id="how-实验评估方式">3.[How] 实验评估方式</h2><p>使用 POPE评估流程的物体幻觉基准。“是”表示模型产生积极响应的可能性。</p><img src="/2024/08/29/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%B9%BB%E8%A7%89%E5%A2%9E%E5%BC%BA%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/image-20240830114328441.png" class="" title="image-20240830114328441"><p>消融研究：</p><img src="/2024/08/29/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%B9%BB%E8%A7%89%E5%A2%9E%E5%BC%BA%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/image-20240830115600009.png" class="" title="image-20240830115600009"><blockquote><p>与选择测试的模型有较大挂钩</p></blockquote>]]></content>
    
    
    
    <tags>
      
      <tag>幻觉</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>多模态大模型幻觉综述整理</title>
    <link href="/2024/08/17/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%B9%BB%E8%A7%89%E7%BB%BC%E8%BF%B0%E6%95%B4%E7%90%86/"/>
    <url>/2024/08/17/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%B9%BB%E8%A7%89%E7%BB%BC%E8%BF%B0%E6%95%B4%E7%90%86/</url>
    
    <content type="html"><![CDATA[<p>参考综述：</p><p>①<ahref="https://arxiv.org/abs/2402.00253">大视觉语言模型中幻觉的调查</a></p><p>②<ahref="https://arxiv.org/abs/2405.09589">揭示文本、图像、视频和音频基础模型中的幻觉：综合调查</a></p><p>③<ahref="https://arxiv.org/abs/2404.18930">多模态大语言模型的幻觉：一项调查</a></p><h2 id="一.研究方向概述">一.研究方向概述</h2><h3 id="基础">1.1 基础</h3><p>幻觉分类：视觉语义学的视角为我们提供了三元分类：对象、属性和关系的幻觉。</p><p>目前的方法根据模型的认知表现来<strong>评估</strong> LVLM中的这些幻觉，重点关注两个主要方面：<strong>非幻觉生成和幻觉辨别</strong>。前者涉及对模型反应中的幻觉元素进行详细分析并量化其比例。另一方面，后者只需要对响应是否包含任何幻觉内容进行二元判断。</p><p>LVLM架构通常包含三个组件：视觉编码器、模态连接模块和LLM。视觉编码器将输入图像转换为视觉标记。连接模块旨在将视觉标记与LLM的词嵌入空间对齐，确保LLM能够处理视觉信息。其中模态对齐又有多种方法。LLM功能类似于LVLM中的中央处理单元，接收对齐的视觉和文本信息，然后综合这些信息以产生响应。</p><p>LVLM 的训练涉及两个关键阶段：(1) 预训练，其中 LVLM从对齐的图像文本对中获取视觉语言知识；(2) 指令调整，在此期间 LVLM学习使用不同的任务来遵循人类指令数据集。</p><p>从视觉语言任务的角度来看，LVLM幻觉症状可以解释为判断或描述方面的缺陷。</p><h3 id="常见方向">1.2常见方向</h3><p><strong>幻觉的复合缓解方法</strong>：除了采用LLM目标幻觉缓解方法（例如数据质量增强、编码优化和符合人类偏好）之外，LVLM特定方法还包括<strong>细化视觉表示和改进多模态对齐</strong>。细化视觉表示:<ahref="https://arxiv.org/abs/2308.12966">Qwen-VL：用于理解、本地化、文本阅读等的多功能视觉语言模型</a>;改进多模态对齐:<ahref="https://arxiv.org/abs/2312.06968">多模态大语言模型的幻觉增强对比学习</a></p><p>通过新的评价方法来改善模型：</p><p>​评估非幻觉生成是为了衡量输出中幻觉内容的比例。目前，主要有两种类型：<em>手工制作的管道</em>和<em>基于模型的端到端</em>方法。【详见参考综述①中的3.1】</p><p>​幻觉辨别评估方法旨在评估LVLM的幻觉辨别能力。遵循这种方法的方法通常采用问答格式。【与数据集/评估挂钩较多】</p><p>LVLM 幻觉的4个原因（可切入点）：数据、视觉编码器、模态对齐、LLM的幻觉【详见参考综述①中的4,5】</p><h2 id="二.其余相关论文">二.其余相关论文</h2><ol type="1"><li><ahref="https://arxiv.org/abs/2210.07688">似是而非：在视觉语言预训练中探测物体幻觉</a><ul><li><strong>问题</strong>:视觉-语言预训练（VLP）模型中的对象幻觉问题。</li><li><strong>特点</strong>:研究了<strong>视觉语言预训练（VLP）模型中的物体幻觉问题</strong>, 研究了VLP中不同类型的图像编码如何影响幻觉，包括基于区域、基于网格和基于块的图像编码。解耦了各种VLP目标，并证明令牌级图像文本对齐和受控生成对于减少幻觉至关重要。提出了一种简单而有效的VLP 损失，名为 ObjMLM，以进一步减轻物体幻觉。</li></ul></li><li><ahref="https://arxiv.org/abs/2305.10355">评估大视觉语言模型中的物体幻觉</a><ul><li><strong>问题</strong>: 视觉指令可能影响幻觉。</li><li><strong>方法</strong>:通过观察发现视觉指令中频繁描述的对象更易产生幻觉。</li><li><strong>工具</strong>:引入基于轮询的<strong>查询方法POPE</strong>，提高评估稳定性和灵活性。</li></ul></li><li><ahref="https://arxiv.org/abs/2310.05338">用于测量视觉语言模型中的物体幻觉的负物体存在评估</a><ul><li><strong>问题</strong>: 缺乏标准化的幻觉评估指标。</li><li><strong>工具</strong>:引入<strong>NOPE基准</strong>（负物体存在评估），通过视觉问答（VQA）评估VLMs中的对象幻觉。该研究利用LLMs 生成了包含 29.5k 个 NOPE 合成否定代词 (NegP)实例的数据集。它全面评估了 10 个 VLM在检测视觉问题中是否存在对象的能力，以及它们在其他 9 个 VQA数据集中处理视觉问题的典型表现。</li></ul></li><li><ahref="https://arxiv.org/abs/2403.11116">博士：提示视幻觉评估数据集</a><ul><li><strong>问题</strong>: 大型视觉语言模型（LVLM）的幻觉问题。</li><li><strong>方法</strong>:引入挑战性<strong>基准数据集</strong>评估和探索IVL-Hallu。沿任务（从低级到中级视觉识别任务）和模式（包括正常、不准确、不正确和反常识）两个维度构建评估体系。</li></ul></li><li><ahref="https://arxiv.org/abs/2402.08680">通过无分类器指导减轻大视觉语言模型中的物体幻觉</a><ul><li><strong>问题</strong>: LVLMs中的对象幻觉。</li><li><strong>方法</strong>:提出了<strong>MARINE框架</strong>，无需昂贵训练或API依赖，通过结合现有的开源视觉模型并利用没有分类器的指导来合并对象接地功能，从而增强了LVLM 的视觉理解能力，从而提高了生成输出的精度。具体来说，MARINE通过集成现有的开源视觉模型丰富了 LVLM的视觉环境，并采用无分类器引导来合并额外的对象接地功能，以提高 LVLM各代的精度。</li></ul></li><li><ahref="https://arxiv.org/abs/2311.13614">HalluciDoctor：减轻视觉指令数据中的幻觉毒性</a><ul><li><strong>问题</strong>:机器生成的视觉指令数据集中的严重幻觉毒性。</li><li><strong>方法</strong>:利用人类识别事实错误的能力，提出了一种基于交叉检查范式的新型幻觉检测和消除<strong>框架HalluciDoctor</strong>自动识别和消除训练数据中的幻觉。有趣的是,HalluciDoctor 还指出，long-tail object共现产生的虚假相关性会导致幻觉。在此基础上，我们执行反事实视觉指令扩展来平衡数据分布，从而增强MLLM 对幻觉的抵抗力。</li></ul></li><li><ahref="https://arxiv.org/abs/2312.15915">ChartBench：图表中复杂视觉推理的基准</a><ul><li><strong>问题</strong>: MLLMs在图表分析和理解上的挑战。</li><li><strong>工具</strong>: 提出了ChartBench，这是一个<strong>综合基准</strong>，旨在通过复杂的视觉推理来评估图表理解和数据可靠性。ChartBench 包括 42 个类别、66.6k 个图表和 600k个问答对。值得注意的是，许多图表缺乏数据点注释，这需要 MLLM通过利用固有的图表元素（例如颜色、图例和坐标系）来导出类似于人类理解的值。还设计了一个增强的评估指标Acc+，用于评估MLLM，而无需进行大量的手动或昂贵的基于LLM的评估。此外，提出了两个基于思想链和监督微调的基线，以提高模型在未注释图表上的性能。</li></ul></li><li><ahref="https://arxiv.org/abs/2309.15112">InternLM-XComposer：用于高级文本图像理解和合成的视觉语言大型模型</a><ul><li><strong>问题</strong>: 图像-文本理解和合成中的幻觉问题。</li><li><strong>工具</strong>: 提出 InternLM-XComposer，这是一种<strong>LVLM</strong>，可以实现高级图像文本理解和合成。InternLM-XComposer 文本图像合成的性能通过涉及人工评估和与 GPT4-Vision比较的稳健程序进行评估，该模型展示了与 GPT4-V 和 GPT3.5等解决方案相比的竞争性能。【感觉是比较偏工程性的】</li></ul></li><li><ahref="https://arxiv.org/abs/2308.06394">大视觉语言模型中检测和预防幻觉</a><ul><li><strong>问题</strong>:指导式BLIP生成的视觉基础响应中的不准确性。</li><li><strong>工具</strong>:引入M-HalDetect，<strong>一个多模态细粒度幻觉检测数据集。</strong>该数据集可作为训练LVLM 的基准，从而获得更精确的输出。使用细粒度的多模式奖励模型并增强 FDPO显着降低了 InstructBLIP 的幻觉率。这些方法不仅提高了 LLaVA 和 mPLUG-OWL等 LVLM 的准确性，而且还强调了 M-HalDetect在识别和减少幻觉方面的多功能性和有效性。</li></ul></li><li><ahref="https://arxiv.org/abs/2306.14565">通过稳健的指令调整减轻大型多模态模型中的幻觉</a><ul><li><strong>问题</strong>: LMMs生成与图像或人类指令不一致的描述。</li><li><strong>方法</strong>:通过引入LRV-Instruction，<strong>一个视觉指令调整数据集</strong>。这是一个综合数据集，包含16 个任务的 40万条视觉指令。该数据集包括各种风格和语义级别的正面和负面指令。通过LRV-Instruction，对现有 LMM中的幻觉问题进行了广泛检查，证实了其在增强视觉指令调整方面的有效性。此外，他们还推出了GAVIE，这是一种评估视觉指令调整的新颖方法，无需人工标记答案，可以适应不同类型的指令</li></ul></li><li><ahref="https://arxiv.org/abs/2310.00754">分析和减轻大视觉语言模型中的物体幻觉</a><ul><li><strong>问题</strong>: LVLMs中的对象幻觉。</li><li><strong>工具</strong>: <strong>开发了 LVLM幻觉修正器(LURE)通过改进描述来纠正 LVLM中的物体幻觉，以产生更准确和更少幻觉的输出。</strong>其方法基于深入的统计分析，识别导致物体幻觉的关键因素，例如图像中某些物体的共现、LVLM解码过程中与物体相关的不确定性，以及幻觉发生的趋势，生成文本的末尾。LURE 专为与各种 LVLM 无缝集成而设计。当在多个 LVLM 上进行测试时，LURE的集成显着增强了物体幻觉校正能力，在基于各种指标的 GPT和人类评估中始终优于其他方法。</li></ul></li></ol><p>文档图像类：</p><p><ahref="https://arxiv.org/html/2402.05121?_immersive_translate_auto_translate=1">表处理的大型语言模型：调查</a></p><p><ahref="https://github.com/godaai/llm-table-survey">他人做的整合</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>幻觉</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大模型幻觉论文泛读</title>
    <link href="/2024/08/12/%E5%B9%BB%E8%A7%89%E8%AE%BA%E6%96%87%E6%B3%9B%E8%AF%BB/"/>
    <url>/2024/08/12/%E5%B9%BB%E8%A7%89%E8%AE%BA%E6%96%87%E6%B3%9B%E8%AF%BB/</url>
    
    <content type="html"><![CDATA[<p>1.【Nature】通过语义熵检测大语言模型幻觉</p><p>语义熵算是老瓶新酒（？）</p><p>提到过去针对不确定性的方法聚焦在：更简单的比如分类器、regressor</p><p>从“大语言模型不好回答什么问题”下手？</p><p>2.<ahref="https://arxiv.org/abs/2311.06318">【WWW】用于个性化上下文查询建议的知识增强大型语言模型</a></p><p>针对大模型在网络搜索这个方面的应用：</p><p>摘要：具体来说，我们<strong>根据每个用户在网络上的搜索和浏览活动构建一个以实体为中心的知识存储，然后利用该知识存储提供上下文相关的信息LLM及时增强</strong>。该知识存储是轻量级的，因为它仅在<strong>公共知识图谱</strong>上生成特定于用户的兴趣和知识的聚合投影，并利用现有的搜索日志基础设施，从而减轻与构建深度用户配置文件相关的隐私、合规性和可扩展性问题。个性化。</p><p>可能用不上，以后做项目之类可能可以试试用来优化搜索</p><p>3.【AAAI 2024】<ahref="https://arxiv.org/abs/2308.06394">大视觉语言模型中检测和预防幻觉</a></p><p>摘要：我们首先分析了 MLLM中<strong>文本和视觉标记的表示分布</strong>，揭示了两个重要发现1）文本和视觉表示之间存在显着差距，表明跨模态表示对齐不令人满意；2）包含和不包含幻觉的文本表示是纠缠在一起的，使得区分它们变得困难。这两个观察结果启发我们找到一种简单而有效的方法来减轻幻觉。具体来说，我们将对比学习引入MLLM，并使用带有幻觉的文本作为硬负例，自然地使非幻觉文本和视觉样本的表示更加接近，同时推动非幻觉和幻觉文本的表示。</p><p>4.<ahref="https://arxiv.org/abs/2401.06855">语言模型的细粒度幻觉检测和编辑</a></p><p>提供了幻觉的一个分类（针对这个下手）</p><p>对它提出的 自动细粒度幻觉检测 没什么想法</p><img src="/2024/08/12/%E5%B9%BB%E8%A7%89%E8%AE%BA%E6%96%87%E6%B3%9B%E8%AF%BB/image-20240910214618255.png" class="" title="image-20240910214618255"><img src="/2024/08/12/%E5%B9%BB%E8%A7%89%E8%AE%BA%E6%96%87%E6%B3%9B%E8%AF%BB/image-20240911105057958.png" class="" title="image-20240911105057958"><img src="/2024/08/12/%E5%B9%BB%E8%A7%89%E8%AE%BA%E6%96%87%E6%B3%9B%E8%AF%BB/image-20240911112002597.png" class="" title="image-20240911112002597"><p>实体级、</p><p>5.<ahref="Detecting%20and%20Preventing%20Hallucinations%20in%20Large%20Vision%20Language%20Models">大视觉语言模型中检测和预防幻觉</a></p><p>做了一个幻觉的数据集</p><p>训练用到了embedding&amp;图注意力网络（不懂）</p><p>6.<ahref="https://arxiv.org/abs/2202.03629">自然语言生成中的幻觉调查</a></p><p>一篇针对NLP(含LLMs)幻觉问题进行了广泛讨论的综述</p><p>7.<ahref="https://arxiv.org/abs/2305.11747">HaluEval：大型语言模型的大规模幻觉评估基准</a></p><p>引入了大型语言模型的幻觉评估基准(HaluEval)，这是一个大量生成的和人工注释的幻觉样本的集合，用于评估LLMs在识别幻觉方面的表现。</p><p>里面提出他们发现加外部知识效果不错，加思路链效果一般，纯用幻觉样本对比学习甚至有负面效果</p><img src="/2024/08/12/%E5%B9%BB%E8%A7%89%E8%AE%BA%E6%96%87%E6%B3%9B%E8%AF%BB/image-20240912090700192.png" class="" title="image-20240912090700192"><p>8.<ahref="https://arxiv.org/abs/2307.10236">三思而后行：大型语言模型不确定性测量的探索性研究</a></p><p>做了大量实验</p><p>关键词“<strong>不确定性</strong>”，或许可以作为一个研究方向</p><p>9.<ahref="https://www.bilibili.com/video/BV1PE421A7Aa/?spm_id_from=333.337.search-card.all.click&amp;vd_source=8ee65aba2fbc74db17c49168d65cc4be">通过语义熵检测大语言模型幻觉</a></p><p>关键词“语义熵”</p><p>在句子级上有做相似的分类成一组</p><p>10.<a href="https://arxiv.org/abs/2309.05463">教科书就是你所需要的II：phi-1.5 技术报告</a></p><p>关注于更优秀的数据集（更偏向实践）</p><p>11.【ICAI】<ahref="https://arxiv.org/abs/2405.19648">检测大型语言模型生成中的幻觉：令牌概率方法</a></p><p>两个结论：</p><p>其他模型作为评估比自己评估更好</p><p>较大和较小的模型评估性能差异不显著</p><p>12.<ahref="https://arxiv.org/abs/2311.05232">大语言模型中的幻觉调查：原理、分类、挑战和开放问题</a></p><p>综述，未来方向挑战值得再看找灵感（含RAG）</p><p>13.<ahref="https://arxiv.org/abs/2307.01379">将注意力转向相关性：自由形式大型语言模型的预测不确定性量化</a></p><p>有一些低重要度的词却有高<strong>语义熵</strong>，和OPERA发现锚点很相似</p><p>14.<ahref="https://arxiv.org/abs/2302.09664">语义不确定性：自然语言生成中不确定性估计的语言不变性</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>幻觉</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>OPERA：通过过度信任惩罚和回顾分配减轻多模态大语言模型中的幻觉</title>
    <link href="/2024/08/12/OPERA%EF%BC%9A%E9%80%9A%E8%BF%87%E8%BF%87%E5%BA%A6%E4%BF%A1%E4%BB%BB%E6%83%A9%E7%BD%9A%E5%92%8C%E5%9B%9E%E9%A1%BE%E5%88%86%E9%85%8D%E5%87%8F%E8%BD%BB%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E5%B9%BB%E8%A7%89/"/>
    <url>/2024/08/12/OPERA%EF%BC%9A%E9%80%9A%E8%BF%87%E8%BF%87%E5%BA%A6%E4%BF%A1%E4%BB%BB%E6%83%A9%E7%BD%9A%E5%92%8C%E5%9B%9E%E9%A1%BE%E5%88%86%E9%85%8D%E5%87%8F%E8%BD%BB%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E5%B9%BB%E8%A7%89/</url>
    
    <content type="html"><![CDATA[<p>论文原址：<a href="https://arxiv.org/abs/2311.17911">OPERA:Alleviating Hallucination in Multi-Modal Large Language Models viaOver-Trust Penalty and Retrospection-Allocation</a></p><p>新概念：知识聚合模式/锚模式</p><h2 id="why-研究背景动机贡献">1.[Why] 研究背景、动机、贡献</h2><p>现况：</p><ol type="1"><li><p>现有的方法通过<strong>使用特定设计的数据进行训练</strong>或使用<strong>来自其他来源的外部知识进行推理</strong>来缓解这个问题，从而不可避免地产生<strong>额外成本</strong>。</p></li><li><p>现有的解码策略： 贪婪解码只是在每一步选择最有可能的下一个单词。虽然速度快且计算效率高，但贪婪解码通常会导致文本重复且变化较少波束搜索是一种更复杂的方法，波束搜索在每个步骤中跟踪预定义数量的假设，对其进行扩展以找到更优化的序列。Top-k 采样通过从前 k个可能的下一个单词中随机选择来增加生成过程的随机性，在输出中引入多样性，但有时会产生不太连贯的结果。</p><p>Top-p（Nucleus）采样是Top-k的演变，Nucleus采样考虑动态数量的单词累积达到概率p。此方法提供了随机性和相关性之间的平衡，通常会产生比Top-k 采样更连贯和有趣的输出。</p><p>DoLa解码是最近提出的一种解码方法，旨在减轻 MLLM中的幻觉，它对比成熟层和未成熟层的逻辑，并重新缩放增量作为输出。</p></li></ol><p>新方法：提出了<strong>OPERA</strong>，一种基于<strong>过度信任惩罚</strong>和<strong>回顾分配策略</strong>的新型<strong>MLLM解码方法</strong>，而无需额外的数据、知识或培训。[其余解码的优化？]</p><p>贡献/成果：</p><ol type="1"><li>提出了 OPERA 缓解了 MLLM在推理过程中的幻觉问题，无需引入任何外部数据、知识或额外的培训。</li><li>揭示了幻觉和过度信任模式的出现，并提出了一种配备回顾重新分配策略的基于惩罚的解码方法。</li></ol><h2 id="what-提出的新方法">2 [What] 提出的新方法</h2><h4 id="引入">2.1 引入</h4><p>论文的调查始于在<strong>可视化解码序列的自注意力图</strong>时发现的值得注意的“<strong>部分过度信任</strong>”情况。[相同实质的其他体现？]</p><p>如图2红色方框所示：论文作者发现一种重复出现的模式(知识聚合模式)，即柱状注意力模式与幻觉内容的起始经常伴随发生。</p><p>值得注意的是，这些柱状注意力模式通常表现在缺乏大量信息的标记上，例如句号或引号。[方块代表该单词对前面（或周围）其他单词的注意力权重分布，颜色深浅表示注意力权重的大小？]</p><p>直观上，这种特性揭示了一个奇怪的事实，即表现出柱状注意力模式的令牌通常拥有有限的信息，但却对所有后续令牌的预测产生显着影响。而且，如图3所示，可以发现后续内容大多包含推论或幻觉。</p><img src="/2024/08/12/OPERA%EF%BC%9A%E9%80%9A%E8%BF%87%E8%BF%87%E5%BA%A6%E4%BF%A1%E4%BB%BB%E6%83%A9%E7%BD%9A%E5%92%8C%E5%9B%9E%E9%A1%BE%E5%88%86%E9%85%8D%E5%87%8F%E8%BD%BB%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E5%B9%BB%E8%A7%89/image-20240816014821932.png" class="" title="image-20240816014821932"><img src="/2024/08/12/OPERA%EF%BC%9A%E9%80%9A%E8%BF%87%E8%BF%87%E5%BA%A6%E4%BF%A1%E4%BB%BB%E6%83%A9%E7%BD%9A%E5%92%8C%E5%9B%9E%E9%A1%BE%E5%88%86%E9%85%8D%E5%87%8F%E8%BD%BB%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E5%B9%BB%E8%A7%89/image-20240816021740520.png" class="" title="image-20240816021740520"><p><strong>“聚合模式”似乎是 LLM的本质。</strong>作者假设这些标记充当<strong>摘要标记</strong>，它聚合序列中先前标记的关键知识并指导后续标记的生成。这NLP 领域最近的“锚标记”观察一致，该观察发现LLM倾向于聚合浅层的一些锚标记的先前信息，并根据深层的这些锚标记来预测下一个标记(图4(a))。</p><p><strong>“聚合模式”导致当前 MLLM 的幻觉</strong>。当前的 MLLM通常将视觉标记放在序列的开头，并且期望它们专注于视觉标记并提供精确的理解。然而，随着生成的文本越长，摘要标记之间的信息传递过程中视觉信息就越容易衰减（单个摘要标记无法记住整个上下文给出的密集而丰富的信息）。<strong>具体来说，后续标记可能会忽略之前的图像标记，并通过其更强的注意力过度信任更接近的摘要标记，从而导致模型偏差引起的幻觉</strong>，例如，根据上一句提到的“路”产生“汽车”的幻觉。换句话说，出现的摘要标记越多，就越容易诱发MLLM 幻觉。为了证明这一点，我们根据摘要标记的位置分割 MLLM的长响应，并分别计算不同分割的 CHAIR 分数。如图<ahref="https://arxiv.org/html/2311.17911v3#S1.F4">4</a>(b)(c)所示，CHAIR得分与生成文本的分割数呈现明显<strong>的正相关关系</strong>，即当更多的摘要标记出现在上下文中时，就会产生更多的幻觉，表现为它们的同时出现。</p><img src="/2024/08/12/OPERA%EF%BC%9A%E9%80%9A%E8%BF%87%E8%BF%87%E5%BA%A6%E4%BF%A1%E4%BB%BB%E6%83%A9%E7%BD%9A%E5%92%8C%E5%9B%9E%E9%A1%BE%E5%88%86%E9%85%8D%E5%87%8F%E8%BD%BB%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E5%B9%BB%E8%A7%89/image-20240816025816760.png" class="" title="image-20240816025816760"><p>为了缓解部分过度信任问题，我们提出了OPERA，一种基于<strong>过度信任惩罚</strong>和<strong>回顾分配策略</strong>的新型<strong>MLLM解码方法</strong>。过度信任惩罚为波束搜索(BeamSearch)中的候选选择步骤引入了加权分数，因此具有过度信任模式的候选将具有较低的优先级被选择。<strong>具体来说，对于每个解码标记，我们研究在解码序列的自注意力图上分段的局部窗口，并设计一个列式度量来计算知识聚合模式的强度。该指标产生一个值，该值指示窗口内令牌和摘要令牌之间的过度信任程度。它自然地与波束搜索中为下一个标记预测的模型逻辑结合在一起，并惩罚过度信任模式的出现。</strong>此外，考虑到知识聚合模式出现的滞后性，当可以观察到幻觉时，幻觉可能存在于所有候选者中。我们提出了一种<strong>回顾重新分配策略</strong>来帮助解码过程回滚到摘要标记的位置并重新选择可以避免这种模式的更好的候选者。当最大窗口内罚分的位置重叠达到阈值时，会触发这种回顾。</p><h4 id="具体方法">2.2 具体方法</h4><h5 id="前置知识">2.2.1 前置知识</h5><p>LLMs可以被解析为三个部分： input formulation, model forward,decoding.</p><p><strong>InputFormulation</strong>：MLLM的输入包含图像和文本。抛开具体的架构差异不谈，MLLM通常使用视觉编码器从原始图像中提取视觉标记，并将它们映射到LLMs'具有跨模态映射模块的输入空间。映射的视觉标记用作LLM输入，以及文本输入。我们]将视觉标记表示为<spanclass="math inline">\(\mathbf{x}^v=\{x_0,x_1,...,x_{N-1}\}\)</span>。这里<spanclass="math inline">\(N\)</span>是视觉标记的长度，在大多数情况下它是固定数字。相应地，输入文本使用分词器进行分词，我们将其表示为<spanclass="math inline">\(\mathbf{x}^p=\{x_N,x_{N+1},...,x_{M+N-1}\}\)</span>。图像和文本标记连接起来作为最终的输入序列，我们将其表示为<spanclass="math inline">\(\left\{x_i\right\}_{t=0}^{T-1}\)</span>那<spanclass="math inline">\(T=N+M\)</span> 。</p><p><strong>Model Forward</strong>： MLLM使用因果注意掩模以自回归方式进行训练，每个标记根据先前的标记预测其下一个标记，形式上<spanclass="math inline">\(\mathbf{h}=\)</span>MLLM<spanclass="math inline">\((\mathbf{x}_{i})\)</span> ，<spanclass="math inline">\(\mathbf{h}\)</span> <span class="math inline">\(=\{ h_{0}, h_{1}, . . . , h_{T-1}\}\)</span>，其中h是MLLM最后一层的输出隐藏状态。</p><p><span class="math display">\[\text{接下来,MLLM 使用词汇头}\mathscr{H}\text{ 投影隐藏状态 }\mathbf{h}\text{并获取下一个标记预测的逻辑(或概率),形式如下:}\\p(x_{t}\midx_{&lt;t})=\mathrm{SoftMax}[\mathcal{H}(h_{t})]_{x_{t}},\quadx_{t}\in\mathcal{X},\text{我们使用 }x_{&lt;t}\text{ 简化序列}\{x_i\}_{i=0}^{t-1}\text{ 和 }\mathcal{X}\text{表示整个词汇集。}\]</span></p><p><strong>Decoding</strong>： OPERA 基于 BeamSearch，这是一种基于累积分数的解码策略。简而言之，对于给定的Beam尺寸<spanclass="math inline">\(N_{beam}\)</span>，束搜索保持<spanclass="math inline">\(N_{beam}\)</span>候选序列，其中每个候选序列都是解码序列<spanclass="math inline">\(\mathbf{x}^{N_{beam}}\)</span>带有光束分数。解码令牌时<spanclass="math inline">\(x_t\)</span>,每个候选假说将选择<spanclass="math inline">\(N_{beam}\)</span>基于 Top 的候选令牌<spanclass="math inline">\(N_{beam}\)</span> logits中的概率。最后，解码过程将输出有最高分数的假设。</p><p>补充：</p><blockquote><img src="/2024/08/12/OPERA%EF%BC%9A%E9%80%9A%E8%BF%87%E8%BF%87%E5%BA%A6%E4%BF%A1%E4%BB%BB%E6%83%A9%E7%BD%9A%E5%92%8C%E5%9B%9E%E9%A1%BE%E5%88%86%E9%85%8D%E5%87%8F%E8%BD%BB%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E5%B9%BB%E8%A7%89/image-20240816102623421.png" class="" title="image-20240816102623421"><p>logits是模型在做出最终预测之前的最后一步输出，它们是模型对每个可能结果的原始评分。通过适当的转换（如softmax函数），这些logits可以被解释为概率，并用于做出最终的预测决策。</p><img src="/2024/08/12/OPERA%EF%BC%9A%E9%80%9A%E8%BF%87%E8%BF%87%E5%BA%A6%E4%BF%A1%E4%BB%BB%E6%83%A9%E7%BD%9A%E5%92%8C%E5%9B%9E%E9%A1%BE%E5%88%86%E9%85%8D%E5%87%8F%E8%BD%BB%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E5%B9%BB%E8%A7%89/image-20240816102917009.png" class="" title="image-20240816102917009"></blockquote><h5 id="过度信任惩罚">2.2.2 过度信任惩罚</h5><p>前面提到，幻觉与知识聚合模式之间存在高概率共现。但这种模式具有显著的滞后性，即当相应的令牌被解码时，不能立即观察到，但在后续的几个令牌被解码之后，幻觉可能已经发生。</p><p>为了应对滞后现象，我们提出了“过度信任 Logit惩罚”，这是一种<em>在波束分数中加权的累积惩罚</em>，它<strong>影响当前令牌和候选序列的选择</strong>。累积有大惩罚的候选序列将具有较低的优先级被选择，使得具有幻觉的输出将可能被省略。</p><img src="/2024/08/12/OPERA%EF%BC%9A%E9%80%9A%E8%BF%87%E8%BF%87%E5%BA%A6%E4%BF%A1%E4%BB%BB%E6%83%A9%E7%BD%9A%E5%92%8C%E5%9B%9E%E9%A1%BE%E5%88%86%E9%85%8D%E5%87%8F%E8%BD%BB%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E5%B9%BB%E8%A7%89/image-20240816034650149.png" class="" title="image-20240816034650149"><p><strong>(Local WindowAttention)</strong>在实践中，我们研究了自注意力权重的局部窗口，并利用列乘积来计算指标值。将当前生成的序列表示为<spanclass="math inline">\(\left\{x_i\right\}_{t=0}^{t-1}\)</span>以及他们随意的自注意力权重<spanclass="math inline">\(\{\omega_{t-1,j}\}_{j=0}^{t-1}\)</span>支付下一个令牌预测，其中权重可以由softmax 结果描述为<span class="math inline">\(\omega=\)</span>SoftMax<span class="math inline">\(( \frac {QK^\top }{\sqrt{D}})\)</span>和<spanclass="math inline">\(Q,K,D\)</span>分别表示查询特征、关键特征、特征维度。我们考虑在本地窗口中收集所有先前的自注意力权重来表征知识模式，即局部窗口注意力定义为<span class="math display">\[\mathbf{W}_{t-1}^{k}=\{\mathbf{w}^{i}\}_{i=t-k}^{t-1},\quad\mathrm{s.t.}\mathbf{w}^{i}=\{\omega_{i,j}\}_{j=t-k}^{i},\]</span> 其中<spanclass="math inline">\(k\)</span>表示我们在注意力图上裁剪的局部窗口的大小，<spanclass="math inline">\(\omega_{i,j}\)</span>表示<spanclass="math inline">\(j^{th}\)</span>令牌到<spanclass="math inline">\(i^{th}\)</span>令牌分配的注意力权重。</p><p><strong>(Scaled AttentionWeights)</strong>接下来使用局部窗口注意力权重<spanclass="math inline">\(\mathbf{W}_{t-1}^k\)</span>,我们可以计算一个简单的度量来描述知识聚合模式的大小.具体来说，我们首先对<spanclass="math inline">\(\mathbf{W}^{k}_{t-1}\)</span>包括用零填充矩阵的上三角形并放大注意力值，因为这些值通常太小<span class="math display">\[\mathbf{W}_{t-1}^k\triangleq\{\mathbf{w}^i\}_{i=t-k}^{t-1},\quad\mathrm{s.t.}\mathbf{w}^i=\{\sigma\omega_{i,j}\}_{j=t-k}^{t-1}\]</span> 其中<spanclass="math inline">\(\left\{\omega_{i,j}\right\}_{j=i+1}^{t-1}\)</span>是零并且<spanclass="math inline">\(\sigma\)</span>是可配置的缩放因子。</p><p><strong>(Column-Wise Scores&amp;MaximumScore)</strong>然后我们对注意力矩阵的下三角进行列乘法，得到列分数向量。直观上，分数越大表示相应位置存在的模式越强。因此，我们选择列向得分向量的最大值作为知识聚合模式的特征</p><p><span class="math display">\[\phi(\omega_{&lt;t})=\prod\limits_{i=c}^{t-1}\sigma\omega_{i,c},\quad\text{s.t.}c=\underset{t-k\leqj\leqt-1}{\text{arg}\operatorname*{max}}\prod\limits_{i=j}^{t-1}\sigma\omega_{i,j}.\]</span> 我们选择top-N{can}在每个波束的logit中组成一个候选集y <spanclass="math display">\[\text{通过这种方式,我们将预测限制在候选集中并合并 }\phi(w_{\leqt})\text{ 使用模型 logits 来预测下一个标记}\]</span> 最后使用以下作为预测标记 <span class="math display">\[p(x_t\mid x_{&lt;t})=\text{Softmax}[\mathcal{H}(h_t)-\alpha\phi(w_{\let})]_{x_t},\quad\text{s.t.}x_t\in\mathcal{Y},\]</span></p><h5 id="回顾-分配策略">2.2.3 回顾-分配策略</h5><p>具体来说，当解码过程遇到知识聚合模式并且幻觉不可避免时，<strong>它回滚到摘要令牌并选择除了之前选择的候选者之外的其他候选者用于下一个令牌预测</strong>。根据经验，解码回顾的条件被设计为对应于几个连续标记的列分数中最大值的位置重叠，其中我们手动将阈值计数设置为<spanclass="math inline">\(r\)</span>。与不同模型之间变化的最大值不同，位置计数是一个更加稳健和通用的决策指标。</p><p>回顾过程如图6所示，我们可以很容易的求出位置坐标c最大的列得分，由此也可以得知最近解码的几个token的位置坐标集合<spanclass="math inline">\(x_{t-l}...x_{t-1}\)</span> <spanclass="math display">\[\mathcal{C}=\{c|c=\arg\max\prod_{t-k\leq j\leqz\:i=j}^{z}\sigma\omega_{i,j},z\in[t-l,t-1]\},\]</span> 其中 <span class="math inline">\(l\)</span> 默认设置为k <spanclass="math display">\[\begin{aligned}&amp;|\text{给定一个序列}\left\{x_0,x_1,...,x_{t-1}\right\}\text{及其最近位置坐标集}C\text{,我们可以轻松检查坐标是否一致。形式上,重叠时间可以通过}\\&amp;\text{以下方式计算}\end{aligned}\]</span></p><p><span class="math display">\[N_{overlap}=\sum_{c\in\mathcal{C}}\mathbb{1}_{c=s},\quad\mathrm{s.t.}\:s=\mathrm{Mode}(\mathcal{C}),\]</span></p><p>如果<span class="math inline">\(N_{overlap}\geq r\)</span>,我们考虑实施回顾，关于<span class="math inline">\(s=\)</span>Mode<spanclass="math inline">\(( C)\)</span>作为摘要标记的位置。假设序列<spanclass="math inline">\(\{x_0,x_1,...,x_s,...,x_{t-1}\}\)</span>在摘要标记处呈现了知识聚合模式<spanclass="math inline">\(x_\mathrm{s}\)</span>,我们打算将解码过程回滚到序列<spanclass="math inline">\(\{x_0,x_1,...,x_s\}\)</span>并在互补集中选择新的下一个标记<spanclass="math inline">\(y/\{x_{s+1}\}\)</span>。由于后续的回滚会比之前的回滚更靠前，所以我们手动指定回滚位置s 必须是单调不递减的。此外，我们还配置了最大时间 <spanclass="math inline">\(\beta\)</span> 进行回滚并考虑回滚到<spanclass="math inline">\(\{x_0,x_1,...,x_{s-1}\}\)</span>如果<spanclass="math inline">\(x_\mathrm{s}\)</span>已经达到最大回滚次数。<img src="/2024/08/12/OPERA%EF%BC%9A%E9%80%9A%E8%BF%87%E8%BF%87%E5%BA%A6%E4%BF%A1%E4%BB%BB%E6%83%A9%E7%BD%9A%E5%92%8C%E5%9B%9E%E9%A1%BE%E5%88%86%E9%85%8D%E5%87%8F%E8%BD%BB%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E5%B9%BB%E8%A7%89/image-20240816044938731.png" class="" title="image-20240816044938731"></p><h2 id="how-实验评估方式">3[How] 实验评估方式</h2><h4 id="定量结果">3.1 定量结果</h4><h5 id="chair-evaluation-on-hallucinations.">3.1.1<strong>CHAIRevaluation on hallucinations.</strong></h5><p>CHAIR<strong>是一种专门设计的评估工具，旨在评估图像字幕任务中的对象幻觉问题。更准确地说，CHAIR通过计算描述中提到的所有对象在真实标签集中不存在的比例来量化给定图像描述中的对象幻觉程度。</strong></p><p><span class="math inline">\(C_s\)</span>句子级别，<spanclass="math inline">\(C_I\)</span>图像级别，分子为幻觉所占比例</p><img src="/2024/08/12/OPERA%EF%BC%9A%E9%80%9A%E8%BF%87%E8%BF%87%E5%BA%A6%E4%BF%A1%E4%BB%BB%E6%83%A9%E7%BD%9A%E5%92%8C%E5%9B%9E%E9%A1%BE%E5%88%86%E9%85%8D%E5%87%8F%E8%BD%BB%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E5%B9%BB%E8%A7%89/image-20240816052349066.png" class="" title="image-20240816052349066"><p>仍用CHAIR，再MSCOCO数据集上测试：</p><img src="/2024/08/12/OPERA%EF%BC%9A%E9%80%9A%E8%BF%87%E8%BF%87%E5%BA%A6%E4%BF%A1%E4%BB%BB%E6%83%A9%E7%BD%9A%E5%92%8C%E5%9B%9E%E9%A1%BE%E5%88%86%E9%85%8D%E5%87%8F%E8%BD%BB%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E5%B9%BB%E8%A7%89/image-20240816052519927.png" class="" title="image-20240816052519927"><p>用GPT-4V辅助评估幻觉结果，数据集为MSCOCO</p><img src="/2024/08/12/OPERA%EF%BC%9A%E9%80%9A%E8%BF%87%E8%BF%87%E5%BA%A6%E4%BF%A1%E4%BB%BB%E6%83%A9%E7%BD%9A%E5%92%8C%E5%9B%9E%E9%A1%BE%E5%88%86%E9%85%8D%E5%87%8F%E8%BD%BB%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E5%B9%BB%E8%A7%89/image-20240816052701085.png" class="" title="image-20240816052701085"><h4 id="消融研究">3.2 消融研究</h4><p>结论：</p><ol type="1"><li>过度信任惩罚和回顾重新分配策略中的任何一个都可以帮助 MLLM模型减少幻觉，其中过度信任惩罚对最终性能的贡献相对更大。</li><li>设置合适的候选人数<spanclass="math inline">\(N_{can}\)</span>有较好的性能提升，和选择的LLM模型有关</li><li>一些基于结果的参数调整（如列乘法中的比例因子σ，处罚权重α等)</li></ol><h2 id="局限性与未来方向">4.局限性与未来方向</h2><p>潜力：重复也是 MLLM的一个问题，通常表现为模型不断重复特定的句子。我们注意到 OPERA可以很好地处理这种重复。<strong>重复句子的自注意力图出现了周期性的知识聚合模式。</strong>[从这个角度或者相似的角度处理重复的问题]</p><p>局限性：</p><ol type="1"><li>它无法解决MLLM的各种幻觉（比如可以解决偏见问题，即过度依赖摘要token，但无法解决视觉感知方面带来的幻觉）</li><li>OPERA在以简短答案解决幻觉时表现出边际收益(10token内)。为了克服这一限制，一个潜在的解决方案是增强用于检测知识聚合模式的指标并提高其敏感性。</li></ol><p>[其他角度用相似方法？]</p>]]></content>
    
    
    
    <tags>
      
      <tag>幻觉, 多模态</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TruthX：通过在真实空间中编辑大型语言模型来减轻幻觉</title>
    <link href="/2024/07/28/TruthX%EF%BC%9A%E9%80%9A%E8%BF%87%E5%9C%A8%E7%9C%9F%E5%AE%9E%E7%A9%BA%E9%97%B4%E4%B8%AD%E7%BC%96%E8%BE%91%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%9D%A5%E5%87%8F%E8%BD%BB%E5%B9%BB%E8%A7%89/"/>
    <url>/2024/07/28/TruthX%EF%BC%9A%E9%80%9A%E8%BF%87%E5%9C%A8%E7%9C%9F%E5%AE%9E%E7%A9%BA%E9%97%B4%E4%B8%AD%E7%BC%96%E8%BE%91%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%9D%A5%E5%87%8F%E8%BD%BB%E5%B9%BB%E8%A7%89/</url>
    
    <content type="html"><![CDATA[<h2 id="why论文研究背景动机贡献">1.[Why]论文研究背景、动机、贡献</h2><h3 id="研究背景">1.1 研究背景</h3><p>LLMs有时会产生流畅、符合指令但不真实的反应，通常称为“幻觉”。这种现象极大地损害了应用程序中LLMs的可信度。减轻LLMs的幻觉带来了巨大的挑战，因为幻觉可能源于多种因素，例如盲目遵循指令、嘈杂的数据、缺乏知识和生成过程。</p><p>另外有其他研究表明：</p><ol type="1"><li>即使拥有正确的知识，LLMs也不能够始终如一地产生真实的反应。</li><li>LLMs可以通过自我验证来识别自己产生的幻觉的存在</li><li>一些工作发现 LLMs的内部表征与输出的真实性之间存在相关性，即使他们知道正确的知识，其中一些错误的激活的内部表征也会导致LLMs产生幻觉</li></ol><p>相关工作：</p><ol type="1"><li>对比解码(Contrastdecoding)：对比解码根据强/弱模型之间的比较来修改输出概率。通过使用带有错觉的弱模型，对比解码可以提高LLM 的真实性。</li><li>表示编辑(Representationediting)：早期研究表明，风格迁移和可控文本生成等任务可以通过编辑模型来实现交涉。最近，对比一致搜索(CCS)使用成对的内部激活找到了真实的方向。推理时间干预（ITI）（Li et al.,2023b）探测并调整 LLM 注意力头中的真实性。真理森林 (TrFr)（Chen等人，2024）在 ITI 的基础上进行了扩展，结合了正交约束来完善探测能力。虽然 ITI 和 Trfr已经显示出有希望的结果，但仅编辑注意力头以尽量减少对生成能力的干扰，因为FFN 模块始终被认为是知识记忆。</li></ol><h3 id="动机">1.2 动机</h3><p>针对LLMs会产生幻觉的问题，尝试提出一种方法来减轻幻觉/增强真实性</p><h3 id="贡献">1.3 贡献</h3><p>提出了 TruthX，一种<strong>推理时间干预方法</strong>，通过识别和编辑LLM 控制真实性的内部表示中的特征来激活 LLM 的真实性。 TruthX采用自动编码器将 LLM的表示分别映射到语义和真实潜在空间，并应用对比学习来识别真实空间内的真实编辑方向。在推理过程中，通过编辑LLM在真实空间中的内部表示，TruthX有效增强了LLM的真实性。进一步的分析表明，TruthX可以通过仅编辑 LLM 内部表示中的<strong>一个向量</strong>来控制 LLM产生真实或幻觉的响应</p><p>相比表示编辑领域中其他的工作，TruthX，关注LLM的所有内部表示，而不仅仅是关注头。此外，TruthX在真实空间中进行探测和编辑，从而展示出更有效的真实性增强和更大的编辑灵活性。</p><h2 id="what论文提出的新方法">2.[What]论文提出的新方法</h2><h3 id="truthx总览">2.1 TruthX总览</h3><img src="/2024/07/28/TruthX%EF%BC%9A%E9%80%9A%E8%BF%87%E5%9C%A8%E7%9C%9F%E5%AE%9E%E7%A9%BA%E9%97%B4%E4%B8%AD%E7%BC%96%E8%BE%91%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%9D%A5%E5%87%8F%E8%BD%BB%E5%B9%BB%E8%A7%89/image-20240724135619048.png" class="" title="image-20240724135619048"><ol type="1"><li><strong>提取内部表示（Extracting internalrepresentations）</strong>:<ul><li>该部分使用了一个由前馈神经网络（FFN）和注意力机制（ATTN）组成的模块来处理输入问题（Q）以及正样本（truthfulanswers）和负样本（untruthful answers）。</li><li>内部表示从这些样本中提取出来，并标记为真（蓝色）或假（紫色）。</li></ul></li><li><strong>用自编码器探测（Probing with auto-encoder）</strong>:<ul><li>内部表示接着被输入到两个不同的编码器中：一个是真实编码器（TruthfulEncoder），另一个是语义编码器（Semantic Encoder）。</li><li>通过对比学习（contrastivelearning），真实编码器将表示映射到真实空间（TruthfulSpace），而语义编码器将表示映射到语义空间（Semantic Space）。</li><li>真实空间中的向量（h_truth）和语义空间中的向量（h_sem）通过自编码器的解码器进行解码。</li></ul></li><li><strong>在真实空间中编辑（Editing in truthful space）</strong>:<ul><li>内部表示在真实空间中进行编辑，调整使其更加真实。</li><li>通过编辑模块，添加了一个新的向量（表示为Δ）来调整原始表示，使其更接近真实答案的空间。</li><li>经过编辑后的表示再次经过FFN和ATTN模块进行处理，生成最终的输出。</li></ul></li></ol><p>通过这三个步骤，TruthX能够探测并编辑LLM的内部表示，使其回答更加真实。这种方法通过对比学习和自编码器的结合，有效地增强了LLM的真实性。</p><h3 id="提取内部表示">2.2 提取内部表示</h3><img src="/2024/07/28/TruthX%EF%BC%9A%E9%80%9A%E8%BF%87%E5%9C%A8%E7%9C%9F%E5%AE%9E%E7%A9%BA%E9%97%B4%E4%B8%AD%E7%BC%96%E8%BE%91%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%9D%A5%E5%87%8F%E8%BD%BB%E5%B9%BB%E8%A7%89/image-20240724150252144.png" class="" title="image-20240724150252144"><p>大多数 LLMs 通常由堆叠的 Transformer块组成，其中<strong>每个块包含一个注意力模块和一个通过残差连接互连的前馈神经网络（FFN）模块</strong>。LLM中下一个 token 的生成可以概念化为作为主流的剩余连接，而注意力和 FFN模块从上下文和参数中提取信息并将它们添加到残余流中.</p><p>首先<strong>用真实和不真实的响应来刺激LLM，并在生成具有相反真实性的内容时提取其内部表示。</strong>为此，我们构造三元组<spanclass="math inline">\({D}=\{(Q,A^{pos},A^{neg})\}\)</span>其中𝑄是问题，A<sup>{pos}</sup>是真实答案，,A<sup>{neg}</sup>是不真实答案回答。给定D，我们用Q+A<sup>{pos}</sup>或Q+A<sup>{neg}</sup>刺激 LLM 以提取相应的内部表示</p><p>为了最大限度地减少不同令牌语义对探测的干扰，我们<strong>只提取同时出现在A<sup>pos</sup>和A<sup>neg</sup>中的令牌的内部表示</strong>，从而确保表示之间最大的语义相似性。形式上，当出现真实和不真实的刺激时，我们提取每层注意力模块和FFN 模块输出的表示，表示为<spanclass="math inline">\(X^{pos}=\{x^{pos}\}\text{ 和}X^{neg}=\{x^{neg}\}\)</span>，其中<spanclass="math display">\[X^{pos},X^{neg}\in\mathbb{R}^{d_{model}}\]</span>是LLM隐藏状态的维度。</p><h3 id="使用自动编码器进行探测">2.3 使用自动编码器进行探测</h3><img src="/2024/07/28/TruthX%EF%BC%9A%E9%80%9A%E8%BF%87%E5%9C%A8%E7%9C%9F%E5%AE%9E%E7%A9%BA%E9%97%B4%E4%B8%AD%E7%BC%96%E8%BE%91%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%9D%A5%E5%87%8F%E8%BD%BB%E5%B9%BB%E8%A7%89/image-20240724151201107.png" class="" title="image-20240724151201107"><p>给定 LLM的内部表示，我们使用<strong>自动编码器</strong>将它们映射到真实和语义的潜在空间。如图2(b)所示，自动编码器由<strong>真实编码器、语义编码器和解码器</strong>组成，全部由<strong>多层感知器</strong>(MLP) 实现。[MLP?]</p><h4 id="表示重建representation-reconstruction">2.3.1表示重建(<strong>Representation Reconstruction</strong>)</h4><p>自动编码器的主要目标是<strong>通过编码器将 LLM的内部表示映射到不同的潜在空间，然后通过解码器重建自身</strong>。首先，真实编码器TruthEnc和语义编码器Semenc将内部表示<spanclass="math display">\[x\in\{X^{pos},X^{neg}\}\]</span>分别映射到真实空间和语义空间：<span class="math display">\[h_{truth}=\text{TruthEnc}(x),\:h_{sem}=\text{SemEnc}(x)\]</span> 其中<spanclass="math display">\[h_{truth},h_{sem}\in\mathbb{R}^{d_{latent}}\]</span>分别是真实空间和语义空间中的潜在表示<spanclass="math display">\[d_{latent}\]</span>是潜在的维度。然后解码器Dec根据潜在空间表示重建LLM的内部表示，计算如下 <spanclass="math display">\[x^{&#39;}=\mathrm{Dec}(h_{sem}+\mathrm{Attn}(h_{sem},h_{truth}))\]</span> 其中<spanclass="math inline">\(x^\prime\)</span>是重构的表示，Attn是从语义潜在表示(充当查询)到真实潜在表示(充当键和值)的注意操作。自动编码器通过<spanclass="math inline">\(x^{\prime}\)</span>和<spanclass="math inline">\(x\)</span>之间的重建损失<spanclass="math inline">\(\mathcal L_recon\)</span> 进行优化，计算如下：<span class="math display">\[\mathcal{L}_{recon}=\mathrm{MSE}(x,x^{^{\prime}})\]</span> 其中MSE是均方误差损失函数</p><h4 id="对比学习contrastive-learning">2.3.2 对比学习(<strong>ContrastiveLearning</strong>)</h4><p>为了在真实空间内的真实样本和不真实样本之间以及语义空间内<strong>具有不同语义的样本之间创建清晰的界限</strong>，对这两个空间内的<strong>潜在表示</strong>采用对比学习。</p><p>在这里，我们首先提供对比学习的总体目标。对于空间中的表示<spanclass="math inline">\(s\)</span>,我们构造一组具有相同类别的样本<spanclass="math inline">\(S^+\)</span>和一组来自不同类别的样本<spanclass="math inline">\(S^-\)</span> 。对比学习通过最小化 s 和<spanclass="math inline">\(S^+\)</span>之间的距离同时最大化<spanclass="math inline">\(s\)</span>和<spanclass="math inline">\(S^-\)</span>之间的距离来对齐空间中的表示，其中训练目标计算如下：<span class="math display">\[\begin{aligned}&amp;\mathrm{CTR}(s,S^{+},S^{-})=\\&amp;-\log\frac{\sum_{s^{&#39;}\inS^{+}}\exp(sim(s,s^{&#39;}/\tau)}{\sum_{s^{&#39;}\in(S^{+},S^{-})}\exp(sim(s,s^{&#39;})/\tau)}.\end{aligned}\]</span> 由于对比学习应用于整个数据集,我们将所有正样本<spanclass="math inline">\(x^{pos}\inX^{pos}\)</span>在真实空间中的潜<strong>在表示</strong>组成的集合表示为<spanclass="math inline">\(H_{truth}^{pos}\)</span>,并且负样本<spanclass="math inline">\(x^{pos}\in X^{neg}\)</span>为<spanclass="math inline">\(H_{truth}^{neg}\)</span>。类似地，由所有正样本和负样本的语义潜在表示组成的集合分别表示为<spanclass="math inline">\(H_{sem}^{pos}\)</span>和<spanclass="math inline">\(H_{sem}^{neg}\)</span>。</p><p>在<strong>真实空间</strong>中，应该区分真实样本和不真实样本的潜在表示。因此，对于给定的样本<spanclass="math inline">\(h_{truth}^{pos}\)</span>那些具有相同真实性的样本<spanclass="math inline">\(H_{truth}^{pos}\)</span>形成<spanclass="math inline">\(S^+\)</span>,而具有相反真实性的样本<spanclass="math inline">\(H_{truth}^{neg}\)</span>形成<spanclass="math display">\[S^{-}\]</span>。对比学习是： <spanclass="math display">\[\begin{aligned}&amp;\mathcal{L}_{truth}\:=\:\mathrm{CTR}(h_{truth}^{pos},H_{truth}^{pos},H_{truth}^{neg})\\&amp;+\quad\mathrm{CTR}(h_{truth}^{neg},H_{truth}^{neg},H_{truth}^{pos}).\end{aligned}\]</span>在<strong>语义空间</strong>中，应该区分具有不同标记含义的样本的潜在表示。因此，对于给定的样本<spanclass="math inline">\(h_{sem}^{pos}\)</span>,其对应的<spanclass="math inline">\(h_\mathrm{sem}^{neg}\)</span>来自相同的标记但相反的真实性，形成<spanclass="math inline">\(S^+\)</span>,而那些具有相同真实性但不同含义的表示形式<spanclass="math display">\[S^-\]</span>。对比学习是： <spanclass="math display">\[\begin{aligned}&amp;\mathcal{L}_{sem}\:=\:\mathrm{CTR}(h_{sem}^{pos},h_{sem}^{neg},H_{sem}^{pos}\setminush_{sem}^{pos})\\&amp;+\quad\mathrm{CTR}(h_{sem}^{neg},h_{sem}^{pos},H_{sem}^{neg}\setminush_{sem}^{neg}),\end{aligned}\]</span>由于引入了对比学习，真实空间捕获真实特征并可以探测真实/不真实表示，而语义空间捕获语义特征。</p><h4 id="真实性编辑">2.33 真实性编辑</h4><p>真实性编辑将LLM 的内部表示映射到真实和语义空间后，TruthX的目标是<strong>编辑真实空间中的潜在表示并重建相应的表示</strong>。为了增强TruthX从编辑后的潜在表示中重建的能力，我们引入了<strong>编辑损失</strong>。具体来说，对于一对具有相反真实性的<spanclass="math inline">\((x^{pos},x^{neg})\)</span>,我们在真实空间<spanclass="math inline">\(h_{truth}^{pos}\Leftrightarrowh_{truth}^{neg}\)</span>中交换它们的潜在表示，并通过解码器分别重建<spanclass="math inline">\((x^{neg},x^{pos})\)</span>,表示为： <spanclass="math display">\[x^{pos\rightarrowneg}\:=\:\mathrm{Dec}(h_{sem}^{pos}+\mathrm{Attn}(h_{sem}^{pos},h_{truth}^{neg})),\\x^{neg\rightarrowpos}\:=\:\mathrm{Dec}(h_{sem}^{neg}+\mathrm{Attn}(h_{sem}^{neg},h_{truth}^{pos})).\]</span></p><p><span class="math display">\[\begin{aligned}&amp;x^{pos\to neg}\text{ 由 }h_{sem}^{pos}\text{ 和}h_{truth}^{neg}\text{重建,即,将真实性从正变为负,因此重建的表示预计接近于 }x^{neg}\text{应接近 }x^{pos}\text{ 。因此,编}\\&amp;\text{辑损失}\mathcal{L}_{edit}\text{ 为:}\end{aligned}\]</span></p><p><span class="math display">\[\begin{aligned}&amp;\mathcal{L}_{edit}\:=\:\mathrm{MSE}(x^{neg}\:,x^{pos\toneg}\:)\\&amp;+\quad\mathrm{MSE}(x^{pos},x^{neg\to pos}).\end{aligned}\]</span></p><p>通过编辑损失，TruthX可以通过编辑真实空间中的潜在表示来调整真实性.总的来说，TruthX的训练目标<spanclass="math inline">\(L\)</span>由<strong>重建损失、对比学习和编辑损失</strong>组成：<span class="math display">\[\mathcal{L}=\mathcal{L}_{recon}+\mathcal{L}_{ctr}+\mathcal{L}_{edit}\:\]</span>经过训练后，真实和不真实的表示在真实空间中表现出不同的分布。我们的目标是在这个空间内确定一个真实的编辑方向，该方向从不真实表征的中心指向真实表征的中心。形式上，<strong>真实编辑方向</strong><spanclass="math inline">\(\delta\in\mathbb{R}^d_{latent}\)</span>计算如下：<span class="math display">\[\delta=\bar{H}_{truth}^{pos}-\bar{H}_{truth}^{neg}\:\]</span> 其中<span class="math inline">\(\bar{H}_{truth}^{pos}\)</span>和<span class="math inline">\(\bar{H}_{truth}^{neg}\)</span>是整个数据集中所有真实样本和不真实样本在真实空间中的平均表示。</p><h3 id="真实空间中的编辑">2.4 真实空间中的编辑</h3><p>在推理过程中，TruthX 将 LLM 内的内部表示<spanclass="math inline">\(x\)</span>映射到真实空间<spanclass="math inline">\(h_{truth}\)</span>和语义空间<spanclass="math inline">\(h_{sem}\)</span>,然后编辑潜在表示位于真实空间中，从而增强LLM的真实性。具体来说，给定真实空间中经过训练的编辑方向<spanclass="math inline">\(\delta\in\mathbb{R}^{d_{latent}}\)</span>,TruthX将其转换为<span class="math inline">\(x\)</span>表示空间内的编辑方向<spanclass="math inline">\(\Delta\in\mathbb{R}^d_{model}:\)</span> <spanclass="math display">\[\Delta=\mathrm{Dec}(h_{sem}+\mathrm{Attn}(h_{sem},h_{truth}+\delta))\\-\mathrm{Dec}(h_{sem}+\mathrm{Attn}(h_{sem},h_{truth}-\delta)).\]</span> 然后，TruthX 沿方向<spanclass="math inline">\(\Delta\)</span>编辑内部表示<spanclass="math inline">\(x:\)</span> <span class="math display">\[\stackrel{\wedge}{x}=x+\alpha\times\Delta\]</span> 其中<strong><spanclass="math inline">\(\alpha\)</span>是编辑强度的标量</strong>。最后，TruthX将表示<span class="math inline">\(\hat{x}\)</span>返回到LLM。在实践中，TruthX 根据验证集上每层的探测精度，编辑所有注意力层和 FFN层中选定的顶部<span class="math inline">\(k\)</span>层上的 LLM的内部表示。例如，对于 32 层 LLM 和<spanclass="math inline">\(k=10\)</span> , TruthX 从总共 64个模块 (32个注意力模块 + 32 个 FFN 模块) 中选择探测精度最高的前 1o个模块编辑LLM。</p><h2 id="how论文实验的验证">3.[How]论文实验的验证</h2><p>待续...</p>]]></content>
    
    
    
    <tags>
      
      <tag>幻觉</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SelfCheckGPT：生成大语言模型的零资源黑盒幻觉检测</title>
    <link href="/2024/07/23/SelfCheckGPT%EF%BC%9A%E7%94%9F%E6%88%90%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%9B%B6%E8%B5%84%E6%BA%90%E9%BB%91%E7%9B%92%E5%B9%BB%E8%A7%89%E6%A3%80%E6%B5%8B/"/>
    <url>/2024/07/23/SelfCheckGPT%EF%BC%9A%E7%94%9F%E6%88%90%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%9B%B6%E8%B5%84%E6%BA%90%E9%BB%91%E7%9B%92%E5%B9%BB%E8%A7%89%E6%A3%80%E6%B5%8B/</url>
    
    <content type="html"><![CDATA[<h2 id="why研究背景动机贡献">1.[Why]研究背景、动机、贡献</h2><h3 id="研究背景">1.1 研究背景</h3><p>LLMs可能会产生幻觉并做出非事实陈述，这可能会破坏对其输出的信任。本文侧重于<strong>检测幻觉</strong>问题。</p><h3 id="动机">1.2 动机</h3><p>现有的事实核查方法：</p><ol type="1"><li>访问输出概率分布（这可能不适用于 ChatGPT等系统，因为有限的外部API访问系统时，用户可能无法获得这些信息）</li><li>需要通过单独的、通常很复杂的模块连接的外部数据库（事实只能根据数据库中存在的知识进行评估并且每个内容都建立外部数据库不现实）</li></ol><h3 id="贡献">1.3 贡献</h3><p>核心：提出<strong>"SelfCheckGPT"</strong>，<strong>一种简单的基于采样的方法</strong>可用于以<strong>零资源方式</strong>（即无需外部数据库）对<strong>黑盒模型</strong>的响应进行<strong>事实检查</strong>。</p><p>结果：该论文指出 SelfCheckGPT 可以： i) 检测非事实和事实句子； ii)根据事实性对段落进行排名。并且该方法与几个基线进行比较的结果表明，与灰盒方法相比，该方法在句子级幻觉检测中具有更高的AUC-PR 分数，以及在段落级事实性评估中具有更高的相关性分数。</p><h2 id="what提出的新方法">2.[What]提出的新方法</h2><p>基本思想：如果 LLM了解给定的概念，则采样的响应可能相似并包含一致的事实。然而，对于幻觉事实，<strong>随机采样的响应可能会出现分歧并相互矛盾</strong>。</p><h3 id="基础工作概述">2.1 基础工作概述</h3><img src="/2024/07/23/SelfCheckGPT%EF%BC%9A%E7%94%9F%E6%88%90%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%9B%B6%E8%B5%84%E6%BA%90%E9%BB%91%E7%9B%92%E5%B9%BB%E8%A7%89%E6%A3%80%E6%B5%8B/image-20240725153833905.png" class="" title="image-20240725153833905"><p>图中为带有提示的 SelfCheckGPT。每个 LLM生成的句子都会与没有外部数据库的随机生成的响应进行比较。比较的方法例如可以是通过如上所示的LLM提示</p><blockquote><p>[开销是否会比较大？]</p><p>[和TruthX的相似点是都考虑到通过生成多份内容，幻觉会生成具有分歧的内容，不同点在一个是直接针对文本(SelfCheckGPT)，一个是针对内在表示最后再反馈到文本(TruthX)]</p><p>[另一篇有相似思路的文献：Detecting hallucinations in large languagemodels <strong>using semantic entropy</strong> 2024.6相同点是都是基于采样的方法，不同点是利用语义熵来检测LLM幻觉，并且通过算法聚类相似的答案并评估是否有双向关联等...我觉得可以理解为一种复杂的采样方式。由此推论也可以对大语言模型中的文本或中间过程采用其他的手段做处理]</p></blockquote><p>符号说明(Notation)：</p><ol type="1"><li>让 <span class="math inline">\(R\)</span>表示从给定用户查询中<strong>提取的 LLM 响应</strong>。</li><li>SelfCheckGPT 使用相同的查询进一步绘制 <spanclass="math inline">\(N\)</span> 随机LLM响应样本<spanclass="math inline">\(\{S^1,S^2,...,S^n,...,S^N\}\)</span>,然后测量响应与随机样本之间的一致性。</li><li><strong>设计 SelfCheckGPT 来预测第 <spanclass="math inline">\(i\)</span> 个句子 S(i)的幻觉分数</strong>，使得<spanclass="math inline">\(\mathcal{S}(i)\in[0.0,1.0]\)</span>,其中如果第<spanclass="math inline">\(i\)</span>个句子基于有效信息则<spanclass="math inline">\(S(i)\to0.0\)</span>，而如果第<spanclass="math inline">\(i\)</span>个句子是幻觉则<spanclass="math inline">\(S(i)\to1.0\)</span>。</li></ol><p><strong>以下小节将描述每个 SelfCheckGPT 变体。</strong></p><h3 id="with-bertscore">2.2 with BERTScore</h3><p>这里用B(.,.)表示两个句子之间的BERTScore，SelfCheckGPT使用BERTScore来计算<strong>每个句子</strong>与<strong>每个抽取样本中最相似句子</strong>的平均BERTScore。如果句子中的信息出现在许多抽取的样本中，可以假设该信息是事实，而如果该陈述没有出现在其他样本中，则很可能是幻觉。<span class="math display">\[\mathcal{S}_{\mathrm{BERT}}(i)=1-\frac{1}{N}\sum_{n=1}^{N}\max_{k}\bigl(B(r_{i},s_{k}^{n})\bigr)\]</span> 其中：</p><blockquote><ol type="1"><li><p><span class="math inline">\(r_i\)</span>表示响应<spanclass="math inline">\(R\)</span>中的第<spanclass="math inline">\(i\)</span>句。</p></li><li><p><span class="math inline">\(s_k^n\)</span>表示第<spanclass="math inline">\(n\)</span>个样本<spanclass="math inline">\(S^n\)</span>中的第<spanclass="math inline">\(k\)</span>句。</p></li><li><p><span class="math inline">\(N\)</span>是抽取样本的数量。</p></li><li><p><spanclass="math inline">\(\mathcal{B}(r_i,s_k^n)\)</span>是第<spanclass="math inline">\(i\)</span>句<spanclass="math inline">\(r_i\)</span>与第<spanclass="math inline">\(n\)</span>个样本中第<spanclass="math inline">\(k\)</span>句的BERTScore。</p></li><li><p><span class="math inline">\(\max_k\)</span>表示从样本<spanclass="math inline">\(S^n\)</span>中选择与<spanclass="math inline">\(r_i\)</span>最相似的句子，并取其最大BERTScore。</p></li></ol></blockquote><h3 id="with-qa">2.3 with QA</h3><p>引用了另一个论文的成果（MQAG：用于评估摘要信息一致性的多项选择题回答和生成）[ps:与SelfChatGPT是一个研究所产出的]</p><p>MQAG 由两个阶段组成：问题生成 G 和问题回答 A</p><p>问题生成阶段G 对于响应<spanclass="math inline">\(R\)</span>中的句子<spanclass="math inline">\(r_i\)</span> ,我们绘制问题<spanclass="math inline">\(q\)</span>和选项 o : <span class="math display">\[q,\mathbf{o}\sim P_G(q,\mathbf{o}\mid r_i,R)\]</span> 回答阶段A 选择答案（上面为主要响应，下面为采样响应） <spanclass="math display">\[a_{R}=\underset{k}{\operatorname*{argmax}}\big[P_{A}(o_{k}\midq,R,\mathbf{o})\big]\\a_{S^{n}}=\underset{k}{\operatorname*{argmax}}\big[P_{A}(o_{k}\midq,S^{n},\mathbf{o})\big]\]</span> 我们比较主要响应<spanclass="math inline">\(a_R\)</span>和每个样本中的响应<spanclass="math inline">\(a_{S^n}\)</span>是否相同，得到匹配次数<spanclass="math inline">\(N_m\)</span>和不匹配次数<spanclass="math inline">\(N_n\)</span>。将基于匹配/不匹配计数的<strong>不一致性评分</strong>定义为：[比较响应相同时是用一些指标？或用另一个LLM？]<span class="math display">\[\mathcal{S}_{\mathrm{QA}}(i,q)=\frac{\gamma_{2}^{N_{\mathrm{n}}^{&#39;}}}{\gamma_{1}^{N_{\mathrm{m}}^{&#39;}}+\gamma_{2}^{N_{\mathrm{n}}^{&#39;}}}\]</span> 最终，带有 QA 的 SelfCheckGPT 是 <spanclass="math inline">\(q\)</span> 中不一致分数的平均值： <spanclass="math display">\[\mathcal{S}_{\mathrm{QA}}(i)=\mathbb{E}_{q}\Big[\mathcal{S}_{\mathrm{QA}}(i,q)\Big]\]</span></p><h3 id="with-n-gram">2.4 with n-gram</h3><p>基本思想：给定由 LLM 生成的样本<spanclass="math inline">\(\{S^1,...,S^N\}\)</span>,我们可以使用这些样本创建一种近似于LLM 的新语言模型。当<spanclass="math inline">\(N\)</span>变得足够大时，新的语言模型将<strong>收敛</strong>到生成响应的LLM。因此，我们可以使用新的语言模型来<strong>近似 LLM的标记概率</strong>。</p><p>在实践中，由于时间和成本的限制，只能有有限数量的样本<spanclass="math inline">\(N\)</span> 。因此，我们使用样本<spanclass="math inline">\(\{S^1,...,S^N\}\)</span>以及主要响应<spanclass="math inline">\(R\)</span> 训练一个简单的<spanclass="math inline">\(n\)</span> -gram模型，并通过计算以下指标评估流畅性和一致性 <span class="math display">\[\mathcal{S}_{n-\mathrm{gram}}^{\mathrm{Avg}}(i)=-\frac{1}{J}\sum_{j}\log\widetilde{p}_{ij}\]</span></p><p><span class="math display">\[\mathcal{S}_{n\text{-gram}}^{\mathrm{Max}}(i)=\max_{j}\Big(-\log\widetilde{p}_{ij}\Big)\]</span></p><blockquote><p>对于第 i 个句子的第 j 个标记(单词),计算其生成的概率<spanclass="math inline">\(\tilde{p}_{ij}\)</span>。</p><p><spanclass="math inline">\(\text{这里，J是句子中的标记数，}\tilde{p}_{ij}\text{是第i个句子第j个标记的概率。}\)</span></p><p>第一个式子计算所有单词的平均负对数概率，这反映了句子的整体流畅性。</p><p>第二个式子找出句子中最不可能的单词的负对数概率，这反映了句子中最可能出现错误的地方。</p></blockquote><h3 id="with-nli自然语言推理">2.5 with NLI(自然语言推理)</h3><p>前置知识：自然语言推理 (NLI)确定假设是否遵循前提，分为蕴涵/中性/矛盾。</p><p>NLI分类器的输入通常是连接到假设的前提，对于本论文提出方法来说，它是连接到要评估的句子<spanclass="math inline">\(r_i\)</span>和采样段落<spanclass="math inline">\(S^n\)</span>，<strong>仅考虑与“蕴涵”和“矛盾”类别相关的逻辑</strong>。</p><p><span class="math display">\[P(\mathrm{contradict}\mid r_i,S^n)=\frac{\exp(z_c)}{\exp(z_e)+\exp(z_c)}\]</span> 其中<span class="math inline">\(z_e\)</span>和<spanclass="math inline">\(z_c\)</span>分别是“蕴涵”和“矛盾”类的逻辑。此归一化忽略中性类并确保概率限制在0.0和1.0之间。每个样本<spanclass="math inline">\(S^n\)</span>的 SelfCheckGPT with NLI 分数定义为，<span class="math display">\[\mathcal{S}_{\mathrm{NLI}}(i)=\frac{1}{N}\sum_{n=1}^{N}P(\mathrm{contradict}\midr_{i},S^{n})\]</span>[这里算法比较简单，也有可能是考虑到各个变体便于打分拉表排名]</p><h3 id="with-prompt">2.6 with Prompt</h3><p>使用以下提示查询LLM(本文选择GPT-3)来评估S<sup>n</sup>是否支持第<spanclass="math inline">\(i\)</span>个句子</p><hr /><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm"><span class="hljs-symbol">Context:</span> &#123;&#125;<br><span class="hljs-symbol">Sentence:</span> &#123;&#125;<br>Is the sentence supported <span class="hljs-keyword">by </span>the <span class="hljs-built_in">context</span> above?<br>Answer Yes <span class="hljs-keyword">or </span>No:<br></code></pre></td></tr></table></figure><hr /><p>初步调查显示，GPT-3 (text-davinci-003) 在 98% 的情况下会输出 Yes 或No，而任何剩余的输出都可以设置为 N/A</p><p><span class="math inline">\(\text{将第 }i\text{ 句与样本 }S^n\text{进行比较时的提示输出通过映射 }\{\text{Yes: }0.0,\text{No:}1.0,\text{N/A: }0.5\}\)</span></p><p>评估参数与之前类似，都是求均值：</p><img src="/2024/07/23/SelfCheckGPT%EF%BC%9A%E7%94%9F%E6%88%90%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%9B%B6%E8%B5%84%E6%BA%90%E9%BB%91%E7%9B%92%E5%B9%BB%E8%A7%89%E6%A3%80%E6%B5%8B/image-20240725225529883.png" class="" title="image-20240725225529883"><p>[显而易见的开销巨大，是否有其他手段避开这个开销呢？]</p><h2 id="how实验评估方法">3.[How]实验评估方法</h2><h3 id="句子级幻觉检测">3.1 句子级幻觉检测</h3><p>针对本文提出的幻觉检测方法，通过以下步骤进行检测</p><ol type="1"><li>使用GPT-3（text-davinci-003）模型，通过提示"This is a Wikipediapassage about (concept):"来生成关于特定概念的维基百科文章。</li><li>对生成文本中的句子进行<strong>手动标注</strong>，将每个句子分类为：<ul><li>主要不准确（非事实，1分）：句子完全是虚构的，与主题无关。</li><li>次要不准确（非事实，0.5分）：句子包含一些非事实信息，但与主题相关。</li><li>准确（事实，0分）：句子中呈现的信息是准确的。</li></ul></li><li>评估系统检测幻觉的能力</li></ol><p>补充信息：</p><ol type="1"><li>第一步中，数据集是WikiBio，其中每个输入都包含特定概念的维基百科文章的第一段（以及表格信息）。该论文根据段落长度对WikiBio 测试集进行排名，并从最长文章的前 20% 中随机抽取 238篇文章（以确保不会选择非常晦涩的概念）。</li><li>第二步中，在 1908 个带注释的句子中，761个（39.9%）句子被标记为严重不准确，631个（33.1%）句子被标记为轻微不准确，516个（27.0%）句子被标记为准确。数据集中的 201个句子有来自两个不同注释者的注释。为了获得该子集的单个标签，如果两个注释者都同意，则使用商定的标签。然而，如果存在分歧，则选择最坏情况的标签。</li></ol><p>此外，<strong>通过平均每个段落中的句子级标签来获得段落级分数</strong>。段落级别分数的分布如下图所示，我们在+1.0处观察到一个大峰值。我们将这个峰值处的点称为完全幻觉，当响应的信息与真实概念无关并且完全由LLM捏造时，就会发生这种情况。[这个评估方式是否合理？]，[考虑按重要性/句子长度加权平均？]</p><img src="/2024/07/23/SelfCheckGPT%EF%BC%9A%E7%94%9F%E6%88%90%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%9B%B6%E8%B5%84%E6%BA%90%E9%BB%91%E7%9B%92%E5%B9%BB%E8%A7%89%E6%A3%80%E6%B5%8B/image-20240725230925301.png" class="" title="image-20240725230925301"><img src="/2024/07/23/SelfCheckGPT%EF%BC%9A%E7%94%9F%E6%88%90%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%9B%B6%E8%B5%84%E6%BA%90%E9%BB%91%E7%9B%92%E5%B9%BB%E8%A7%89%E6%A3%80%E6%B5%8B/image-20240725230914357.png" class="" title="image-20240725230914357"><p>检测 GPT-3 生成的 WikiBio 段落中的非事实和事实句子的 PR 曲线。</p><blockquote><p>从PR图角度分析，曲线下的面积越大性能越好，则：</p><p>对于主要不准确的评估：SelfCK-Prompt和SelfCk-NLI表现几乎并肩第一</p><p>对于次要不准确的评估：各类表现类似(a)但性能整体逊于(a)[或许可以从这里下手来进一步提高诊断幻觉的准确率]</p><p>对于准确句子的评估：SelfCK-Prompt和SelfCk-NLI同样并列第一，但相较(a)性能仍有下降</p></blockquote><img src="/2024/07/23/SelfCheckGPT%EF%BC%9A%E7%94%9F%E6%88%90%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%9B%B6%E8%B5%84%E6%BA%90%E9%BB%91%E7%9B%92%E5%B9%BB%E8%A7%89%E6%A3%80%E6%B5%8B/image-20240726010408539.png" class="" title="image-20240726010408539"><p>表2为句子级检测任务的 AUC-PR和段落级别的排名表现。后者通过 Pearson相关系数和 Spearman 相关系数来衡量。</p><blockquote><p>AUC-PR: 是精确召回率 (PR) 曲线下面积的缩写</p><p>Random: 随机基线</p><ul><li><strong>Avg(-logp)</strong>：计算所有句子中每个token的负对数概率的平均值。</li><li><strong>Avg(H)</strong>：计算所有句子中每个token的熵（entropy）的平均值。</li><li><strong>Max(-logp)</strong>：在每个句子中找到具有最大负对数概率的token，并计算这些token的平均值。</li><li><strong>Max(H)</strong>：在每个句子中找到具有最大熵的token，并计算这些token的平均值。</li></ul><p>作者的 观点：</p><ol type="1"><li>LLM产生幻觉/事实与标签概率p密切相关 [比较显然]</li><li>SelfCheckGPT 优于灰盒方法以及其他黑盒方法。代理 LLM 的性能明显比 LLM(GPT-3) 差。</li></ol></blockquote><p>[但是前面子版块中对于SelfCheckGPT的变体每个的打分方式都是基于变体方法的，评估算法不同这个分数有足够说服力吗？]</p><img src="/2024/07/23/SelfCheckGPT%EF%BC%9A%E7%94%9F%E6%88%90%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%9B%B6%E8%B5%84%E6%BA%90%E9%BB%91%E7%9B%92%E5%B9%BB%E8%A7%89%E6%A3%80%E6%B5%8B/image-20240726012741081.png" class="" title="image-20240726012741081"><p>图6为 段落级别分数的散点图，其中 Y 轴 = 方法分数，X 轴 =人类分数。</p><blockquote><p>最好能看到点集中在第三象限（高人类分数和高方法分数）和第一象限（低人类分数和低方法分数），这表示方法的评分与人类评估高度一致。</p><p>从图中结果来说c&gt;a&gt;b</p></blockquote><h3 id="段落级别真实性排名">3.2 段落级别真实性排名</h3><p>段落级事实性分数是通过对所有句子的句子级分数进行平均来计算的。 <spanclass="math display">\[\mathcal{S}_\text{passage}=\frac{1}{|R|}\sum_i\mathcal{S}(i)\]</span> 其中<spanclass="math inline">\(\mathcal{S}(i)\)</span>是句子级别得分，<spanclass="math inline">\(|R|\)</span>是段落中的句子数。</p><p>其中结果也在表2中：<img src="/2024/07/23/SelfCheckGPT%EF%BC%9A%E7%94%9F%E6%88%90%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%9B%B6%E8%B5%84%E6%BA%90%E9%BB%91%E7%9B%92%E5%B9%BB%E8%A7%89%E6%A3%80%E6%B5%8B/image-20240726010408539.png" class="" title="image-20240726010408539"></p><blockquote><p>Pearson相关系数评估两个连续变量之间的<strong>线性关系</strong>。在模型评估中，Pearson系数越高，表示<strong>模型预测与人类评估之间的线性关系</strong>越强。</p><p>Spearman相关系数评估两个连续变量之间的单调关系。在模型评估中，Spearman系数高表示<strong>模型预测的排序与人类评估的排序一致性</strong>较好。</p><p>在与人类表现判断一致性的指标来看：SelfCheckGPT表现明显优异&gt;GPT&gt;LLaMA-30B</p></blockquote><h3 id="消融研究">3.3 消融研究</h3><h4 id="外部知识">3.3.1 外部知识</h4><img src="/2024/07/23/SelfCheckGPT%EF%BC%9A%E7%94%9F%E6%88%90%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%9B%B6%E8%B5%84%E6%BA%90%E9%BB%91%E7%9B%92%E5%B9%BB%E8%A7%89%E6%A3%80%E6%B5%8B/image-20240726015723656.png" class="" title="image-20240726015723656"><p>表3为与外部知识库方法相比较的性能情况</p><blockquote><ol type="1"><li>首先，SelfCheckGPT 与 BERTScore/QA结合使用自样本，可以产生与使用参考段落相当甚至更好的性能。</li><li>其次，当使用 WikiBio 段落而不是自我样本时，带有<spanclass="math inline">\(n\)</span> -gram 的 SelfCheckGPT显示出较大的性能下降。这一失败的原因是 WikiBio参考文本本身不足以训练<span class="math inline">\(n\)</span> -gram模型。</li><li>第三，相比之下，当可以访问检索到的信息时，带有 NLI/Prompt 的SelfCheckGPT 可以获益匪浅。然而，在实践中，为 LLM生成的每个可能的用例都建立一个外部数据库是不可行的。</li></ol></blockquote><h4 id="样本数量的影响">3.3.2 样本数量的影响</h4><img src="/2024/07/23/SelfCheckGPT%EF%BC%9A%E7%94%9F%E6%88%90%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%9B%B6%E8%B5%84%E6%BA%90%E9%BB%91%E7%9B%92%E5%B9%BB%E8%A7%89%E6%A3%80%E6%B5%8B/image-20240726015914656.png" class="" title="image-20240726015914656"><blockquote><p>图 7 中的结果表明，随着使用更多样本，SelfCheckGPT的性能会平稳提高，但随着生成更多样本，增益会逐渐减小。</p><p>另外带有 n-gram的SelfCheckGPT在其性能达到稳定状态之前需要最多数量的样本。</p></blockquote><h4 id="selfcheckgpt的llm选择">3.3.3 SelfCheckGPT的LLM选择</h4><img src="/2024/07/23/SelfCheckGPT%EF%BC%9A%E7%94%9F%E6%88%90%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%9B%B6%E8%B5%84%E6%BA%90%E9%BB%91%E7%9B%92%E5%B9%BB%E8%A7%89%E6%A3%80%E6%B5%8B/image-20240726020143724.png" class="" title="image-20240726020143724"><blockquote><p>这里研究生成文本的 LLM是否可以自我检查自己的文本。使用减少的样本集(N=4)进行这种消融研究。</p><p>表4的结果表明，GPT-3可以自检自己的文本，即使只使用4个样本，也比unigram方法更好。</p></blockquote><h2 id="局限性与未来方向">4.局限性与未来方向</h2><p>局限性：</p><ol type="1"><li>在这项研究中，238 个 GPT-3 生成的文本主要是关于 WikiBio数据集中的个人的段落。为了进一步调查 LLM幻觉的本质，这项研究可以扩展到更广泛的概念，例如，还可以考虑生成的有关位置和物体的文本。</li></ol><p>未来方向：</p><ol type="1"><li>此外，这项工作考虑了句子层面的事实性，但我们注意到单个句子可能同时包含事实和非事实信息。例如，Min等人的以下工作。（2023）通过将句子分解为原子事实来考虑细粒度的事实性评估。</li><li>SelfCheckGPT with Prompt有最好的效果，但计算量相当大。这可能会导致不切实际的计算成本，这可以在未来的工作中解决，以提高效率。</li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>幻觉</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>通过忠实微调(F2)减轻大语言模型的幻觉 Models A Comprehensive Survey.md</title>
    <link href="/2024/07/21/%E9%80%9A%E8%BF%87%E5%BF%A0%E5%AE%9E%E5%BE%AE%E8%B0%83(F2)%E5%87%8F%E8%BD%BB%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%B9%BB%E8%A7%89/"/>
    <url>/2024/07/21/%E9%80%9A%E8%BF%87%E5%BF%A0%E5%AE%9E%E5%BE%AE%E8%B0%83(F2)%E5%87%8F%E8%BD%BB%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%B9%BB%E8%A7%89/</url>
    
    <content type="html"><![CDATA[<h2 id="一.引入">一.引入</h2><p>​ 相关背景：现有的缓解方法（Tonmoy et al.,2024）主要包括两类：即时工程和开发模型。</p><p>​<strong>即时工程</strong>是指在不触及模型架构或参数的情况下，设计输入指令以获得尽可能好的输出，展现出轻量级和快速的优势。这一类的常见方法包括<strong>检索增强生成</strong>（Varshneyet al., 2023; Peng et al.,2023）、<strong>通过反馈和推理进行自我完善</strong>（Dhuliawala et al.,2023; Ji et al., 2023c）和<strong>提示调整</strong>（Cheng等人，2023；Jones 等人，2023）。</p><p>​<strong>开发模型</strong>方法不是将LLM编辑为黑匣子，而是专注于开发或修改模型架构或参数，以有针对性地解决幻觉挑战。<strong>引入新的解码策略</strong>(Shiet al., 2023; Chuang et al., 2023)，<strong>利用知识图谱</strong>(Ji etal., 2023b; Bayat et al.,2023)，<strong>引入基于忠实度的损失函数</strong>(Yoon et al., 2023) .,2022; Qiu et al., 2023）和<strong>监督微调</strong>（Tian et al., 2023;Razumovskaia et al.,2023）是<strong>该类别中的四种主要方案</strong>。在本文中，我们也遵循这种类型的方法，探索如何通过消除LLM本身的一些缺陷和缺陷来减轻幻觉。该论文是第一篇<strong>将基于启发式的加权策略和任务分解的思想引入幻觉缓解领域</strong>。</p><p>​<strong>[why]</strong>动机：最近的努力试图通过表示编辑和解码算法来解决这个问题，而无需进行重大的结构改变或重新训练。然而，这些方法要么隐式编辑LLMs在潜在空间中的行为，要么抑制解码过程中输出不忠实结果的倾向，本质上将LLMs视为不透明实体，而不是对幻觉进行显式建模。</p><p>​ <strong>[what]</strong>创新点：提出忠实微调<strong>F</strong>aithful<strong>F</strong>inetuning（<strong>F2</strong>），它通过<strong>在微调期间精心设计的损失函数来明确地模拟忠实问答的过程</strong>。通过对流行的数据集进行了广泛的实验，证明了F2 比普通模型和基线取得了显着的改进。</p><p>​ <strong>[how]F2</strong>的实现方式：</p><ol type="1"><li>首先将传统的 QA 目标分解为两个明确的子目标：内部事实检索和基于事实的QA，从而通知 LLM 有效利用其内部知识来获得忠实的答案。</li><li>然后，我们设计了一种有针对性的微调方法，引导模型关注检索到的事实范围内基于实体和基于注意力的启发式识别的热点。此外，为了尽量减少模型幻觉的发生，我们选择了LLM结构中容易产生幻觉的层，并对其进行专门的微调。[有哪些层容易产生幻觉？原因？]</li></ol><p>​[过程图如下，基于实体和基于注意力的启发式的方法(右边第二、三个虚线框内容，2.2.1、2.2.2中提到)]</p><h2 id="二.f2具体方法">二.F2具体方法</h2><h3 id="概述">2.0 概述</h3><p>​ 这种方法将 QA 目标分解为两个明确的子目标：①内部事实检索(<strong>Internal FactRetrieval</strong>)：该目标训练模型有效检索并利用其内部知识来产生忠实的答案。②基于事实的QA(<strong>Fact-groundedQA</strong>)：此目标训练模型提供基于事实信息的答案。</p><p>​基于从输出概率和网络结构角度对LLMs产生幻觉行为的观察，F2利用加权目标，对容易产生幻觉的跨度和层等热点进行有针对性的微调。这些加权目标强调了LLMs 容易产生幻觉的跨度，并增强了 LLMs 检索准确 QA任务所需的可靠且关键信息的能力。</p><h3id="multi-objective-decomposition-for-faithful-qa-忠实-qa-的多目标分解">2.1Multi-objectiveDecomposition for Faithful QA 忠实 QA 的多目标分解</h3><p>[优化的核心式：<strong>the cross entropy loss 交叉熵损失</strong><imgsrc="C:\Users\Roy\AppData\Roaming\Typora\typora-user-images\image-20240719012931580.png"alt="image-20240719012931580" />]</p><p>[[其他的重要参数呢？有什么可以类似这样加入k/q/a来增强的？]</p><h4 id="vanilla-qa-objective-普通-qa-目标">2.1.0 Vanilla QA Objective普通 QA 目标</h4><blockquote><ul><li>Φ 是模型的参数。</li><li>𝐸表示期望值，这里是指在所有可能的样本(𝑘,𝑞,𝑎)上的期望值，这些样本是从数据集 𝐷ras 中随机抽取的。</li><li>log⁡𝑇𝜃(𝑎∣𝑞)是对数似然函数，计算的是给定问题 <em>q</em>时，模型预测答案 a的概率的对数。</li></ul></blockquote><h4 id="fact-retrieval-objective-事实检索目标">2.1.1Fact RetrievalObjective 事实检索目标</h4><p>[与普通QA相比只是把公式参数中的a换成了k？]</p><h4 id="fact-grounded-qa-objective-基于事实的质量保证目标">2.1.2Fact-grounded QA Objective 基于事实的质量保证目标</h4><p>[相比普通QA在log真数额外给定事实k]</p><h3id="targeted-training-on-hallucination-hotspots-幻觉热点针对性训练">2.2Targeted Training on Hallucination Hotspots 幻觉热点针对性训练</h3><h4 id="概述-1">2.2.0 概述</h4><p>​ 采取的措施：</p><p>​采用了有针对性的微调方法，重点关注𝐿𝑟(Φ)中检索到的事实范围内识别的热点以及LLMs 中的图层。</p><p>​在训练过程中为这些<strong>特定跨度的损失函数分配了更高的权重</strong>。这些关键跨度的识别是<strong>由基于实体和基于注意力的启发法指导的</strong>，从之前关于幻觉解释和检测的研究中汲取灵感</p><p>​ 此外，还通过探测技术确定了与幻觉关联性最强的前 10个模块，并对其进行了微调。</p><h4 id="entity-based-heuristics-基于实体的启发法">2.2.1 Entity-basedHeuristics 基于实体的启发法</h4><p>​问题1：由之前研究可知，<strong>实体</strong>是文本生成任务中最常见的幻觉或捏造的单词类型</p><p>​ 解决方法：Weighted Cross Entropy(WCE) 加权交叉熵</p><p>​ 具体过程与公式：</p><blockquote><ol type="1"><li>公式（5）中提到的 WCE(𝑘∣𝑞,𝑤)WCE(<em>k</em>∣<em>q</em>,<em>w</em>)表示加权交叉熵损失，其中 𝑤<em>w</em>是权重列表，用于调整不同部分的损失贡献。</li><li><em>w</em> 是一个权重列表，用于在计算损失时对不同的知识 𝑘<em>k</em>赋予不同的权重。这有助于模型在检索过程中更加关注重要的实体或事实。</li><li>Spacy是一个自然语言处理库，常用于语言模型中进行命名实体识别（NER）</li><li>公式（6）中的 𝐿𝑒(Φ 表示实体损失，其中Went是实体权重。这里特别强调了对实体跨度的加权，意味着模型在生成答案时会更多地考虑这些实体。</li></ol></blockquote><p>问题2：除了关注实体跨度之外，之前的研究还揭示了语言模型和人类评估信息的方式存在显着差异。在评估前面的单词时，模型倾向于考虑具有不同实体类型的各种选项。相比之下，人类直观地将候选词缩小到特定的集合，主要由与有限数量的类型相关的术语组成。这种差异可能会导致模型的预测显得不太自信。[说白了就是近似模仿人类思维]</p><p>解决办法：我们通过在Spacy识别的每个命名实体之前插入<strong>命名实体类型</strong>来利用LLMs的上下文学习功能。实体类型充当生成约束，使我们能够近似理想的候选集。最后，我们通过基于实体的启发式方法得到最终的损失设计：.</p><h4 id="attention-based-heuristics-基于注意力的启发法">2.2.2Attention-based Heuristics 基于注意力的启发法</h4><p>​具体措施1：<strong>根据最大池化注意力矩阵构建了一个加权图</strong>，以将更重要的标记包含到LM中，并保留相关样本事实中包含的更多底层信息，例如实体之间的推理。基于注意力的焦点启发式为具有高注意力分数的跨度分配更高的权重：</p><p>​ 然后，通过 <strong>PageRank 算法</strong> Rogers (2002)<strong>来衡量相关事实的显着性</strong>，这是一种流行的算法，<strong>用于根据传入链接的结构对图中节点的重要性进行排名。</strong>PageRank的运行前提是，节点的重要性不仅取决于它收到的链接数量，还取决于这些链接节点的重要性。本质上，它为图中的每个节点分配了数字重要性，分数越高表示重要性越高。这种迭代算法使用图的链接结构通过网络分配排名能力，使我们能够识别基于注意力的图中最显着的事实。按PageRank 分数排名的前 K 个令牌也在微调损失中加权。[重要性加权]</p><p>​ 具体措施2：</p><p>​ [注意力权重]</p><p>2.1~2.2总结：</p><h3 id="finetuning-hallucination-prone-layers-微调易产生幻觉的层">2.3Finetuning Hallucination-Prone Layers 微调易产生幻觉的层</h3><p>​ 与F2的关系：使用该方法补充F2进一步提升效果</p><p>​ 具体措施：采用了 <strong>TruthX</strong> (Zhang等人)提出的方法(2024)。该方法仅对与幻觉最密切相关的前 10个模块进行微调，这通过验证集上的探测准确性来确定。例如，在32层语言模型中，TruthX从总共64个模块（32个注意力模块和32个FFN模块）中选择探测精度最高的前10个模块进行模型编辑。结构选择实验结果如表1所示。[提升看着不多？]</p><p>表1：不同层选择策略的结果。TruthfulQA(MCmax)是在整个微调过程中实现的最高指标。</p><h2 id="三.实验过程">三.实验过程</h2><h3 id="数据集以及测试结果">3.1数据集以及测试结果</h3><p>​使用了三个数据集：<strong>HaluEval</strong>,<strong>TruthfulQA</strong>,<strong>FACTOR</strong></p><blockquote><p>​ TruthX 显示的结果好坏参半，Wiki 子集的性能提高了 5点，但新闻子集的性能下降了 8.5 点。这种不均匀性可能是由于 TruthX 在FaithfulQA 数据集上的过度拟合，限制了其域外性能。</p><p>相比之下，LLama2-7b + F2 提高了 News 和 Wiki 子集的性能，分别超过LLama2-7b 2.7 和 2.4 个百分点。此外，LLama2-7b + TruthX +F2有效缓解了LLama2-7b +TruthX在News子集上的性能下降，同时进一步将Wiki准确率从61.06提高到63.66，证明了F2方法的鲁棒性。</p></blockquote><p>[<strong>Contrastive Decoding</strong>中加上F2后再TruthfulQA测试中有显著效果提升]</p><p>[<strong>Representation Editing</strong>中加上F2后从数据集看News上很大幅度增强但是Wiki上效果一般，再加上TruthX后在TruthfulQA中表现优异，但从只加TruthX一行的来看，在TruthfulQA数据集的效果主要还是来自TruthX]</p><h3 id="metrics-测试指标">3.2Metrics 测试指标</h3><ol type="1"><li>MC1:在一组正确和错误的参考答案中，我们需要选择最佳的正确答案。 MC1是通过语言模型是否将最高可能性分配给给定问题的错误答案来计算的。</li><li>MC2:MC2是真实参考答案的总归一化概率。分数是正确答案的概率质量。</li><li>MC3:MC3是根据语言模型是否为给定问题的错误答案指定更高的可能性来计算的。</li><li>对于 FACTOR 数据集，简单地使用选择精度作为指标。</li></ol><h3 id="baselines-基线">3.3Baselines 基线</h3><p>​ 与F2比较的方法/层面：</p><ol type="1"><li><strong>Base LLMs</strong>：包括原始的Llama-2-7B 模型与Alpaca等其他前沿方法进行比较。</li><li><strong>Contrastive Decoding</strong> 此类别包括 CD（Li等人，2022）、DoLa（Chuang 等人，2023）、SH2（Kai 等人，2024）和ICD（Zhang等人，2023c）等技术。每种方法都独特地应用对比解码，通过操纵输出概率、层输出、标记方差以及真实/虚幻模型之间的区别来放大LLMs的真实性。</li><li><strong>Representation Editing</strong>我们探索通过修改内部表示来增强LLM真实性的高级策略。这包括推理时间干预（ITI）（Liet al., 2024）和真理森林（TrFr）（Chen et al.,2024b），它们通过学习特定的内容来识别和调整 LLMs内的注意力模式。注意力集中的方向。</li><li><strong>TruthX</strong> 我们将TruthX 应用到Llama-2-7B模型，遵循标准TruthfulQA 设置（Lin 等人，2022）。对比解码方法的结果基于Kai 等人的复制和Zhang 等人，而 ITI 和 TrFr结果源于我们使用其可公开访问的模型和输出进行的复制。</li><li><strong>TruthX + F2</strong> 采用TruthX 方法来细化Llama2-7b + F2模型，该模型已使用F2 方法进行了微调。</li></ol><h3 id="作者对结果的判断">3.4作者对结果的判断</h3><p>​ 诚然，单独使用 F2方法无法提供与结果所示的基线一样多的性能改进。这<strong>可能</strong>是因为LoRa 微调是一种相对保守的模型优化方法，与 <strong>ITI</strong> 和<strong>TruthX</strong> 等表示编辑方法不同，后者使用大权重积极编辑 LLMs的隐藏状态，并且可以带来更多的性能提升。然而，非常有趣的是，F2方法仍然与诸如 TruthX 等表示编辑方法保持<strong>正交</strong>。与Llama2-7b + TruthX 相比，Llama2-7b + TruthX + F2 可以进一步提升TruthfulQA (MC) 上的性能，MC1/MC2 分数提高约 0.7/3.1分。这表明从微调中学到的信息对于使LLMs更加忠实仍然至关重要，并且结合多角度的幻觉缓解方法可以带来更多的忠实度性能提升。[正交是很大的优点，特别是工程应用，或许可以考虑更多正交的方法？]</p><h2 id="四.局限与未来研究">四.局限与未来研究</h2><p>​ 局限：虽然 F2 证明了正交有效性，但表 2 中的实验表明，单独使用 F2方法并不能实现与 TruthfulQA (MC) 基线相同的改进，这可能是由于 LoRA的保守更新和微调过程中信息利用率不高所致。</p><p>​ 未来研究：未来的工作将探索加强拟议的 F2培训目标中的知识学习和利用。</p>]]></content>
    
    
    
    <tags>
      
      <tag>幻觉</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>揭示文本、图像、视频和音频基础模型中的幻觉：综合调查</title>
    <link href="/2024/07/17/%E6%8F%AD%E7%A4%BA%E6%96%87%E6%9C%AC%E3%80%81%E5%9B%BE%E5%83%8F%E3%80%81%E8%A7%86%E9%A2%91%E5%92%8C%E9%9F%B3%E9%A2%91%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E5%B9%BB%E8%A7%89%EF%BC%9A%E7%BB%BC%E5%90%88%E8%B0%83%E6%9F%A5/"/>
    <url>/2024/07/17/%E6%8F%AD%E7%A4%BA%E6%96%87%E6%9C%AC%E3%80%81%E5%9B%BE%E5%83%8F%E3%80%81%E8%A7%86%E9%A2%91%E5%92%8C%E9%9F%B3%E9%A2%91%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E5%B9%BB%E8%A7%89%EF%BC%9A%E7%BB%BC%E5%90%88%E8%B0%83%E6%9F%A5/</url>
    
    <content type="html"><![CDATA[<h1id="unveiling-hallucination-in-text-image-video-and-audio-foundation-models-a-comprehensive-survey">UnveilingHallucination in Text, Image, Video, and Audio Foundation Models: AComprehensive Survey</h1><h2 id="一.简介">一.简介</h2><h3 id="四种幻觉的类型">1.四种幻觉的类型</h3><p><strong>Contextualdisconnection</strong>(情景脱节)：模型多模态生成的内容不一致或期望的上下文不符合预期</p><p><strong>Semantic distortion</strong>(语义扭曲)：生成内容不一致或错误，期中输入的语义或潜在含义在输出中被歪曲</p><p><strong>Content hallucination</strong>(内容幻觉)：模型输出的内容不真实或不存在于输入数据中</p><p><strong>Factual inaccuracy</strong>(事实不准确)：生成信息不准确，有欺骗性或与已知信息不一致</p><p>举例如下：</p><h2 id="二.hallucination-in-large-language-models">二.Hallucination inLarge Language Models</h2><h3 id="幻觉检测和缓解">1.幻觉检测和缓解</h3><h4 id="检测">1.1检测：</h4><ol type="1"><li>SelfCheckGPT Manakul等人提供了一种<strong>零资源黑盒解决方案</strong>，用于在不依赖外部资源的情况下检测任何LLM中的幻觉。此方法的运作原理是，<strong>熟悉某个主题的LLM将在其响应中产生一致且可比较的事实。相比之下，从不熟悉的主题中随机抽取的回答可能包含矛盾和幻觉的事实。</strong>(动机:现有的事实核查方法通常依赖于复杂的模块或外部数据库，需要输出概率分布或与外部源连接。)</li><li>Yang等人继续探索通道级幻觉检测方法，提出了一种<strong>基于反向验证的新型自检方法</strong>，旨在自动识别事实错误，而无需外部资源。他们引入了一个<strong>基准</strong>，即<strong>段落级(Passage-level)幻觉检测（PHD）</strong>，该基准使用ChatGPT 生成并由人类专家注释以评估不同的方法。</li><li>Min 等人<strong>引入了FACTSCORE（原子性分数中的事实精度）</strong>，这是一种新颖的评估方法，可将文本分解为单个事实并衡量其可靠性。</li><li>Huang 和Chang<strong>通过与已建立的网络系统进行比较，引入了一种独特的策略</strong>来减轻LLMs 中的幻觉风险。他们认为缺乏“引用”机制（即承认或引用来源或证据）是一个重大差距。</li></ol><h4 id="缓解">1.2缓解：</h4><ol type="1"><li>Rawte 等人<strong>开发多任务学习 (MTL)框架，集成先进的长文本嵌入</strong>（如 e5-mistral-7b-instruct）以及GPT-3、SpanBERT 和 RoFormer 等模型。(动机：为了解决识别 LLM生成内容中事实不准确的问题)</li><li>Xu 等人引入了一个正式<strong>框架</strong>，将幻觉定义为可计算 LLMs与真实函数之间观察到的差异，通过这个框架，该研究检查了现有的幻觉缓解策略及其对现实世界LLM部署的实际影响。</li><li>Rawte 等人引入了<strong>“Sorry, Come Again”(SCA)提示技术</strong>来解决当代LLMs中的幻觉。 SCA 通过采用最佳释义和注入[PAUSE] 标记来延迟 LLM生成来增强理解。它分析了提示中的语言细微差别及其对产生幻觉的一代的影响，强调了以可读性、形式性或具体性较低为特征的提示所带来的困难。他们还做了调查LLMs如何对事实上正确和错误的提示做出反应，将他们的幻觉分为mild,moderate, and alarming的子类别。此外，该论文还<strong>引入了幻觉eLiciTation 数据集</strong>，其中包含由人类注释的 75,000个文本片段，并引入了一种新颖的幻觉漏洞指数指标。</li></ol><h3 id="特定领域中的幻觉问题">2.特定领域中的幻觉问题</h3><h4 id="医疗保健">2.1医疗保健：</h4><ol type="1"><li>Pal 等人引入了医学领域幻觉<strong>测试</strong>(Med-HALT)，这是一个旨在评估和减轻幻觉的专门基准<strong>数据集</strong>。Med-HALT由来自多个国家的医疗记录的各种跨国数据集组成，总共包含七个数据集。</li><li>Med-HALT等人。概述了创建可靠、值得信赖和公正模型的<strong>基本步骤</strong>，强调需要在医疗保健背景下量化、验证和减轻幻觉。</li><li>Ji等人引入了一种<strong>交互式自我反思方法</strong>，旨在提高医疗问答系统使用LLMs生成的答案的准确性和连贯性。通过知识获取和答案生成反馈，该方法增强了答案的真实性、一致性和逻辑进展。</li></ol><h4 id="财务">2.2财务</h4><ol type="1"><li>Kang and Liu<strong>对LLMs在财务任务中的幻觉倾向进行了实证调查</strong>。他们的研究评估了LLMs在解释金融概念、查询历史股票价格方面的熟练程度，并检验了几次学习和基于提示的工具学习等方法在减轻幻觉方面的有效性。</li><li>Roychowdhury et al<strong>提出了一种基于 Langchain的新颖方法</strong>，旨在将数据表转换为分层文本数据块，促进多功能金融问题回答。该框架包括按意图对用户查询进行分类、检索相关数据块、生成定制的LLM 提示以及评估幻觉和置信度的响应。</li></ol><h4 id="法律">2.3法律</h4><ol type="1"><li>Feijo 和 Moreira<strong>引入了LegalSumm</strong>，它创建源文本的独特“视图”，训练摘要模型以生成独立的摘要，并采用蕴含模块来评估其对源文本的保真度。（动机:抽象文本摘要的传统方法通常采用编码器-解码器架构，其中编码器封装源文本的本质，而解码器生成摘要。然而，这种方法可能会生成包含不相关或不准确信息的摘要，这在准确性至关重要的法律环境中引起了重大问题。)</li><li>Deroy et al 通过将 SOTA模型应用于印度法院案件，<strong>调查</strong>了 LLMs生成案件判决抽象摘要的准备情况。</li><li>Savelka et al <strong>评估 GPT-4在对立法中的法律术语生成事实上准确、清晰且相关的解释方面的表现。</strong>对基线方法（GPT-4直接解释法律术语）与增强方法（使用法律信息检索模块提供判例法中的上下文句子）进行了比较(动机:虽然抽象模型的得分通常略高，但作者注意到生成的摘要中存在不一致和幻觉。理解开放式法律术语的含义对于法律专业人士来说非常重要。他们经常研究这些术语在以前的法庭案件中是如何使用和解释的。)</li><li>Dahl etal<strong>提供了关于法律领域不准确的频率和类型的初步证据</strong>，为评估法律环境中的LLMs提供了宝贵的见解。通过考察美国判例法的结构化格式，该研究评估了三个主要的LLMs：GPT-3.5、PaLM2 和 Llama。</li></ol><h3 id="基准评估">3.基准评估</h3><ol type="1"><li>Zhang 等人<strong>设计了三个跨越不同领域的问答数据集</strong>，其中ChatGPT 和 GPT-4经常提供不准确的答案以及至少一个错误说法的解释。值得注意的是，<strong>该研究表明语言模型可以将这些错误的说法识别为不正确</strong>。(动机：在某些情况下，LLMs会出现一种被称为“幻觉滚雪球”的现象，即他们编造虚假的说法来合理化之前的幻觉，尽管他们承认自己的幻觉不准确。)</li><li>FactCHD Chen等人<strong>引入了一个基准数据集</strong>，<strong>目的是在复杂的推理环境中检测与事实相冲突的幻觉</strong>。它包含一系列捕获不同事实模式的数据集，并集成基于事实的证据链以提高评估准确性。</li><li>Li等人<strong>引入了一个数据集</strong>来<strong>评估LLMs识别和识别幻觉或不正确信息的能力</strong>。结果突显了ChatGPT倾向于产生幻觉内容，特别是在某些主题上，从而引入无法验证的信息。</li></ol><h2id="三.hallucination-in-large-vision-language-models">三.Hallucinationin Large Vision-Language Models</h2><h3id="hallucination-detection-and-mitigation-幻觉检测和缓解">1.HallucinationDetection and Mitigation 幻觉检测和缓解</h3><h4 id="detection">1.1 Detection</h4><ol type="1"><li>Dai etal<strong>研究了视觉语言预训练（VLP）模型中的物体幻觉问题</strong>，其中这些模型生成的文本描述包含基于输入图像的不存在或不准确的物体</li><li>Li etal<strong>揭示了广泛且严重的物体幻觉问题，并表明视觉指令可能会影响幻觉。</strong>他们观察到，经常在视觉指令中描绘的物体或与图像物体同时出现的物体更容易产生幻觉。为了增强物体幻觉的评估过程，作者引入了一种名为POPE的基于轮询的查询方法，该方法提高了评估物体幻觉的稳定性和灵活性。</li><li>Lovenia 等人<strong>引入了 NOPE（负物体存在评估），这是通过视觉问答(VQA) 评估视觉语言模型 (VLM) 中物体幻觉的基准。</strong>该研究利用 LLMs生成了包含 29.5k 个 NOPE 合成否定代词 (NegP) 实例的数据集。它全面评估了10 个 VLM 在检测视觉问题中是否存在对象的能力，以及它们在其他 9 个 VQA数据集中处理视觉问题的典型表现。(动机:由于缺乏评估物体幻觉的标准化指标，阻碍了理解和解决这一问题的进展。为了解决这一差距。)</li><li>Liu etal深入<strong>研究内在视觉语言幻觉（IVL-Hallu）并提出了几个新颖的IVL-Hallu任务，包括属性、物体、多模态冲突和反常识幻觉</strong>。他们引入了一个具有挑战性的基准数据集来评估和探索 IVL-Hallu，对五个 LVLM进行了实验，结果表明它们在解决所提出的任务方面效果有限。</li><li>Zhao 等人<strong>推出了 MARINE，这是一种免培训、免 API的解决方案。</strong> MARINE通过结合现有的开源视觉模型并利用没有分类器的指导来合并对象接地功能，从而增强了LVLM 的视觉理解能力，从而提高了生成输出的精度。对六个 LVLM 的评估揭示了MARINE 在减少幻觉和增强输出细节方面的有效性，并通过使用 GPT-4V的评估进行了验证。</li></ol><h4 id="mitigation">1.2Mitigation</h4><ol type="1"><li>HalluciDoctor Yu etal通过<strong>使用人为错误检测来识别和消除各种类型的幻觉，从而解决了多模态大语言模型（MLLM）中的幻觉问题</strong>。通过反事实视觉指令扩展重新平衡数据分布，他们成功减轻了44.6% 的幻觉，同时保持了竞争性的表现。</li><li>Xu <strong>提出 ChartBench，一个评估图表理解力的基准</strong>。ChartBench 通过复杂的图表暴露了 MLLM的有限推理能力，这促使人们需要新的评估指标，例如 Acc+ 和手工制作的提示符ChartCoT。</li><li>Zhang et al<strong>介绍了 InternLM-XComposer，这是一种LVLM，旨在解决图像文本理解和合成中的幻觉挑战</strong>。InternLM-XComposer 文本图像合成的性能通过涉及人工评估和与 GPT4-Vision比较的稳健程序进行评估，该模型展示了与 GPT4-V 和 GPT3.5等解决方案相比的竞争性能。</li><li>Gunjal 等人<strong>推出了M-HalDetect，这是一个开创性的多模态细粒度幻觉检测数据集。</strong>该数据集可作为训练LVLM 的基准，从而获得更精确的输出。使用细粒度的多模式奖励模型并增强 FDPO显着降低了 InstructBLIP 的幻觉率。这些方法不仅提高了 LLaVA 和 mPLUG-OWL等 LVLM 的准确性，而且还强调了 M-HalDetect在识别和减少幻觉方面的多功能性和有效性。(动机:InstructBLIP Dai 等尖端LVLM虽然产生了基于视觉的响应，但通常包含虚构项目和有缺陷的关系等不准确之处。为了提高准确性)</li><li>Liu et al.<strong>开发 LRV-Instruction，这是一个综合数据集，包含 16个任务的 40万条视觉指令。该数据集包括各种风格和语义级别的正面和负面指令。通过LRV-Instruction，对现有 LMM中的幻觉问题进行了广泛检查，证实了其在增强视觉指令调整方面的有效性。</strong>此外，他们还推出了GAVIE，这是一种评估视觉指令调整的新颖方法，无需人工标记答案，可以适应不同类型的指令。(动机：尽管多模态任务取得了进步，但LMM 经常生成与随附图像或人类指令不一致的描述。为了解决这个问题)</li><li>Zhou 等人<strong>开发了 LVLM 幻觉修正器(LURE)通过改进描述来纠正 LVLM中的物体幻觉，以产生更准确和更少幻觉的输出。</strong>其方法基于深入的统计分析**，识别导致物体幻觉的关键因素，例如图像中某些物体的共现、LVLM解码过程中与物体相关的不确定性，以及幻觉发生的趋势。生成文本的末尾。LURE 专为与各种 LVLM 无缝集成而设计。当在多个 LVLM 上进行测试时，LURE的集成显着增强了物体幻觉校正能力，在基于各种指标的 GPT和人类评估中始终优于其他方法。</li></ol><h3 id="benchmark-evaluation-基准评估">2.Benchmark Evaluation基准评估</h3><ol type="1"><li>Li等人<strong>提出了一种新颖的数据收集方法</strong>，该方法可以同步合成图像和对话以进行视觉指令调整，从而产生图像-对话对和多图像实例的大型数据集。</li><li>Huang et al.<strong>引入了 VHTest，这是一个基准数据集</strong>，包含8 种 VH 模式下的 1,200 个不同的幻视 (VH) 实例。对三个 SOTA MLLM的评估显示出不同的性能，GPT-4V 表现出比 MiniGPT-v2 更低的幻觉。</li><li>Rawte 将 VLM 中的视幻觉分为八个方向，并引入了包含这些类型的 2,000个样本的数据集。他们<strong>提出了三大类减轻幻觉的方法：数据驱动方法、训练调整和后处理技术</strong></li><li>Wang et al<strong>提出了视觉指令生成和校正 (VIGC) 框架来解决 MLLM高质量指令调优数据的短缺问题。</strong> VIGC 使 MLLM能够生成多样化的指令调整数据，同时通过视觉指令校正 (VIC)迭代地改进其质量，从而降低幻觉风险。该框架生成多样化的高质量数据，用于微调模型，通过评估进行验证，提高基准性能，并克服仅限语言的数据限制。</li></ol><h2 id="四.hallucinations-in-large-video-models">四.Hallucinations inLarge Video Models</h2><h3 id="hallucination-detection-and-mitigation">1.HallucinationDetection and Mitigation</h3><ol type="1"><li>Mun等人引入了一种新颖的方法来<strong>建立时间依赖性模型并利用上下文来连贯地讲述故事</strong>。通过集成事件序列生成网络和经过强化学习和两级奖励训练的顺序视频字幕网络，该模型可以更有效地捕获上下文信息，产生连贯且准确的字幕，同时最大限度地降低幻觉风险</li><li>Liu 和 Wan<strong>引入了一种新颖的弱监督、基于模型的事实性度量，称为FactVC，它的性能优于以前的度量。</strong>此外，他们<strong>还提供了两个带注释的数据集，以促进评估视频字幕真实性的进一步研究</strong></li><li>Wu 和Gau（2023）<strong>提出了一种上下文感知模型，该模型结合了过去和未来事件的信息，有条件地影响当前事件的描述</strong>。他们的方法利用强大的预训练上下文编码器来编码有关周围上下文事件的信息，然后使用门注意机制将其集成到字幕模块中。YouCookII 和 ActivityNet数据集上的实验结果表明，所提出的上下文感知模型显着优于现有的上下文感知和预训练模型。</li><li>Zhou etal<strong>引入了一种流模型，包括用于长视频处理的内存模块和能够在视频完成之前进行预测的流解码算法。这种方法显着提高了著名的密集视频字幕基准测试的性能，</strong>例如YouCook2、ActivityNet 和 ViTT。</li><li>Himakunthala等人<strong>引入了一个推理时间挑战数据集，其中包含具有密集字幕和结构化场景描述的关键帧。该数据集包含关键帧，辅以非结构化密集字幕和结构化FAMOUS：（焦点、动作、情绪、对象和设置）场景描述，提供有价值的上下文信息以支持模型对视频内容的理解。</strong>他们采用了GPT-3、GPT-4 和 Vicuna等各种语言模型，并通过贪婪解码来降低幻觉风险。</li><li>Yu et al<strong>提出了一种缺陷感知 Masked Transformer(DMT)，一种双模态兼容的修复框架。</strong>该方法通过预训练图像修复模型作为训练视频模型的先验，改进了信息不完整的场景处理。</li><li>Kulal etal<strong>引入了一种将人物真实地插入场景中的方法。</strong>该模型通过根据上下文推导出逼真的姿势并确保视觉上令人愉悦的构图，将个体无缝地融入场景中</li><li>Chuang 和 Fazli<strong>引入了 CLearViD，这是一种基于 Transformer的模型，利用课程学习技术来提高表现。</strong>通过采用这种方法，模型获得了更稳健和更通用的特征。此外，CLearViD结合了 Mish激活函数来解决梯度消失等问题，从而通过引入非线性和非单调性来降低产生幻觉的风险。</li></ol><h3 id="benchmark-evaluation">2.Benchmark Evaluation</h3><ol type="1"><li>Zhang etal<strong>创建了一种创新的两级分层融合方法，仅使用一张具有中性表情的正面面部图像，从训练视频样本中产生幻觉面部表情序列。</strong>为了有效地训练系统，他们引入了专门为面部表情幻觉设计的数据集，其中包括来自28 个人的 112个视频序列，涵盖了四种类型的面部表情（快乐、愤怒、惊讶和恐惧），从而生成了合理的面部表情时域和空间域中的序列具有较少的伪影。</li><li>Zhou et al<strong>组装了 YouCook2数据集</strong>，这是一组广泛的烹饪视频，其中包含时间本地化和描述的程序片段，<strong>以促进程序学习任务</strong>。</li><li>Li etal.<strong>引入了“VideoChat”，这是一种通过可学习的神经接口集成视频基础模型和LLMs的新颖方法，以增强视频理解中的时空推理、事件定位和因果关系推理。</strong>研究人员构建了一个以视频为中心的教学数据集，其中包含详细的描述和对话，强调时空推理和因果关系。为了抵消模型幻觉，他们采用了多步骤过程，使用GPT-4将视频描述压缩为连贯的叙述，并对其进行细化以提高清晰度和连贯性。</li><li>Kulal et al <strong>策划了一个包含 240万个视频剪辑的数据集</strong>，展示了与场景背景相符的各种看似合理的姿势</li></ol><h2 id="五.hallucinations-in-large-audio-models">五.Hallucinations inLarge Audio Models</h2><h3 id="hallucination-detection-and-mitigation-1">1.HallucinationDetection and Mitigation</h3><ol type="1"><li>Xu 等人 <strong>引入了 AudioSet标签引导模型，旨在引导大规模音频文本数据(BLAT)。</strong>值得注意的是，该模型回避了视频的合并，从而最大限度地减少了与视觉模态相关的噪声。一系列任务（包括检索、生成和分类）的实验结果验证了BLAT在减轻幻觉问题方面的有效性。(动机：在音频字幕领域，自动生成音频剪辑的自然语言描述，在音频文本模型的预训练过程中过度依赖视觉模态带来了重大挑战。这种依赖引入了数据噪音和幻觉，最终破坏了生成的字幕的准确性。为了解决这个问题)</li><li>Xu et al. <strong>设计了SECap，这是一个专为语音情感字幕设计的框架</strong>，旨在使用自然语言捕捉语音中复杂的情感细微差别。SECap 利用各种组件，包括作为文本解码器的 LLaMA、作为音频编码器的 HuBERT和作为 Bridge-Net 的 Q-Former，根据语音特征生成连贯的情感字幕。</li><li>Elizalde 等人<strong>引入了对比语言音频预训练 (CLAP) 模型。</strong>CLAP 经过 460万个不同的音频文本对的预训练，具有双编码器架构，可增强表示学习，从而改善跨声音、音乐和语音领域的任务泛化。</li></ol><h3 id="benchmark-evaluation-1">2.Benchmark Evaluation</h3><ol type="1"><li>Doh 等人<strong>引入了LP-MusicCaps，这是一个综合数据集</strong>，包含 50 万个音频剪辑以及大约220 万个字幕。利用LLMs，他们使用数据集训练基于 Transformer的音乐字幕模型，并评估其在零样本和迁移学习场景下的性能，证明其相对于监督基线模型的优越性</li><li>Nishimura etal.<strong>研究了大型音频-视频语言模型中的音频幻觉</strong>。他们将这些幻觉分为三种不同的类型，例如同时涉及物体和动作的幻觉、具有准确的物体但幻觉的动作以及显示正确的动作但幻觉的物体</li><li>Ghosh 等人 <strong>推出了CompA，由两个专家注释的基准测试组成，主要关注真实世界的音频样本</strong>。该基准用于通过新颖的学习方法对CompA-CLAP进行微调，增强其组合推理技能，并在需要组合推理的任务中展示出相对于所有基线模型的显着改进。</li></ol><h2 id="六.hallucination-good-or-bad">六.Hallucination: Good orBad?</h2><p>Good:</p><ol type="1"><li>创造力的体现</li><li>激发探索性学习，可以作为压力测试的一种形式</li><li>乃至激发人类的创造力</li></ol><p>Bad:</p><ol type="1"><li>对输出的质量和一致性可能值得怀疑，让他暂时无法在对准确性和可靠性需求高的应用使用</li><li>传播错误信息和偏见</li><li>道德问题</li></ol><h2 id="七.limitations">七.Limitations</h2><p>本文创新点：涵盖视觉、音频、视频模式中的幻觉</p><p>受限：不一定足够全面</p><h2 id="八.future-directions">八.Future Directions</h2><p>潜在方向：</p><h3 id="data-resources">1.Data Resources:</h3><p>最近的研究强调了<strong>对精心挑选的高质量样本进行简单微调对于减少幻觉的功效，超越了大规模微调和强化学习方法的影响。</strong>对于知识密集型领域，<strong>开发以实体为中心的微调指令，集成知识图中的结构化知识</strong>，有望提高准确性和相关性。此外，事实证明，<strong>采用针对特定任务或领域量身定制的对齐技术可以有效减轻幻觉</strong>。随着该领域研究的进展，预计会有更多资源专注于通过特定于任务或领域适应的方法来提高一致性，从而进一步增强语言模型在生成事实和值得信赖的内容方面的可靠性。</p><p>2.<strong>Automated Evaluation:</strong></p><p>自动评估：<strong>开发考虑事实准确性和连贯性等因素的专门评估指标</strong>对于幻觉检测非常有用。通过<strong>众包</strong>将自动化评估与人类判断相结合，可以捕获仅对自动化系统构成挑战的细微差别。此外，<strong>对抗性测试方法也正在开发中</strong>，以使人工智能系统接受精心设计的输入，帮助识别弱点并增强抵御幻觉的能力。此外，在<strong>数据集上微调FM，强调事实检查和准确性</strong>，为提高内容可靠性和减少幻觉的发生提供了另一种途径。</p><p>3.<strong>Improving Detection and Mitigation techniques:</strong></p><p>改进检测和缓解技术：缓解 FM中的幻觉需要采取多方面的方法，<strong>利用推理机制、知识图集成、专门的事实检查模型、偏差缓解技术和主动学习方法。诸如Chain of Thought (CoT) Wei等新兴技术</strong>和思想树。增强了这些模型的推理能力，有可能减少幻觉。集成知识图可以增强对事实信息和概念关系的理解，有助于内容生成和事实检查。专门的验证模型将输出与精选知识进行交叉引用，以识别不准确之处，而<strong>偏差检测和缓解技术</strong>则可促进公平性。最后，在人工智能开发中负责任地<strong>使用精选知识的道德准则和监管框架</strong>可以降低风险并培养公众信任，共同提高人工智能生成内容的质量、准确性和可信度。</p><p>4.<strong>Multimodal Hallucination:</strong></p><p>多模态幻觉：解决多模态大模型中的幻觉需要采取一种全面的方法，涵盖以数据为中心的举措、<strong>跨模态调整工作、架构创新、标准化基准测试、重构幻觉以及增强可解释性和信任</strong>。用于稳健数据收集、增强和校准的以数据为中心的技术可确保训练数据的多样性和高质量。<strong>跨模式对齐</strong>侧重于通过复杂的架构来对齐跨模式的表示。<strong>模型架构的进步</strong>涉及设计能够有效处理复杂语言和视觉输入的专用模型。<strong>建立统一的指标和标准化基准</strong>可以实现对幻觉的准确评估和可靠的绩效评估。将幻觉重新定义为一项功能，探索其与下游应用程序的集成，优化人类体验。最后，<strong>开发解释模型行为、可视化内部结构和改进可靠性评估的技术</strong>可以培养对MLLM的信任。这种多方面的方法共同解决了关键的幻觉挑战，为更可靠、更值得信赖的多模式人工智能系统铺平了道路。</p>]]></content>
    
    
    
    <tags>
      
      <tag>幻觉</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
